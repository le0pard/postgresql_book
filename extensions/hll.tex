\section{Postgresql-hll}

На сегодняшний день широко распространена задача подсчета количества уникальных элементов (count-distinct problem) в потоке данных, которые могут содержать повторяющиеся элементы. Например, сколько уникальных IP-адресов подключалось к серверу за последний час? Сколько различных слов в большом куске текстов? Какое количество уникальных посетителей побывало на популярном сайте за день? Сколько уникальных URL было запрошено через прокси-сервер? Данную задачу можно решить <<в лоб>>: пройтись по всем элементам и убрать дубликаты, после этого посчитать их количество (например использовать множество, set). Трудности в таком подходе возникают при увеличении масштаба. С минимальными затратами можно подсчитать тысячу или даже миллион уникальных посетителей, IP-адресов, URL или слов. А что если речь идет о 100 миллионах уникальных элементов на один сервер при наличии тысяч серверов? Теперь это уже становится интересным.

Текущее решение проблемы будет выглядеть так: необходимо сформировать множества (set) уникальных элементов для каждого из 1000 серверов, каждое из которых может содержать около 100 миллионов уникальных элементов, а затем подсчитать количество уникальных элементов в объединении этих множеств. Другими словами, мы имеем дело с распределенным вариантом задачи подсчета уникальных элементов. Хоть это решение является вполне логичным, на практике этот подход обойдется высокой ценой. Для примера возьмем URL, средняя длина которого составляет 76 символов. В нашем случае один сервер обслуживает около 100 миллионов уникальных URL, следовательно, размер файла с их перечнем составит около 7.6 ГБ. Даже если каждый URL преобразовать в 64-битный хеш, размер файла составит 800 МБ. Это намного лучше, но не забывайте, что речь идет о 1000 серверов. Каждый сервер отправляет файл с перечнем уникальных URL на центральный сервер, следовательно, при наличии 1000 серверов функция объединения множеств должна обработать 800 ГБ данных. Если такая операция должна выполняться часто, тогда необходимо будет либо установить систему для обработки больших данных (и нанять команду для ее обслуживания), либо найти другое решение.

И вот на сцену выходит \href{https://en.wikipedia.org/wiki/HyperLogLog}{HyperLogLog} алгоритм. Этот алгоритм реализует вероятностный подход к задаче подсчета уникальных элементов и базируется на двух следующих положениях:

\begin{itemize}
  \item вероятность того, что любой данный бит двоичного представления случайного числа равен единице, составляет 50\%;
  \item вероятность того, что совместно произойдут два независимых случайных события $A$ и $B$, вычисляется по формуле $P(A)*P(B)$. Таким образом, если вероятность равенства единице одного любого бита случайного числа составляет 50\%, тогда вероятность равенства единице двух любых битов составляет 25\%, трех~--- 12,5\% и т.д;
\end{itemize}

Вспомним еще одно базовое положение теории вероятностей, согласно которому ожидаемое количество испытаний, необходимое для наступления события, вычисляется по формуле $1/P(event)$. Следовательно, если $P(one\ specific\ bit\ set) = 50\%$, то ожидаемое количество испытаний равно 2. Для двух битов~--- 4, для трех битов~--- 8 и т.д.

В общем случае входные значения не являются равномерно распределенными случайными числами, поэтому необходим способ преобразования входных значений к равномерному распределению, т.е. необходима хеш-функция. Обратите внимание, в некоторых случаях распределение, получаемое на выходе хеш-функции, не оказывает существенное влияние на точность системы. Однако HyperLogLog очень чувствителен в этом отношении. Если выход хеш-функции не соответствует равномерному распределению, алгоритм теряет точность, поскольку не выполняются базовые допущения, лежащие в его основе.

Рассмотрим алгоритм подробно. Вначале необходимо хешировать все элементы исследуемого набора. Затем нужно подсчитать количество последовательных начальных битов, равных единице, в двоичном представлении каждого хеша и определить максимальное значение этого количества среди всех хешей. Если максимальное количество единиц обозначить $n$, тогда количество уникальных элементов в наборе можно оценить, как $2^n$. То есть, если максимум один начальный бит равен единице, тогда количество уникальных элементов, в среднем, равно 2; если максимум три начальных бита равны единице, в среднем, мы можем ожидать 8 уникальных элементов и т.д.

Подход, направленный на повышение точности оценки и являющийся одной из ключевых идей HyperLogLog, заключается в следующем: разделяем хеши на подгруппы на основании их конечных битов, определяем максимальное количество начальных единиц в каждой подгруппе, а затем находим среднее. Этот подход позволяет получить намного более точную оценку общего количества уникальных элементов. Если мы имеем $m$ подгрупп и $n$ уникальных элементов, тогда, в среднем, в каждой подгруппе будет $n/m$ уникальных элементов. Таким образом, нахождение среднего по всем подгруппам дает достаточно точную оценку величины $log_2{(n/m)}$, а отсюда легко можно получить необходимое нам значение. Более того, HyperLogLog позволяет обрабатывать по отдельности различные варианты группировок, а затем на основе этих данных находить итоговую оценку. Следует отметить, что для нахождения среднего HyperLogLog использует среднее гармоническое, которое обеспечивает лучшие результаты по сравнению со средним арифметическим (более подробную информацию можно найти в оригинальных публикациях, посвященных \href{http://www.ic.unicamp.br/~celio/peer2peer/math/bitmap-algorithms/durand03loglog.pdf}{LogLog} и \href{http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf}{HyperLogLog}).

Вернемся к задаче. По условию существует 1000 серверов и 100 миллионов уникальных URL на каждый сервер, следовательно, центральный сервер должен обрабатывать 800 ГБ данных при каждом выполнении простого варианта алгоритма. Это также означает, что 800 ГБ данных каждый раз необходимо передавать по сети. HyperLogLog меняет ситуацию кардинально. Согласно анализу, проведенному авторами оригинальной публикации, HyperLogLog обеспечивает точность около 98\% при использовании всего 1.5 КБ памяти. Каждый сервер формирует соответствующий файл размером 1.5 КБ, а затем отправляет его на центральный сервер. При наличии 1000 серверов, центральный сервер обрабатывает всего 1.5 МБ данных при каждом выполнении алгоритма. Другими словами, обрабатывается лишь 0.0002\% данных по сравнению с предыдущим решением. Это полностью меняет экономический аспект задачи. Благодаря HyperLogLog, возможно выполнять такие операции чаще и в большем количестве. И все это ценой всего лишь 2\% погрешности.

Для работы с этим алгоритмом внутри PostgreSQL было создано расширение \href{https://github.com/aggregateknowledge/postgresql-hll}{postgresql-hll}. Оно добавляет новый тип поля \lstinline!hll!, который представляет собой HyperLogLog структуру данных. Рассмотрим пример его использования.

\subsection{Установка и использование}

Для начала инициализируем расширение в базе данных:

\begin{lstlisting}[language=SQL,label=lst:pghll1,caption=Инициализация hll]
# CREATE EXTENSION hll;
\end{lstlisting}

Давайте предположим, что есть таблица \lstinline!users_visits!, которая записывает визиты пользователей на сайт, что они сделали и откуда они пришли. В таблице сотни миллионов строк.

\begin{lstlisting}[language=SQL,label=lst:pghll2,caption=users\_visits]
CREATE TABLE users_visits (
  date            date,
  user_id         integer,
  activity_type   smallint,
  referrer        varchar(255)
);
\end{lstlisting}

Требуется получать очень быстро представление о том, сколько уникальных пользователей посещают сайт в день на админ панеле. Для этого создадим агрегатную таблицу:

\begin{lstlisting}[language=SQL,label=lst:pghll3,caption=daily\_uniques]
CREATE TABLE daily_uniques (
  date            date UNIQUE,
  users           hll
);

-- Fill it with the aggregated unique statistics
INSERT INTO daily_uniques(date, users)
    SELECT date, hll_add_agg(hll_hash_integer(user_id))
    FROM users_visits
    GROUP BY 1;
\end{lstlisting}

Далее хэшируется \lstinline!user_id! и собираются эти хэш-значения в один \lstinline!hll! за день. Теперь можно запросить информацию по уникальным пользователям за каждый день:

\begin{lstlisting}[language=SQL,label=lst:pghll4,caption=daily\_uniques по дням]
# SELECT date, hll_cardinality(users) FROM daily_uniques;
    date    | hll_cardinality
------------+-----------------
 2017-02-21 |            23123
 2017-02-22 |            59433
 2017-02-23 |          2134890
 2017-02-24 |          3276247
(4 rows)
\end{lstlisting}

Можно возразить, что такую задачу можно решить и через \lstinline!COUNT DISTINCT! и это будет верно. Но в примере только ответили на вопрос: <<Сколько уникальных пользователей посещает сайт каждый день?>>. А что, если требуется знать сколько уникальных пользователей посетили сайт за неделю?

\begin{lstlisting}[language=SQL,label=lst:pghll5,caption=daily\_uniques за неделю]
SELECT hll_cardinality(hll_union_agg(users)) FROM daily_uniques WHERE date >= '2017-02-20'::date AND date <= '2017-02-26'::date;
\end{lstlisting}

Или выбрать уникальных пользователей за каждый месяц в течении года?

\begin{lstlisting}[language=SQL,label=lst:pghll6,caption=daily\_uniques за каждый месяц]
SELECT EXTRACT(MONTH FROM date) AS month, hll_cardinality(hll_union_agg(users))
FROM daily_uniques
WHERE date >= '2016-01-01' AND
      date <  '2017-01-01'
GROUP BY 1;
\end{lstlisting}

Или узнать количество пользователей, что посетили сайт вчера, но не сегодня?

\begin{lstlisting}[language=SQL,label=lst:pghll7,caption=daily\_uniques за вчера но не сегодня]
SELECT date, (#hll_union_agg(users) OVER two_days) - #users AS lost_uniques
FROM daily_uniques
WINDOW two_days AS (ORDER BY date ASC ROWS 1 PRECEDING);
\end{lstlisting}

Это всего пара примеров типов запросов, которые будут возвращать результат в течении миллисекунд благодаря \lstinline!hll!, но потребует либо полностью отдельные предварительно созданные агрегирующие таблицы или \lstinline!self join/generate_series! фокусы в \lstinline!COUNT DISTINCT! мире.

\subsection{Заключение}

Более подробно о использовании расширения можно ознакомиться через \href{https://github.com/aggregateknowledge/postgresql-hll/blob/master/README.markdown}{официальную документацию}.
