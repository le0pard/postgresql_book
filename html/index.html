<!DOCTYPE html> <html> <head> <meta charset=utf-8> <meta name=generator content=pandoc> <meta name=viewport content="width=device-width, initial-scale=1.0, user-scalable=yes"> <meta name=author content="А. Ю. Васильев aka leopard"> <title>Работа с PostgreSQL: настройка и масштабирование</title> <style>code{white-space: pre;}</style> <link rel=stylesheet href="/html/pandoc.css"> <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script><![endif]--> </head> <body> <header> <h1 class=title>Работа с PostgreSQL: настройка и масштабирование</h1> <p class=author>А. Ю. Васильев aka leopard</p> <p class=date>Creative Commons Attribution-Noncommercial 4.0 International<br/> 2017</p> </header> <nav id=TOC> <ul> <li><a href="#введение">Введение</a><ul> <li><a href="#что-такое-postgresql">Что такое PostgreSQL?</a></li> </ul></li> <li><a href="#настройка-производительности">Настройка производительности</a><ul> <li><a href="#введение-1">Введение</a><ul> <li><a href="#не-используйте-настройки-по-умолчанию">Не используйте настройки по умолчанию</a></li> <li><a href="#используйте-актуальную-версию-сервера">Используйте актуальную версию сервера</a></li> <li><a href="#стоит-ли-доверять-тестам-производительности">Стоит ли доверять тестам производительности</a></li> </ul></li> <li><a href="#настройка-сервера">Настройка сервера</a><ul> <li><a href="#используемая-память">Используемая память</a></li> <li><a href="#журнал-транзакций-и-контрольные-точки">Журнал транзакций и контрольные точки</a></li> <li><a href="#планировщик-запросов">Планировщик запросов</a></li> <li><a href="#сбор-статистики">Сбор статистики</a></li> </ul></li> <li><a href="#sec:hard-drive-and-file-systems">Диски и файловые системы</a><ul> <li><a href="#перенос-журнала-транзакций-на-отдельный-диск">Перенос журнала транзакций на отдельный диск</a></li> <li><a href="#sec:hard-drive-cluster">CLUSTER</a></li> </ul></li> <li><a href="#утилиты-для-тюнинга-postgresql">Утилиты для тюнинга PostgreSQL</a><ul> <li><a href="#pgtune">Pgtune</a></li> <li><a href="#pg_buffercache">pg_buffercache</a></li> </ul></li> <li><a href="#оптимизация-бд-и-приложения">Оптимизация БД и приложения</a><ul> <li><a href="#поддержание-базы-в-порядке">Поддержание базы в порядке</a></li> <li><a href="#использование-индексов">Использование индексов</a></li> <li><a href="#перенос-логики-на-сторону-сервера">Перенос логики на сторону сервера</a></li> <li><a href="#sec:pg-optimize-sql">Оптимизация конкретных запросов</a></li> <li><a href="#утилиты-для-оптимизации-запросов">Утилиты для оптимизации запросов</a></li> </ul></li> <li><a href="#заключение">Заключение</a></li> </ul></li> <li><a href="#индексы">Индексы</a><ul> <li><a href="#типы-индексов">Типы индексов</a><ul> <li><a href="#b-tree">B-Tree</a></li> <li><a href="#r-tree">R-Tree</a></li> <li><a href="#hash-индекс">Hash индекс</a></li> <li><a href="#sec:indexes-bitmap-index">Битовый индекс (bitmap index)</a></li> <li><a href="#gist-индекс">GiST индекс</a></li> <li><a href="#gin-индекс">GIN индекс</a></li> <li><a href="#cluster-индекс">Cluster индекс</a></li> <li><a href="#brin-индекс">BRIN индекс</a></li> </ul></li> <li><a href="#возможности-индексов">Возможности индексов</a><ul> <li><a href="#функциональный-индекс-functional-index">Функциональный индекс (functional index)</a></li> <li><a href="#частичный-индекс-partial-index">Частичный индекс (partial index)</a></li> <li><a href="#уникальный-индекс-unique-index">Уникальный индекс (unique index)</a></li> <li><a href="#индекс-нескольких-столбцов-multi-column-index">Индекс нескольких столбцов (multi-column index)</a></li> </ul></li> </ul></li> <li><a href="#sec:partitioning">Партиционирование</a><ul> <li><a href="#введение-2">Введение</a></li> <li><a href="#теория">Теория</a></li> <li><a href="#практика-использования">Практика использования</a><ul> <li><a href="#настройка">Настройка</a></li> <li><a href="#тестирование">Тестирование</a></li> <li><a href="#управление-партициями">Управление партициями</a></li> <li><a href="#важность-constraint_exclusion-для-партиционирования">Важность «constraint_exclusion» для партиционирования</a></li> </ul></li> <li><a href="#pg_partman">Pg_partman</a><ul> <li><a href="#пример-использования">Пример использования</a></li> </ul></li> <li><a href="#pgslice">Pgslice</a></li> <li><a href="#заключение-1">Заключение</a></li> </ul></li> <li><a href="#репликация">Репликация</a><ul> <li><a href="#введение-3">Введение</a></li> <li><a href="#потоковая-репликация-streaming-replication">Потоковая репликация (Streaming Replication)</a><ul> <li><a href="#установка">Установка</a></li> <li><a href="#настройка-1">Настройка</a></li> <li><a href="#общие-задачи">Общие задачи</a></li> <li><a href="#repmgr">Repmgr</a></li> <li><a href="#patroni">Patroni</a></li> <li><a href="#stolon">Stolon</a></li> </ul></li> <li><a href="#sec:bdr">PostgreSQL Bi-Directional Replication (BDR)</a></li> <li><a href="#sec:pglogical">Pglogical</a><ul> <li><a href="#установка-и-настройка">Установка и настройка</a></li> <li><a href="#разрешение-конфликтов">Разрешение конфликтов</a></li> <li><a href="#ограничения-и-недостатки">Ограничения и недостатки</a></li> </ul></li> <li><a href="#sec:slonyI">Slony-I</a><ul> <li><a href="#установка-1">Установка</a></li> <li><a href="#subsec:slonyI-settings">Настройка</a></li> <li><a href="#общие-задачи-1">Общие задачи</a></li> <li><a href="#устранение-неисправностей">Устранение неисправностей</a></li> </ul></li> <li><a href="#sec:londiste">Londiste</a><ul> <li><a href="#установка-2">Установка</a></li> <li><a href="#настройка-2">Настройка</a></li> <li><a href="#общие-задачи-2">Общие задачи</a></li> <li><a href="#устранение-неисправностей-1">Устранение неисправностей</a></li> </ul></li> <li><a href="#bucardo">Bucardo</a><ul> <li><a href="#установка-3">Установка</a></li> <li><a href="#настройка-3">Настройка</a></li> <li><a href="#общие-задачи-3">Общие задачи</a></li> <li><a href="#репликация-в-другие-типы-баз-данных">Репликация в другие типы баз данных</a></li> </ul></li> <li><a href="#заключение-3">Заключение</a></li> </ul></li> <li><a href="#шардинг">Шардинг</a><ul> <li><a href="#введение-4">Введение</a></li> <li><a href="#sec:plproxy">PL/Proxy</a><ul> <li><a href="#установка-4">Установка</a></li> <li><a href="#настройка-4">Настройка</a></li> <li><a href="#все-ли-так-просто">Все ли так просто?</a></li> </ul></li> <li><a href="#sec:postgres-x2">Postgres-X2</a><ul> <li><a href="#sec:postgres-x2-architecture">Архитектура</a></li> <li><a href="#установка-5">Установка</a></li> <li><a href="#распределение-данных-и-масштабируемость">Распределение данных и масштабируемость</a></li> <li><a href="#таблицы-и-запросы-к-ним">Таблицы и запросы к ним</a></li> <li><a href="#высокая-доступность-ha">Высокая доступность (HA)</a></li> <li><a href="#ограничения">Ограничения</a></li> <li><a href="#заключение-4">Заключение</a></li> </ul></li> <li><a href="#sec:postgres-xl">Postgres-XL</a><ul> <li><a href="#postgres-x2-и-postgres-xl">Postgres-X2 и Postgres-XL</a></li> <li><a href="#заключение-5">Заключение</a></li> </ul></li> <li><a href="#sec:citus">Citus</a><ul> <li><a href="#архитектура">Архитектура</a></li> <li><a href="#установка-6">Установка</a></li> <li><a href="#распределенные-таблицы">Распределенные таблицы</a></li> <li><a href="#ребалансировка-кластера">Ребалансировка кластера</a></li> <li><a href="#ограничения-1">Ограничения</a></li> <li><a href="#заключение-6">Заключение</a></li> </ul></li> <li><a href="#sec:greenplum">Greenplum Database</a><ul> <li><a href="#subsec:greenplum_data_storage">Хранение данных</a></li> <li><a href="#взаимодействие-с-клиентами">Взаимодействие с клиентами</a></li> <li><a href="#надёжность-и-резервирование">Надёжность и резервирование</a></li> <li><a href="#производительность">Производительность</a></li> <li><a href="#расширение-кластера">Расширение кластера</a></li> <li><a href="#особенности-эксплуатации">Особенности эксплуатации</a></li> <li><a href="#заключение-7">Заключение</a></li> </ul></li> <li><a href="#заключение-8">Заключение</a></li> </ul></li> <li><a href="#pgpool-ii">PgPool-II</a><ul> <li><a href="#введение-5">Введение</a></li> <li><a href="#установка-и-настройка-1">Установка и настройка</a><ul> <li><a href="#настройка-5">Настройка</a></li> <li><a href="#настройка-команд-pcp">Настройка команд PCP</a></li> <li><a href="#подготовка-узлов-баз-данных">Подготовка узлов баз данных</a></li> </ul></li> <li><a href="#настройка-репликации-1">Настройка репликации</a><ul> <li><a href="#настройка-репликации-2">Настройка репликации</a></li> <li><a href="#проверка-репликации">Проверка репликации</a></li> </ul></li> <li><a href="#параллельное-выполнение-запросов">Параллельное выполнение запросов</a><ul> <li><a href="#настройка-6">Настройка</a></li> <li><a href="#настройка-systemdb">Настройка SystemDB</a></li> <li><a href="#установка-правил-распределения-данных">Установка правил распределения данных</a></li> <li><a href="#установка-правил-репликации">Установка правил репликации</a></li> <li><a href="#проверка-параллельного-запроса">Проверка параллельного запроса</a></li> </ul></li> <li><a href="#master-slave-режим">Master-slave режим</a><ul> <li><a href="#streaming-replication-потоковая-репликация">Streaming Replication (Потоковая репликация)</a></li> </ul></li> <li><a href="#онлайн-восстановление">Онлайн восстановление</a><ul> <li><a href="#streaming-replication-потоковая-репликация-1">Streaming Replication (Потоковая репликация)</a></li> </ul></li> <li><a href="#заключение-9">Заключение</a></li> </ul></li> <li><a href="#мультиплексоры-соединений">Мультиплексоры соединений</a><ul> <li><a href="#введение-6">Введение</a></li> <li><a href="#pgbouncer">PgBouncer</a></li> <li><a href="#pgpool-ii-vs-pgbouncer">PgPool-II vs PgBouncer</a></li> </ul></li> <li><a href="#кэширование-в-postgresql">Кэширование в PostgreSQL</a><ul> <li><a href="#введение-7">Введение</a></li> <li><a href="#sec:pgmemcache">Pgmemcache</a><ul> <li><a href="#установка-7">Установка</a></li> <li><a href="#настройка-7">Настройка</a></li> <li><a href="#проверка-1">Проверка</a></li> <li><a href="#заключение-10">Заключение</a></li> </ul></li> <li><a href="#заключение-11">Заключение</a></li> </ul></li> <li><a href="#расширения">Расширения</a><ul> <li><a href="#введение-8">Введение</a></li> <li><a href="#postgis">PostGIS</a><ul> <li><a href="#установка-и-использование">Установка и использование</a></li> <li><a href="#заключение-12">Заключение</a></li> </ul></li> <li><a href="#pgsphere">pgSphere</a><ul> <li><a href="#установка-и-использование-1">Установка и использование</a></li> <li><a href="#заключение-13">Заключение</a></li> </ul></li> <li><a href="#sec:hstore-extension">HStore</a><ul> <li><a href="#установка-и-использование-2">Установка и использование</a></li> <li><a href="#заключение-14">Заключение</a></li> </ul></li> <li><a href="#plv8">PLV8</a><ul> <li><a href="#установка-и-использование-3">Установка и использование</a></li> <li><a href="#nosql">NoSQL</a></li> <li><a href="#заключение-15">Заключение</a></li> </ul></li> <li><a href="#pg_repack">Pg_repack</a><ul> <li><a href="#пример-использования-1">Пример использования</a></li> <li><a href="#pgcompact">Pgcompact</a></li> <li><a href="#заключение-16">Заключение</a></li> </ul></li> <li><a href="#pg_prewarm">Pg_prewarm</a><ul> <li><a href="#установка-и-использование-4">Установка и использование</a></li> <li><a href="#заключение-17">Заключение</a></li> </ul></li> <li><a href="#smlar">Smlar</a><ul> <li><a href="#похожесть">Похожесть</a></li> <li><a href="#расчет-похожести">Расчет похожести</a></li> <li><a href="#smlar-1">Smlar</a></li> <li><a href="#пример-поиск-дубликатов-картинок">Пример: поиск дубликатов картинок</a></li> <li><a href="#заключение-18">Заключение</a></li> </ul></li> <li><a href="#multicorn">Multicorn</a><ul> <li><a href="#пример">Пример</a></li> <li><a href="#postgresql-9.3">PostgreSQL 9.3+</a></li> <li><a href="#заключение-19">Заключение</a></li> </ul></li> <li><a href="#pgaudit">Pgaudit</a></li> <li><a href="#ltree">Ltree</a><ul> <li><a href="#почему-ltree">Почему Ltree?</a></li> <li><a href="#установка-и-использование-5">Установка и использование</a></li> <li><a href="#заключение-20">Заключение</a></li> </ul></li> <li><a href="#postpic">PostPic</a></li> <li><a href="#fuzzystrmatch">Fuzzystrmatch</a></li> <li><a href="#pg_trgm">Pg_trgm</a></li> <li><a href="#cstore_fdw">Cstore_fdw</a><ul> <li><a href="#установка-и-использование-6">Установка и использование</a></li> <li><a href="#заключение-21">Заключение</a></li> </ul></li> <li><a href="#postgresql-hll">Postgresql-hll</a><ul> <li><a href="#установка-и-использование-7">Установка и использование</a></li> <li><a href="#заключение-22">Заключение</a></li> </ul></li> <li><a href="#tsearch2">Tsearch2</a><ul> <li><a href="#установка-и-использование-8">Установка и использование</a></li> <li><a href="#заключение-23">Заключение</a></li> </ul></li> <li><a href="#plproxy">PL/Proxy</a></li> <li><a href="#texcaller">Texcaller</a></li> <li><a href="#pgmemcache">Pgmemcache</a></li> <li><a href="#prefix">Prefix</a><ul> <li><a href="#установка-и-использование-9">Установка и использование</a></li> <li><a href="#заключение-24">Заключение</a></li> </ul></li> <li><a href="#dblink">Dblink</a><ul> <li><a href="#установка-и-использование-10">Установка и использование</a></li> <li><a href="#курсоры">Курсоры</a></li> <li><a href="#асинхронные-запросы">Асинхронные запросы</a></li> </ul></li> <li><a href="#postgres_fdw">Postgres_fdw</a><ul> <li><a href="#установка-и-использование-11">Установка и использование</a></li> <li><a href="#postgres_fdw-и-dblink">Postgres_fdw и DBLink</a></li> </ul></li> <li><a href="#pg_cron">Pg_cron</a><ul> <li><a href="#установка-и-использование-12">Установка и использование</a></li> </ul></li> <li><a href="#pgstrom">PGStrom</a></li> <li><a href="#zombodb">ZomboDB</a><ul> <li><a href="#установка-и-использование-13">Установка и использование</a></li> </ul></li> <li><a href="#заключение-25">Заключение</a></li> </ul></li> <li><a href="#бэкап-и-восстановление-postgresql">Бэкап и восстановление PostgreSQL</a><ul> <li><a href="#введение-9">Введение</a></li> <li><a href="#sql-бэкап">SQL бэкап</a><ul> <li><a href="#sql-бэкап-больших-баз-данных">SQL бэкап больших баз данных</a></li> </ul></li> <li><a href="#бэкап-уровня-файловой-системы">Бэкап уровня файловой системы</a></li> <li><a href="#непрерывное-резервное-копирование">Непрерывное резервное копирование</a><ul> <li><a href="#настройка-8">Настройка</a></li> </ul></li> <li><a href="#утилиты-для-непрерывного-резервного-копирования">Утилиты для непрерывного резервного копирования</a><ul> <li><a href="#wal-e">WAL-E</a></li> <li><a href="#barman">Barman</a></li> <li><a href="#pg_arman">Pg_arman</a></li> </ul></li> <li><a href="#заключение-28">Заключение</a></li> </ul></li> <li><a href="#стратегии-масштабирования-для-postgresql">Стратегии масштабирования для PostgreSQL</a><ul> <li><a href="#введение-10">Введение</a><ul> <li><a href="#суть-проблемы">Суть проблемы</a></li> </ul></li> <li><a href="#проблема-чтения-данных">Проблема чтения данных</a><ul> <li><a href="#методы-решения">Методы решения</a></li> </ul></li> <li><a href="#проблема-записи-данных">Проблема записи данных</a><ul> <li><a href="#методы-решения-1">Методы решения</a></li> </ul></li> <li><a href="#заключение-29">Заключение</a></li> </ul></li> <li><a href="#утилиты-для-postgresql">Утилиты для PostgreSQL</a><ul> <li><a href="#введение-11">Введение</a></li> <li><a href="#pgcli">Pgcli</a></li> <li><a href="#pgloader">Pgloader</a></li> <li><a href="#postgres.app">Postgres.app</a></li> <li><a href="#pgadmin">pgAdmin</a></li> <li><a href="#postgrest">PostgREST</a></li> <li><a href="#ngx_postgres">Ngx_postgres</a></li> <li><a href="#заключение-30">Заключение</a></li> </ul></li> <li><a href="#полезные-мелочи">Полезные мелочи</a><ul> <li><a href="#введение-12">Введение</a></li> <li><a href="#мелочи">Мелочи</a><ul> <li><a href="#размер-объектов-в-базе-данных">Размер объектов в базе данных</a></li> <li><a href="#размер-самых-больших-таблиц">Размер самых больших таблиц</a></li> <li><a href="#средний-count">«Средний» count</a></li> <li><a href="#случайное-число-из-диапазона">Случайное число из диапазона</a></li> <li><a href="#алгоритм-луна">Алгоритм Луна</a></li> <li><a href="#выборка-и-сортировка-по-данному-набору-данных">Выборка и сортировка по данному набору данных</a></li> <li><a href="#quine-запрос-который-выводит-сам-себя">Quine — запрос который выводит сам себя</a></li> <li><a href="#поиск-дубликатов-индексов">Поиск дубликатов индексов</a></li> <li><a href="#размер-и-статистика-использования-индексов">Размер и статистика использования индексов</a></li> <li><a href="#sec:snippets-bloating">Размер распухания (bloat) таблиц и индексов в базе данных</a></li> </ul></li> </ul></li> </ul> </nav> <h1 id="введение">Введение</h1> <p>Данная книга не дает ответы на все вопросы по работе с PostgreSQL. Главное её задание — показать возможности PostgreSQL, методики настройки и масштабируемости этой СУБД. В любом случае, выбор метода решения поставленной задачи остается за разработчиком или администратором СУБД.</p> <h2 id="что-такое-postgresql">Что такое PostgreSQL?</h2> <p>PostgreSQL (произносится «Пост-Грес-Кью-Эль») — свободная объектно-реляционная система управления базами данных (СУБД).</p> <p>PostgreSQL ведёт свою «родословную» от некоммерческой СУБД Postgres, разработанной, как и многие open-source проекты, в Калифорнийском университете в Беркли. К разработке Postgres, начавшейся в 1986 году, имел непосредственное отношение Майкл Стоунбрейкер, руководитель более раннего проекта Ingres, на тот момент уже приобретённого компанией Computer Associates. Само название «Postgres» расшифровывалось как «Post Ingres», соответственно, при создании Postgres были применены многие уже ранее сделанные наработки.</p> <p>Стоунбрейкер и его студенты разрабатывали новую СУБД в течение восьми лет с 1986 по 1994 год. За этот период в синтаксис были введены процедуры, правила, пользовательские типы и многие другие компоненты. Работа не прошла даром — в 1995 году разработка снова разделилась: Стоунбрейкер использовал полученный опыт в создании коммерческой СУБД Illustra, продвигаемой его собственной одноимённой компанией (приобретённой впоследствии компанией Informix), а его студенты разработали новую версию Postgres — Postgres95, в которой язык запросов POSTQUEL — наследие Ingres — был заменен на SQL.</p> <p>В этот момент разработка Postgres95 была выведена за пределы университета и передана команде энтузиастов. С этого момента СУБД получила имя, под которым она известна и развивается в текущий момент — PostgreSQL.</p> <p>На данный момент, в PostgreSQL имеются следующие ограничения:</p> <table> <thead> <tr class=header> <th style="text-align: left;">Максимальный размер базы данных</th> <th style="text-align: left;">Нет ограничений</th> </tr> </thead> <tbody> <tr class=odd> <td style="text-align: left;">Максимальный размер таблицы</td> <td style="text-align: left;">32 Тбайт</td> </tr> <tr class=even> <td style="text-align: left;">Максимальный размер записи</td> <td style="text-align: left;">1,6 Тбайт</td> </tr> <tr class=odd> <td style="text-align: left;">Максимальный размер поля</td> <td style="text-align: left;">1 Гбайт</td> </tr> <tr class=even> <td style="text-align: left;">Максимум записей в таблице</td> <td style="text-align: left;">Нет ограничений</td> </tr> <tr class=odd> <td style="text-align: left;">Максимум полей в записи</td> <td style="text-align: left;">250—1600, в зависимости от типов полей</td> </tr> <tr class=even> <td style="text-align: left;">Максимум индексов в таблице</td> <td style="text-align: left;">Нет ограничений</td> </tr> </tbody> </table> <p>Согласно <a href="http://www.postgresql.org/about/news/363/">результатам</a> автоматизированного исследования различного ПО на предмет ошибок, в исходном коде PostgreSQL было найдено 20 проблемных мест на 775000 строк исходного кода (в среднем, одна ошибка на 39000 строк кода). Для сравнения: MySQL — 97 проблем, одна ошибка на 4000 строк кода; FreeBSD (целиком) — 306 проблем, одна ошибка на 4000 строк кода; Linux (только ядро) — 950 проблем, одна ошибка на 10 000 строк кода.</p> <h1 id="настройка-производительности">Настройка производительности</h1> <h2 id="введение-1">Введение</h2> <p>Скорость работы, вообще говоря, не является основной причиной использования реляционных СУБД. Более того, первые реляционные базы работали медленнее своих предшественников. Выбор этой технологии был вызван скорее:</p> <ul> <li><p>возможностью возложить поддержку целостности данных на СУБД;</p></li> <li><p>независимостью логической структуры данных от физической;</p></li> </ul> <p>Эти особенности позволяют сильно упростить написание приложений, но требуют для своей реализации дополнительных ресурсов.</p> <p>Таким образом, прежде чем искать ответ на вопрос «как заставить РСУБД работать быстрее в моей задаче?», следует ответить на вопрос «нет ли более подходящего средства для решения моей задачи, чем РСУБД?» Иногда использование другого средства потребует меньше усилий, чем настройка производительности.</p> <p>Данная глава посвящена возможностям повышения производительности PostgreSQL. Глава не претендует на исчерпывающее изложение вопроса, наиболее полным и точным руководством по использованию PostgreSQL является, конечно, <a href="http://www.postgresql.org/docs/manuals/">официальная документация</a> и <a href="https://wiki.postgresql.org/wiki/Category:FAQ">официальный FAQ</a>. Также существует англоязычный список рассылки postgresql-performance, посвящённый именно этим вопросам. Глава состоит из двух разделов, первый из которых ориентирован скорее на администратора, второй — на разработчика приложений. Рекомендуется прочесть оба раздела: отнесение многих вопросов к какому-то одному из них весьма условно.</p> <h3 id="не-используйте-настройки-по-умолчанию">Не используйте настройки по умолчанию</h3> <p>По умолчанию PostgreSQL сконфигурирован таким образом, чтобы он мог быть запущен практически на любом компьютере и не слишком мешал при этом работе других приложений. Это особенно касается используемой памяти. Настройки по умолчанию подходят только для следующего использования: с ними вы сможете проверить, работает ли установка PostgreSQL, создать тестовую базу уровня записной книжки и потренироваться писать к ней запросы. Если вы собираетесь разрабатывать (а тем более запускать в работу) реальные приложения, то настройки придётся радикально изменить. В дистрибутиве PostgreSQL, к сожалению, не поставляется файлов с «рекомендуемыми» настройками. Вообще говоря, такие файлы создать весьма сложно, т.к. оптимальные настройки конкретной установки PostgreSQL будут определяться:</p> <ul> <li><p>конфигурацией компьютера;</p></li> <li><p>объёмом и типом данных, хранящихся в базе;</p></li> <li><p>отношением числа запросов на чтение и на запись;</p></li> <li><p>тем, запущены ли другие требовательные к ресурсам процессы (например, веб-сервер);</p></li> </ul> <h3 id="используйте-актуальную-версию-сервера">Используйте актуальную версию сервера</h3> <p>Если у вас стоит устаревшая версия PostgreSQL, то наибольшего ускорения работы вы сможете добиться, обновив её до текущей. Укажем лишь наиболее значительные из связанных с производительностью изменений.</p> <ul> <li><p>В версии 7.4 была ускорена работа многих сложных запросов (включая печально известные подзапросы IN/NOT IN);</p></li> <li><p>В версии 8.0 были внедрены метки восстановления, улучшение управления буфером, CHECKPOINT и VACUUM улучшены;</p></li> <li><p>В версии 8.1 был улучшен одновременный доступ к разделяемой памяти, автоматическое использование индексов для MIN() и MAX(), pg_autovacuum внедрен в сервер (автоматизирован), повышение производительности для секционированных таблиц;</p></li> <li><p>В версии 8.2 была улучшена скорость множества SQL запросов, усовершенствован сам язык запросов;</p></li> <li><p>В версии 8.3 внедрен полнотекстовый поиск, поддержка SQL/XML стандарта, параметры конфигурации сервера могут быть установлены на основе отдельных функций;</p></li> <li><p>В версии 8.4 были внедрены общие табличные выражения, рекурсивные запросы, параллельное восстановление, улучшена производительность для EXISTS/NOT EXISTS запросов;</p></li> <li><p>В версии 9.0 «асинхронная репликация из коробки», VACUUM/VACUUM FULL стали быстрее, расширены хранимые процедуры;</p></li> <li><p>В версии 9.1 «синхронная репликация из коробки», нелогируемые таблицы (очень быстрые на запись, но при падении БД данные могут пропасть), новые типы индексов, наследование таблиц в запросах теперь может вернуться многозначительно отсортированные результаты, позволяющие оптимизации MIN/MAX;</p></li> <li><p>В версии 9.2 «каскадная репликация из коробки», сканирование по индексу, JSON тип данных, типы данных на диапазоны, сортировка в памяти улучшена на 25%, ускорена команда COPY;</p></li> <li><p>В версии 9.3 materialized view, доступные на запись внешние таблицы, переход с использования SysV shared memory на POSIX shared memory и mmap, cокращено время распространения реплик, а также значительно ускорена передача управления от запасного сервера к первичному, увеличена производительность и улучшена система блокировок для внешних ключей;</p></li> <li><p>В версии 9.4 появился новый тип поля JSONB (бинарный JSON с поддержкой индексов), логическое декодирование для репликации, GIN индекс в 2 раза меньше по размеру и в 3 раза быстрее, неблокируюшие обновление materialized view, поддержка Linux Huge Pages;</p></li> <li><p>В версии 9.5 добавленна поддержка UPSERT, Row Level Security, CUBE, ROLLUP, GROUPING SETS функции, TABLESAMPLE, BRIN индекс, ускорена скорость работы индексов для текстовых и цифровых полей;</p></li> <li><p>В версии 9.6 добавленна поддержка параллелизации некоторых запросов, что позволяет использование несколько ядер (CPU core) на сервере, чтобы возвращать результаты запроса быстрее, полнотекстовый поиск поддерживается фразы, новая опция «remote_apply» для синхронной репликации, которая позволяет дождатся, пока запрос завершится на слейве;</p></li> </ul> <p>Следует также отметить, что большая часть изложенного в статье материала относится к версии сервера не ниже 9.0.</p> <h3 id="стоит-ли-доверять-тестам-производительности">Стоит ли доверять тестам производительности</h3> <p>Перед тем, как заниматься настройкой сервера, вполне естественно ознакомиться с опубликованными данными по производительности, в том числе в сравнении с другими СУБД. К сожалению, многие тесты служат не столько для облегчения вашего выбора, сколько для продвижения конкретных продуктов в качестве «самых быстрых». При изучении опубликованных тестов в первую очередь обратите внимание, соответствует ли величина и тип нагрузки, объём данных и сложность запросов в тесте тому, что вы собираетесь делать с базой? Пусть, например, обычное использование вашего приложения подразумевает несколько одновременно работающих запросов на обновление к таблице в миллионы записей. В этом случае СУБД, которая в несколько раз быстрее всех остальных ищет запись в таблице в тысячу записей, может оказаться не лучшим выбором. Ну и наконец, вещи, которые должны сразу насторожить:</p> <ul> <li><p>Тестирование устаревшей версии СУБД;</p></li> <li><p>Использование настроек по умолчанию (или отсутствие информации о настройках);</p></li> <li><p>Тестирование в однопользовательском режиме (если, конечно, вы не предполагаете использовать СУБД именно так);</p></li> <li><p>Использование расширенных возможностей одной СУБД при игнорировании расширенных возможностей другой;</p></li> <li><p>Использование заведомо медленно работающих запросов (раздел «[sec:pg-optimize-sql] »);</p></li> </ul> <h2 id="настройка-сервера">Настройка сервера</h2> <p>В этом разделе описаны рекомендуемые значения параметров, влияющих на производительность СУБД. Эти параметры обычно устанавливаются в конфигурационном файле postgresql.conf и влияют на все базы в текущей установке.</p> <h3 id="используемая-память">Используемая память</h3> <h4 id="общий-буфер-сервера-shared_buffers">Общий буфер сервера: shared_buffers</h4> <p>PostgreSQL не читает данные напрямую с диска и не пишет их сразу на диск. Данные загружаются в общий буфер сервера, находящийся в разделяемой памяти, серверные процессы читают и пишут блоки в этом буфере, а затем уже изменения сбрасываются на диск.</p> <p>Если процессу нужен доступ к таблице, то он сначала ищет нужные блоки в общем буфере. Если блоки присутствуют, то он может продолжать работу, если нет — делается системный вызов для их загрузки. Загружаться блоки могут как из файлового кэша ОС, так и с диска, и эта операция может оказаться весьма «дорогой».</p> <p>Если объём буфера недостаточен для хранения часто используемых рабочих данных, то они будут постоянно писаться и читаться из кэша ОС или с диска, что крайне отрицательно скажется на производительности.</p> <p>В то же время не следует устанавливать это значение слишком большим: это НЕ вся память, которая нужна для работы PostgreSQL, это только размер разделяемой между процессами PostgreSQL памяти, которая нужна для выполнения активных операций. Она должна занимать меньшую часть оперативной памяти вашего компьютера, так как PostgreSQL полагается на то, что операционная система кэширует файлы, и не старается дублировать эту работу. Кроме того, чем больше памяти будет отдано под буфер, тем меньше останется операционной системе и другим приложениям, что может привести к своппингу.</p> <p>К сожалению, чтобы знать точное число <code>shared_buffers</code>, нужно учесть количество оперативной памяти компьютера, размер базы данных, число соединений и сложность запросов, так что лучше воспользуемся несколькими простыми правилами настройки.</p> <p>На выделенных серверах полезным объемом для <code>shared_buffers</code> будет значение 1/4 памяти в системе. Если у вас большие активные порции базы данных, сложные запросы, большое число одновременных соединений, длительные транзакции, вам доступен большой объем оперативной памяти или большее количество процессоров, то можно подымать это значение и мониторить результат, чтобы не привести к «деградации» (падению) производительности. Выделив слишком много памяти для базы данных, мы можем получить ухудшение производительности, поскольку PostgreSQL также использует кэш операционной системы (увеличение данного параметра более 40% оперативной памяти может давать «нулевой» прирост производительности).</p> <p>Для тонкой настройки параметра установите для него большое значение и потестируйте базу при обычной нагрузке. Проверяйте использование разделяемой памяти при помощи <code>ipcs</code> или других утилит(например, <code>free</code> или <code>vmstat</code>). Рекомендуемое значение параметра будет примерно в 1,2 –2 раза больше, чем максимум использованной памяти. Обратите внимание, что память под буфер выделяется при запуске сервера, и её объём при работе не изменяется. Учтите также, что настройки ядра операционной системы могут не дать вам выделить большой объём памяти (для версии PostgreSQL &lt; 9.3). В <a href="http://www.postgresql.org/docs/current/static/kernel-resources.html">руководстве администратора PostgreSQL</a> описано, как можно изменить эти настройки.</p> <p>Также следует помнить, что на 32 битной системе (Linux) каждый процесс лимитирован в 4 ГБ адресного пространства, где хотя бы 1 ГБ зарезервирован ядром. Это означает, что не зависимо, сколько на машине памяти, каждый PostgreSQL инстанс сможет обратится максимум к 3 ГБ памяти. А значит максимум для <code>shared_buffers</code> в такой системе — 2–2.5 ГБ.</p> <p>Хочу обратить внимание, что на Windows, большие значения для <code>shared_buffers</code> не столь эффективны, как на Linux системах, и в результате лучшие результаты можно будет получить, если держать это значение относительно небольшое (от 64 МБ до 512 МБ) и использовать кэш системы вместо него.</p> <h4 id="память-для-сортировки-результата-запроса-work_mem">Память для сортировки результата запроса: work_mem</h4> <p><code>work_mem</code> параметр определяет максимальное количество оперативной памяти, которое может выделить одна операция сортировки, агрегации и др. Это не разделяемая память, <code>work_mem</code> выделяется отдельно на каждую операцию (от одного до нескольких раз за один запрос). Разумное значение параметра определяется следующим образом: количество доступной оперативной памяти (после того, как из общего объема вычли память, требуемую для других приложений, и <code>shared_buffers</code>) делится на максимальное число одновременных запросов умноженное на среднее число операций в запросе, которые требуют памяти.</p> <p>Если объём памяти недостаточен для сортировки некоторого результата, то серверный процесс будет использовать временные файлы. Если же объём памяти слишком велик, то это может привести к своппингу.</p> <p>Объём памяти задаётся параметром <code>work_mem</code> в файле postgresql.conf. Единица измерения параметра — 1 кБ. Значение по умолчанию — 1024. В качестве начального значения для параметра можете взять 2–4% доступной памяти. Для веб-приложений обычно устанавливают низкие значения <code>work_mem</code>, так как запросов обычно много, но они простые, обычно хватает от 512 до 2048 КБ. С другой стороны, приложения для поддержки принятия решений с сотнями строк в каждом запросе и десятками миллионов столбцов в таблицах фактов часто требуют <code>work_mem</code> порядка 500 МБ. Для баз данных, которые используются и так, и так, этот параметр можно устанавливать для каждого запроса индивидуально, используя настройки сессии. Например, при памяти 1–4 ГБ рекомендуется устанавливать 32–128 MB.</p> <h4 id="максимальное-количество-клиентов-max_connections">Максимальное количество клиентов: max_connections</h4> <p>Параметр <code>max_connections</code> устанавливает максимальное количество клиентов, которые могут подключится к PostgreSQL. Поскольку для каждого клиента требуется выделять память (<code>work_mem</code>), то этот параметр предполагает максимально возможное использование памяти для всех клиентов. Как правило, PostgreSQL может поддерживать несколько сотен подключений, но создание нового является дорогостоящей операцией. Поэтому, если требуется тысячи подключений, то лучше использовать пул подключений (отдельная программа или библиотека для продукта, что использует базу).</p> <h4 id="память-для-работы-команды-vacuum-maintenance_work_mem">Память для работы команды VACUUM: maintenance_work_mem</h4> <p>Этот параметр задаёт объём памяти, используемый командами <code>VACUUM</code>, <code>ANALYZE</code>, <code>CREATE INDEX</code>, и добавления внешних ключей. Чтобы операции выполнялись максимально быстро, нужно устанавливать этот параметр тем выше, чем больше размер таблиц в вашей базе данных. Неплохо бы устанавливать его значение от 50 до 75% размера вашей самой большой таблицы или индекса или, если точно определить невозможно, от 32 до 256 МБ. Следует устанавливать большее значение, чем для <code>work_mem</code>. Слишком большие значения приведут к использованию свопа. Например, при памяти 1–4 ГБ рекомендуется устанавливать 128–512 MB.</p> <h4 id="большие-страницы-huge_pages">Большие страницы: huge_pages</h4> <p>В PostgreSQL начиная с версии 9.4 появилась поддержка больших страниц. В ОС Linux работа с памятью основывается на обращении к страницам размер которых равен 4kB (на самом деле зависит от платформы, проверить можно через <code>getconf PAGE_SIZE</code>). Так вот, когда объем памяти переваливает за несколько десятков, а то и сотни гигабайт управлять ею становится сложнее, увеличиваются накладные расходы на адресацию памяти и поддержание страничных таблиц. Для облегчения жизни и были придуманы большие страницы, размер которых может быть 2MB, а то и 1GB. За счет использования больших страниц можно получить ощутимый прирост скорости работы и увеличение отзывчивости в приложениях которые активно работают с памятью.</p> <p>Вобще запустить PostgreSQL с поддержкой больших страниц можно было и раньше, с помощью <a href="http://www.thislinux.org/2012/08/postgresql-with-hugepages.html">libhugetlbfs</a>. Однако теперь есть встроенная поддержка. Итак ниже описание процесса как настроить и запустить PostgreSQL с поддержкой больших страниц.</p> <p>Для начала следует убедиться что ядро поддерживает большие страницы. Проверяем конфиг ядра на предмет наличия опций <code>CONFIG_HUGETLBFS</code> и <code>CONFIG_HUGETLB_PAGE</code>.</p> <pre><code>$ grep HUGETLB /boot/config-$(uname -r)
CONFIG_CGROUP_HUGETLB=y
CONFIG_HUGETLBFS=y
CONFIG_HUGETLB_PAGE=y</code></pre> <p>В случае отсутствия этих опций, ничего не заработает и ядро следует пересобрать.</p> <p>Очевидно что нам понадобится PostgreSQL версии не ниже 9.4. За поддержку больших страниц отвечает параметр <code>huge_page</code> который может принимать три значения: <code>off</code> — не использовать большие страницы, <code>on</code> — использовать большие страницы, <code>try</code> — попытаться использовать большие страницы и в случае недоступности откатиться на использование обычных страниц. Значение <code>try</code> используется по умолчанию и является безопасным вариантом. В случае <code>on</code>, PostgreSQL не запустится, если большие страницы не определены в системе (или их недостаточно).</p> <p>После перезапуска базы с параметром <code>try</code> потребуется включить поддержку больших страниц в системе (по умолчанию они не задействованы). Расчет страниц приблизительный и здесь следует опираться на то сколько памяти вы готовы выделить под нужды СУБД. Отмечу что значение измеряется в страницах размером 2Mb, если вы хотите выделить 16GB, то это будет 8000 страниц.</p> <p>Официальная документация предлагает опираться на значение <code>VmPeak</code> из status файла который размещен в <code>/proc/PID/</code> директории который соответствует номеру процесса postmaster. <code>VmPeak</code> как следует из названия это пиковое значение использования виртуальной памяти. Этот вариант позволяет определить минимальную планку от которой следует отталкиваться, но на мой вгляд такой способ определения тоже носит случайный характер.</p> <pre><code>$ head -1 /var/lib/pgsql/9.5/data/postmaster.pid
3076
$ grep ^VmPeak /proc/3076/status
VmPeak:  4742563 kB
$ echo $((4742563 / 2048 + 1))
2316
$ echo &#39;vm.nr_hugepages = 2316&#39; » /etc/sysctl.d/30-postgresql.conf
$ sysctl -p --system</code></pre> <p>В ОС Linux есть также система по менеджменту памяти под названием «Transparent HugePages», которая включена по умолчанию. Она может вызывать проблему при работе с huge pages для PostgreSQL, поэтому рекомендуется выключать этот механизм:</p> <pre><code>$ echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag
$ echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</code></pre> <p>После этого перезапускаем PostgreSQL и смотрим использование больших страниц:</p> <pre><code>$ grep ^HugePages /proc/meminfo
HugePages_Total:    2316
HugePages_Free:     2301
HugePages_Rsvd:      128
HugePages_Surp:        0</code></pre> <h4 id="прочие-настройки">Прочие настройки</h4> <ul> <li><p><code>temp_buffers</code> — буфер под временные объекты, в основном для временных таблиц. Можно установить порядка 16 МБ;</p></li> <li><p><code>max_prepared_transactions</code> — количество одновременно подготавливаемых транзакций (PREPARE TRANSACTION). Можно оставить по умолчанию — 5;</p></li> <li><p><code>vacuum_cost_delay</code> — если у вас большие таблицы, и производится много одновременных операций записи, вам может пригодиться функция, которая уменьшает затраты на I/O для VACUUM, растягивая его по времени. Чтобы включить эту функциональность, нужно поднять значение <code>vacuum_cost_delay</code> выше 0. Используйте разумную задержку от 50 до 200 мс. Для более тонкой настройки повышайте <code>vacuum_cost_page_hit</code> и понижайте <code>vacuum_cost_page_limit</code>. Это ослабит влияние VACUUM, увеличив время его выполнения. В тестах с параллельными транзакциями Ян Вик (Jan Wieck) получил, что при значениях <code>delay</code> — 200, <code>page_hit</code> — 6 и <code>limit</code> —100 влияние VACUUM уменьшилось более чем на 80%, но его длительность увеличилась втрое;</p></li> <li><p><code>max_stack_depth</code> — cпециальный стек для сервера, в идеале он должен совпадать с размером стека, выставленном в ядре ОС. Установка большего значения, чем в ядре, может привести к ошибкам. Рекомендуется устанавливать 2–4 MB;</p></li> <li><p><code>max_files_per_process</code> — максимальное количество файлов, открываемых процессом и его подпроцессами в один момент времени. Уменьшите данный параметр, если в процессе работы наблюдается сообщение «Too many open files»;</p></li> </ul> <h3 id="журнал-транзакций-и-контрольные-точки">Журнал транзакций и контрольные точки</h3> <p>Для обеспечения отказоустойчивости СУБД PostgreSQL, как и многие базы данных, использует специальный журнал, в котором ведет историю изменения данных. Перед тем как записать данные в файлы БД, сервер PostgreSQL аккумулирует изменения в оперативной памяти и записывает в последовательный файл журнала, чтобы не потерять их из-за непредвиденного отключения питания.</p> <p>Данные в журнал пишутся до того как пользователь базы данных получит сообщение об успешном применении изменений. Этот журнал называется журналом упреждающей записи (Write-Ahead Log или просто WAL), а файлы журнала хранятся в каталоге <code>pg_xlog</code>. Также периодически PostgreSQL сбрасывает измененные аккумулированные данные из оперативной памяти на диск. Этот процесс согласования данных называется контрольной точкой (<code>checkpoint</code>). Контрольная точка выполняется также при каждом штатном выключении PostgreSQL.</p> <p>В этом случае нет необходимости сбрасывать на диск изменения данных при каждом успешном завершении транзакции: в случае сбоя БД может быть восстановлена по записям в журнале. Таким образом, данные из буферов сбрасываются на диск при проходе контрольной точки: либо при заполнении нескольких (параметр <code>checkpoint_segments</code>, по умолчанию 3) сегментов журнала транзакций, либо через определённый интервал времени (параметр <code>checkpoint_timeout</code>, измеряется в секундах, по умолчанию 300).</p> <p>Изменение этих параметров прямо не повлияет на скорость чтения, но может принести большую пользу, если данные в базе активно изменяются.</p> <h4 id="уменьшение-количества-контрольных-точек-checkpoint_segments">Уменьшение количества контрольных точек: checkpoint_segments</h4> <p>Если в базу заносятся большие объёмы данных, то контрольные точки могут происходить слишком часто («слишком часто» можно определить как «чаще раза в минуту». Вы также можете задать параметр <code>checkpoint_warning</code> (в секундах): в журнал сервера будут писаться предупреждения, если контрольные точки происходят чаще заданного). При этом производительность упадёт из-за постоянного сбрасывания на диск данных из буфера.</p> <p>Для увеличения интервала между контрольными точками нужно увеличить количество сегментов журнала транзакций через параметр <code>checkpoint_segments</code>. Данный параметр определяет количество сегментов (каждый по 16 МБ) лога транзакций между контрольными точками. Этот параметр не имеет особого значения для базы данных, предназначенной преимущественно для чтения, но для баз данных со множеством транзакций увеличение этого параметра может оказаться жизненно необходимым. В зависимости от объема данных установите этот параметр в диапазоне от 12 до 256 сегментов и, если в логе появляются предупреждения (warning) о том, что контрольные точки происходят слишком часто, постепенно увеличивайте его. Место, требуемое на диске, вычисляется по формуле <code>(checkpoint_segments * (2 + checkpoint_completion_target) + 1) * 16</code> МБ, так что убедитесь, что у вас достаточно свободного места. Например, если вы выставите значение 32, вам потребуется больше 1 ГБ дискового пространства.</p> <p>Следует также отметить, что чем больше интервал между контрольными точками, тем дольше будут восстанавливаться данные по журналу транзакций после сбоя.</p> <p>Начиная с версии 9.5 <code>checkpoint_segments</code> был заменен на параметры <code>min_wal_size</code> и <code>max_wal_size</code>. Теперь система может автоматически сама решать сколько <code>checkpoint_segments</code> требуется хранить (вычислять по ранее приведенной формуле от указанного размера). Преимуществом этого является то, что вы можете установить <code>max_wal_size</code> очень большим, но система не будет на самом деле потребляют указанное количество места на жестком диске, если в этом нет никакой необходимости. <code>min_wal_size</code> устанавливает минимальный размер места, который будет использоватся сегментами (можно отключить такую автонастройку установив для <code>min_wal_size</code> и <code>max_wal_size</code> одинаковое значение).</p> <h4 id="fsync-synchronous_commit-и-стоит-ли-их-трогать">fsync, synchronous_commit и стоит ли их трогать</h4> <p>Для увеличение производительности наиболее радикальное из возможных решений — выставить значение «off» параметру <code>fsync</code>. При этом записи в журнале транзакций не будут принудительно сбрасываться на диск, что даст большой прирост скорости записи. Учтите: вы жертвуете надёжностью, в случае сбоя целостность базы будет нарушена, и её придётся восстанавливать из резервной копии! Использовать этот параметр рекомендуется лишь в том случае, если вы всецело доверяете своему «железу» и своему источнику бесперебойного питания. Ну или если данные в базе не представляют для вас особой ценности.</p> <p>Параметр <code>synchronous_commit</code> определяет нужно ли ждать WAL записи на диск перед возвратом успешного завершения транзакции для подключенного клиента. По умолчанию и для безопасности данный параметр установлен в «on» (включен). При выключении данного параметра («off») может существовать задержка между моментом, когда клиенту будет сообщенно об успехе транзакции и когда та самая транзакция действительно гарантированно и безопастно записана на диск (максимальная задержка — <code>wal_writer_delay * 3</code>). В отличие от <code>fsync</code>, отключение этого параметра не создает риск краха базы данных: данные могут быть потеряны (последний набор транзакций), но базу данных не придется восстанавливать после сбоя из бэкапа. Так что <code>synchronous_commit</code> может быть полезной альтернативой, когда производительность важнее, чем точная уверенность в согласовании данных (данный режим можно назвать «режимом MongoDB»: изначально все клиенты для MongoDB не проверяли успешность записи данных в базу и за счет этого достигалась хорошая скорость для бенчмарков).</p> <h4 id="прочие-настройки-1">Прочие настройки</h4> <ul> <li><p><code>commit_delay</code> (в микросекундах, 0 по умолчанию) и <code>commit_siblings</code> (5 по умолчанию) определяют задержку между попаданием записи в буфер журнала транзакций и сбросом её на диск. Если при успешном завершении транзакции активно не менее <code>commit_siblings</code> транзакций, то запись будет задержана на время <code>commit_delay</code>. Если за это время завершится другая транзакция, то их изменения будут сброшены на диск вместе, при помощи одного системного вызова. Эти параметры позволят ускорить работу, если параллельно выполняется много «мелких» транзакций;</p></li> <li><p><code>wal_sync_method</code> Метод, который используется для принудительной записи данных на диск. Если <code>fsync=off</code>, то этот параметр не используется. Возможные значения:</p> <ul> <li><p><code>open_datasync</code> — запись данных методом open() с параметром <code>O_DSYNC</code>;</p></li> <li><p><code>fdatasync</code> — вызов метода fdatasync() после каждого commit;</p></li> <li><p><code>fsync_writethrough</code> — вызов fsync() после каждого commit, игнорируя параллельные процессы;</p></li> <li><p><code>fsync</code> — вызов fsync() после каждого commit;</p></li> <li><p><code>open_sync</code> — запись данных методом open() с параметром <code>O_SYNC</code>;</p></li> </ul> <p>Не все эти методы доступны на разных ОС. По умолчанию устанавливается первый, который доступен для системы;</p></li> <li><p><code>full_page_writes</code> Установите данный параметр в «off», если <code>fsync=off</code>. Иначе, когда этот параметр «on», PostgreSQL записывает содержимое каждой записи в журнал транзакций при первой модификации таблицы. Это необходимо, поскольку данные могут записаться лишь частично, если в ходе процесса «упала» ОС. Это приведет к тому, что на диске окажутся новые данные смешанные со старыми. Строкового уровня записи в журнал транзакций может быть недостаточно, чтобы полностью восстановить данные после «падения». <code>full_page_writes</code> гарантирует корректное восстановление, ценой увеличения записываемых данных в журнал транзакций (Единственный способ снижения объема записи в журнал транзакций заключается в увеличении <code>checkpoint_interval</code>);</p></li> <li><p><code>wal_buffers</code> Количество памяти используемое в Shared Memory для ведения транзакционных логов (буфер находится в разделяемой памяти и является общим для всех процессов). Стоит увеличить буфер до 256–512 кБ, что позволит лучше работать с большими транзакциями. Например, при доступной памяти 1–4 ГБ рекомендуется устанавливать 256–1024 КБ;</p></li> </ul> <h3 id="планировщик-запросов">Планировщик запросов</h3> <p>Следующие настройки помогают планировщику запросов правильно оценивать стоимости различных операций и выбирать оптимальный план выполнения запроса. Существуют 3 настройки планировщика, на которые стоит обратить внимание:</p> <ul> <li><p><code>default_statistics_target</code> — этот параметр задаёт объём статистики, собираемой командой <code>ANALYZE</code>. Увеличение параметра заставит эту команду работать дольше, но может позволить оптимизатору строить более быстрые планы, используя полученные дополнительные данные. Объём статистики для конкретного поля может быть задан командой <code>ALTER TABLE ... SET STATISTICS</code>;</p></li> <li><p><code>effective_cache_size</code> — этот параметр сообщает PostgreSQL примерный объём файлового кэша операционной системы, оптимизатор использует эту оценку для построения плана запроса (указывает планировщику на размер самого большого объекта в базе данных, который теоретически может быть закеширован). Пусть в вашем компьютере 1.5 ГБ памяти, параметр <code>shared_buffers</code> установлен в 32 МБ, а параметр <code>effective_cache_size</code> в 800 МБ. Если запросу нужно 700 МБ данных, то PostgreSQL оценит, что все нужные данные уже есть в памяти и выберет более агрессивный план с использованием индексов и merge joins. Но если <code>effective_cache_size</code> будет всего 200 МБ, то оптимизатор вполне может выбрать более эффективный для дисковой системы план, включающий полный просмотр таблицы.</p> <p>На выделенном сервере имеет смысл выставлять <code>effective_cache_size</code> в 2/3 от всей оперативной памяти; на сервере с другими приложениями сначала нужно вычесть из всего объема RAM размер дискового кэша ОС и память, занятую остальными процессами;</p></li> <li><p><code>random_page_cost</code> — переменная, указывающая на условную стоимость индексного доступа к страницам данных. На серверах с быстрыми дисковыми массивами имеет смысл уменьшать изначальную настройку до 3.0, 2.5 или даже до 2.0. Если же активная часть вашей базы данных намного больше размеров оперативной памяти, попробуйте поднять значение параметра. Можно подойти к выбору оптимального значения и со стороны производительности запросов. Если планировщик запросов чаще, чем необходимо, предпочитает последовательные просмотры (sequential scans) просмотрам с использованием индекса (index scans), понижайте значение. И наоборот, если планировщик выбирает просмотр по медленному индексу, когда не должен этого делать, настройку имеет смысл увеличить. После изменения тщательно тестируйте результаты на максимально широком наборе запросов. Никогда не опускайте значение <code>random_page_cost</code> ниже 2.0; если вам кажется, что <code>random_page_cost</code> нужно еще понижать, разумнее в этом случае менять настройки статистики планировщика.</p></li> </ul> <h3 id="сбор-статистики">Сбор статистики</h3> <p>У PostgreSQL также есть специальная подсистема — сборщик статистики, — которая в реальном времени собирает данные об активности сервера. Поскольку сбор статистики создает дополнительные накладные расходы на базу данных, то система может быть настроена как на сбор, так и не сбор статистики вообще. Эта система контролируется следующими параметрами, принимающими значения <code>true/false</code>:</p> <ul> <li><p><code>track_counts</code> — включать ли сбор статистики. По умолчанию включён, поскольку autovacuum демону требуется сбор статистики. Отключайте, только если статистика вас совершенно не интересует (как и autovacuum);</p></li> <li><p><code>track_functions</code> — отслеживание использования определенных пользователем функций;</p></li> <li><p><code>track_activities</code> — передавать ли сборщику статистики информацию о текущей выполняемой команде и времени начала её выполнения. По умолчанию эта возможность включена. Следует отметить, что эта информация будет доступна только привилегированным пользователям и пользователям, от лица которых запущены команды, так что проблем с безопасностью быть не должно;</p></li> </ul> <p>Данные, полученные сборщиком статистики, доступны через специальные системные представления. При установках по умолчанию собирается очень мало информации, рекомендуется включить все возможности: дополнительная нагрузка будет невелика, в то время как полученные данные позволят оптимизировать использование индексов (а также помогут оптимальной работе autovacuum демону).</p> <h2 id="sec:hard-drive-and-file-systems">Диски и файловые системы</h2> <p>Очевидно, что от качественной дисковой подсистемы в сервере БД зависит немалая часть производительности. Вопросы выбора и тонкой настройки «железа», впрочем, не являются темой данной главы, ограничимся уровнем файловой системы.</p> <p>Единого мнения насчёт наиболее подходящей для PostgreSQL файловой системы нет, поэтому рекомендуется использовать ту, которая лучше всего поддерживается вашей операционной системой. При этом учтите, что современные журналирующие файловые системы не намного медленнее нежурналирующих, а выигрыш — быстрое восстановление после сбоев — от их использования велик.</p> <p>Вы легко можете получить выигрыш в производительности без побочных эффектов, если примонтируете файловую систему, содержащую базу данных, с параметром <code>noatime</code> (но при этом не будет отслеживаться время последнего доступа к файлу).</p> <h3 id="перенос-журнала-транзакций-на-отдельный-диск">Перенос журнала транзакций на отдельный диск</h3> <p>При доступе к диску изрядное время занимает не только собственно чтение данных, но и перемещение магнитной головки.</p> <p>Если в вашем сервере есть несколько физических дисков (несколько логических разделов на одном диске здесь, очевидно, не помогут: головка всё равно будет одна), то вы можете разнести файлы базы данных и журнал транзакций по разным дискам. Данные в сегменты журнала пишутся последовательно, более того, записи в журнале транзакций сразу сбрасываются на диск, поэтому в случае нахождения его на отдельном диске магнитная головка не будет лишний раз двигаться, что позволит ускорить запись.</p> <p>Порядок действий:</p> <ul> <li><p>Остановите сервер (!);</p></li> <li><p>Перенесите каталоги <code>pg_clog</code> и <code>pg_xlog</code>, находящийся в каталоге с базами данных, на другой диск;</p></li> <li><p>Создайте на старом месте символическую ссылку;</p></li> <li><p>Запустите сервер;</p></li> </ul> <p>Примерно таким же образом можно перенести и часть файлов, содержащих таблицы и индексы, на другой диск, но здесь потребуется больше кропотливой ручной работы, а при внесении изменений в схему базы процедуру, возможно, придётся повторить.</p> <h3 id="sec:hard-drive-cluster">CLUSTER</h3> <p><code>CLUSTER table [ USING index ]</code> — команда для упорядочивания записей таблицы на диске согласно индексу, что иногда за счет уменьшения доступа к диску ускоряет выполнение запроса. Возможно создать только один физический порядок в таблице, поэтому и таблица может иметь только один кластерный индекс. При таком условии нужно тщательно выбирать, какой индекс будет использоваться для кластерного индекса.</p> <p>Кластеризация по индексу позволяет сократить время поиска по диску: во время поиска по индексу выборка данных может быть значительно быстрее, так как последовательность данных в таком же порядке, как и индекс. Из минусов можно отметить то, что команда <code>CLUSTER</code> требует «ACCESS EXCLUSIVE» блокировку, что предотвращает любые другие операции с данными (чтения и записи) пока кластеризация не завершит выполнение. Также кластеризация индекса в PostgreSQL не утверждает четкий порядок следования, поэтому требуется повторно выполнять <code>CLUSTER</code> для поддержания таблицы в порядке.</p> <h2 id="утилиты-для-тюнинга-postgresql">Утилиты для тюнинга PostgreSQL</h2> <h3 id=pgtune>Pgtune</h3> <p>Для оптимизации настроек для PostgreSQL Gregory Smith создал утилиту <a href="http://pgtune.projects.postgresql.org/">pgtune</a> в расчёте на обеспечение максимальной производительности для заданной аппаратной конфигурации. Утилита проста в использовании и во многих Linux системах может идти в составе пакетов. Если же нет, можно просто скачать архив и распаковать. Для начала:</p> <pre><code>$ pgtune -i $PGDATA/postgresql.conf -o $PGDATA/postgresql.conf.pgtune</code></pre> <p>опцией <code>-i, --input-config</code> указываем текущий файл postgresql.conf, а <code>-o, --output-config</code> указываем имя файла для нового postgresql.conf.</p> <p>Есть также дополнительные опции для настройки конфига:</p> <ul> <li><p><code>-M, --memory</code> используйте этот параметр, чтобы определить общий объем системной памяти. Если не указано, pgtune будет пытаться использовать текущий объем системной памяти;</p></li> <li><p><code>-T, --type</code> указывает тип базы данных. Опции: DW, OLTP, Web, Mixed, Desktop;</p></li> <li><p><code>-c, --connections</code> указывает максимальное количество соединений. Если он не указан, то будет браться в зависимости от типа базы данных;</p></li> </ul> <p>Существует также <a href="http://pgtune.leopard.in.ua/">онлайн версия pgtune</a>.</p> <p>Хочется сразу добавить, что pgtune не «серебряная пуля» для оптимизации настройки PostgreSQL. Многие настройки зависят не только от аппаратной конфигурации, но и от размера базы данных, числа соединений и сложности запросов, так что оптимально настроить базу данных возможно только учитывая все эти параметры.</p> <h3 id=pg_buffercache>pg_buffercache</h3> <p><a href="http://www.postgresql.org/docs/current/static/pgbuffercache.html">Pg_buffercache</a> — расширение для PostgreSQL, которое позволяет получить представление об использовании общего буфера (<code>shared_buffer</code>) в базе. Расширение позволяет взглянуть какие из данных кэширует база, которые активно используются в запросах. Для начала нужно установить расширение:</p> <pre><code># CREATE EXTENSION pg_buffercache;</code></pre> <p>Теперь доступно <code>pg_buffercache</code> представление, которое содержит:</p> <ul> <li><p><code>bufferid</code> — ID блока в общем буфере;</p></li> <li><p><code>relfilenode</code> — имя папки, где данные расположены;</p></li> <li><p><code>reltablespace</code> — Oid таблицы;</p></li> <li><p><code>reldatabase</code> — Oid базы данных;</p></li> <li><p><code>relforknumber</code> — номер ответвления;</p></li> <li><p><code>relblocknumber</code> — номер страницы;</p></li> <li><p><code>isdirty</code> — грязная страница?;</p></li> <li><p><code>usagecount</code> — количество LRU страниц;</p></li> </ul> <p>ID блока в общем буфере (<code>bufferid</code>) соответствует количеству используемого буфера таблицей, индексом, прочим. Общее количество доступных буферов определяется двумя вещами:</p> <ul> <li><p>Размер буферного блока. Этот размер блока определяется опцией <code>--with-blocksize</code> при конфигурации. Значение по умолчанию — 8 КБ, что достаточно в большинстве случаев, но его возможно увеличить или уменьшить в зависимости от ситуации. Для того чтобы изменить это значение, необходимо будет перекомпилировать PostgreSQL;</p></li> <li><p>Размер общего буфера. Определяется опцией <code>shared_buffers</code> в PostgreSQL конфиге.</p></li> </ul> <p>Например, при использовании <code>shared_buffers</code> в 128 МБ с 8 КБ размера блока получится 16384 буферов. Представление pg_buffercache будет иметь такое же число строк — 16384. С <code>shared_buffers</code> в 256 МБ и размером блока в 1 КБ получим 262144 буферов.</p> <p>Для примера рассмотрим простой запрос показывающий использование буферов объектами (таблицами, индексами, прочим):</p> <pre><code># SELECT c.relname, count(*) AS buffers
FROM pg_buffercache b INNER JOIN pg_class c
ON b.relfilenode = pg_relation_filenode(c.oid) AND
b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database()))
GROUP BY c.relname
ORDER BY 2 DESC
LIMIT 10;

             relname             | buffers
---------------------------------+---------
 pgbench_accounts                |    4082
 pgbench_history                 |      53
 pg_attribute                    |      23
 pg_proc                         |      14
 pg_operator                     |      11
 pg_proc_oid_index               |       9
 pg_class                        |       8
 pg_attribute_relid_attnum_index |       7
 pg_proc_proname_args_nsp_index  |       6
 pg_class_oid_index              |       5
(10 rows)</code></pre> <p>Этот запрос показывает объекты (таблицы и индексы) в кэше:</p> <pre><code># SELECT c.relname, count(*) AS buffers,usagecount
 FROM pg_class c
 INNER JOIN pg_buffercache b
 ON b.relfilenode = c.relfilenode
 INNER JOIN pg_database d
 ON (b.reldatabase = d.oid AND d.datname = current_database())
GROUP BY c.relname,usagecount
ORDER BY c.relname,usagecount;

             relname              | buffers | usagecount
----------------------------------+---------+------------
 pg_rewrite                       |       3 |          1
 pg_rewrite_rel_rulename_index    |       1 |          1
 pg_rewrite_rel_rulename_index    |       1 |          2
 pg_statistic                     |       1 |          1
 pg_statistic                     |       1 |          3
 pg_statistic                     |       2 |          5
 pg_statistic_relid_att_inh_index |       1 |          1
 pg_statistic_relid_att_inh_index |       3 |          5
 pgbench_accounts                 |    4082 |          2
 pgbench_accounts_pkey            |       1 |          1
 pgbench_history                  |      53 |          1
 pgbench_tellers                  |       1 |          1</code></pre> <p>Это запрос показывает какой процент общего буфера используют обьекты (таблицы и индексы) и на сколько процентов объекты находятся в самом кэше (буфере):</p> <pre><code># SELECT
 c.relname,
 pg_size_pretty(count(*) * 8192) as buffered,
 round(100.0 * count(*) /
 (SELECT setting FROM pg_settings WHERE name=&#39;shared_buffers&#39;)::integer,1)
 AS buffers_percent,
 round(100.0 * count(*) * 8192 / pg_table_size(c.oid),1)
 AS percent_of_relation
FROM pg_class c
 INNER JOIN pg_buffercache b
 ON b.relfilenode = c.relfilenode
 INNER JOIN pg_database d
 ON (b.reldatabase = d.oid AND d.datname = current_database())
GROUP BY c.oid,c.relname
ORDER BY 3 DESC
LIMIT 20;

-[ RECORD 1 ]-------+---------------------------------
 relname             | pgbench_accounts
 buffered            | 32 MB
 buffers_percent     | 24.9
 percent_of_relation | 99.9
-[ RECORD 2 ]-------+---------------------------------
 relname             | pgbench_history
 buffered            | 424 kB
 buffers_percent     | 0.3
 percent_of_relation | 94.6
-[ RECORD 3 ]-------+---------------------------------
 relname             | pg_operator
 buffered            | 88 kB
 buffers_percent     | 0.1
 percent_of_relation | 61.1
-[ RECORD 4 ]-------+---------------------------------
 relname             | pg_opclass_oid_index
 buffered            | 16 kB
 buffers_percent     | 0.0
 percent_of_relation | 100.0
-[ RECORD 5 ]-------+---------------------------------
 relname             | pg_statistic_relid_att_inh_index
 buffered            | 32 kB
 buffers_percent     | 0.0
 percent_of_relation | 100.0</code></pre> <p>Используя эти данные можно проанализировать для каких объектов не хватает памяти или какие из них потребляют основную часть общего буфера. На основе этого можно более правильно настраивать <code>shared_buffers</code> параметр для PostgreSQL.</p> <h2 id="оптимизация-бд-и-приложения">Оптимизация БД и приложения</h2> <p>Для быстрой работы каждого запроса в вашей базе в основном требуется следующее:</p> <ol> <li><p>Отсутствие в базе мусора, мешающего добраться до актуальных данных. Можно сформулировать две подзадачи:</p> <ol> <li><p>Грамотное проектирование базы. Освещение этого вопроса выходит далеко за рамки этой книги;</p></li> <li><p>Сборка мусора, возникающего при работе СУБД;</p></li> </ol></li> <li><p>Наличие быстрых путей доступа к данным — индексов;</p></li> <li><p>Возможность использования оптимизатором этих быстрых путей;</p></li> <li><p>Обход известных проблем;</p></li> </ol> <h3 id="поддержание-базы-в-порядке">Поддержание базы в порядке</h3> <p>В данном разделе описаны действия, которые должны периодически выполняться для каждой базы. От разработчика требуется только настроить их автоматическое выполнение (при помощи cron) и опытным путём подобрать оптимальную частоту.</p> <h4 id="команда-analyze">Команда ANALYZE</h4> <p>Служит для обновления информации о распределении данных в таблице. Эта информация используется оптимизатором для выбора наиболее быстрого плана выполнения запроса.</p> <p>Обычно команда используется в связке с <code>VACUUM ANALYZE</code>. Если в базе есть таблицы, данные в которых не изменяются и не удаляются, а лишь добавляются, то для таких таблиц можно использовать отдельную команду ANALYZE. Также стоит использовать эту команду для отдельной таблицы после добавления в неё большого количества записей.</p> <h4 id="команда-reindex">Команда REINDEX</h4> <p>Команда <code>REINDEX</code> используется для перестройки существующих индексов. Использовать её имеет смысл в случае:</p> <ul> <li><p>порчи индекса;</p></li> <li><p>постоянного увеличения его размера;</p></li> </ul> <p>Второй случай требует пояснений. Индекс, как и таблица, содержит блоки со старыми версиями записей. PostgreSQL не всегда может заново использовать эти блоки, и поэтому файл с индексом постепенно увеличивается в размерах. Если данные в таблице часто меняются, то расти он может весьма быстро.</p> <p>Если вы заметили подобное поведение какого-то индекса, то стоит настроить для него периодическое выполнение команды <code>REINDEX</code>. Учтите: команда <code>REINDEX</code>, как и <code>VACUUM FULL</code>, полностью блокирует таблицу, поэтому выполнять её надо тогда, когда загрузка сервера минимальна.</p> <h3 id="использование-индексов">Использование индексов</h3> <p>Опыт показывает, что наиболее значительные проблемы с производительностью вызываются отсутствием нужных индексов. Поэтому столкнувшись с медленным запросом, в первую очередь проверьте, существуют ли индексы, которые он может использовать. Если нет — постройте их. Излишек индексов, впрочем, тоже чреват проблемами:</p> <ul> <li><p>Команды, изменяющие данные в таблице, должны изменить также и индексы. Очевидно, чем больше индексов построено для таблицы, тем медленнее это будет происходить;</p></li> <li><p>Оптимизатор перебирает возможные пути выполнения запросов. Если построено много ненужных индексов, то этот перебор будет идти дольше;</p></li> </ul> <p>Единственное, что можно сказать с большой степенью определённости — поля, являющиеся внешними ключами, и поля, по которым объединяются таблицы, индексировать надо обязательно.</p> <h4 id="команда-explain-analyze">Команда EXPLAIN [ANALYZE]</h4> <p>Команда <code>EXPLAIN [запрос]</code> показывает, каким образом PostgreSQL собирается выполнять ваш запрос. Команда <code>EXPLAIN ANALYZE [запрос]</code> выполняет запрос (и поэтому EXPLAIN ANALYZE DELETE … — не слишком хорошая идея) и показывает как изначальный план, так и реальный процесс его выполнения.</p> <p>Чтение вывода этих команд — искусство, которое приходит с опытом. Для начала обращайте внимание на следующее:</p> <ul> <li><p>Использование полного просмотра таблицы (seq scan);</p></li> <li><p>Использование наиболее примитивного способа объединения таблиц (nested loop);</p></li> <li><p>Для <code>EXPLAIN ANALYZE</code>: нет ли больших отличий в предполагаемом количестве записей и реально выбранном? Если оптимизатор использует устаревшую статистику, то он может выбирать не самый быстрый план выполнения запроса;</p></li> </ul> <p>Следует отметить, что полный просмотр таблицы далеко не всегда медленнее просмотра по индексу. Если, например, в таблице–справочнике несколько сотен записей, умещающихся в одном-двух блоках на диске, то использование индекса приведёт лишь к тому, что придётся читать ещё и пару лишних блоков индекса. Если в запросе придётся выбрать 80% записей из большой таблицы, то полный просмотр опять же получится быстрее.</p> <p>При тестировании запросов с использованием <code>EXPLAIN ANALYZE</code> можно воспользоваться настройками, запрещающими оптимизатору использовать определённые планы выполнения. Например,</p> <pre><code>SET enable_seqscan=false;</code></pre> <p>запретит использование полного просмотра таблицы, и вы сможете выяснить, прав ли был оптимизатор, отказываясь от использования индекса. Ни в коем случае не следует прописывать подобные команды в postgresql.conf! Это может ускорить выполнение нескольких запросов, но сильно замедлит все остальные!</p> <h4 id="использование-собранной-статистики">Использование собранной статистики</h4> <p>Результаты работы сборщика статистики доступны через специальные системные представления. Наиболее интересны для наших целей следующие:</p> <ul> <li><p><code>pg_stat_user_tables</code> содержит — для каждой пользовательской таблицы в текущей базе данных — общее количество полных просмотров и просмотров с использованием индексов, общие количества записей, которые были возвращены в результате обоих типов просмотра, а также общие количества вставленных, изменённых и удалённых записей;</p></li> <li><p><code>pg_stat_user_indexes</code> содержит — для каждого пользовательского индекса в текущей базе данных — общее количество просмотров, использовавших этот индекс, количество прочитанных записей, количество успешно прочитанных записей в таблице (может быть меньше предыдущего значения, если в индексе есть записи, указывающие на устаревшие записи в таблице);</p></li> <li><p><code>pg_statio_user_tables</code> содержит — для каждой пользовательской таблицы в текущей базе данных — общее количество блоков, прочитанных из таблицы, количество блоков, оказавшихся при этом в буфере (см. пункт 2.1.1), а также аналогичную статистику для всех индексов по таблице и, возможно, по связанной с ней таблицей TOAST;</p></li> </ul> <p>Из этих представлений можно узнать, в частности:</p> <ul> <li><p>Для каких таблиц стоит создать новые индексы (индикатором служит большое количество полных просмотров и большое количество прочитанных блоков);</p></li> <li><p>Какие индексы вообще не используются в запросах. Их имеет смысл удалить, если, конечно, речь не идёт об индексах, обеспечивающих выполнение ограничений PRIMARY KEY и UNIQUE;</p></li> <li><p>Достаточен ли объём буфера сервера;</p></li> </ul> <p>Также возможен «дедуктивный» подход, при котором сначала создаётся большое количество индексов, а затем неиспользуемые индексы удаляются.</p> <h3 id="перенос-логики-на-сторону-сервера">Перенос логики на сторону сервера</h3> <p>Этот пункт очевиден для опытных пользователей PostrgeSQL и предназначен для тех, кто использует или переносит на PostgreSQL приложения, написанные изначально для более примитивных СУБД.</p> <p>Реализация части логики на стороне сервера через хранимые процедуры, триггеры, правила<a href="#fn1" class=footnoteRef id=fnref1><sup>1</sup></a> часто позволяет ускорить работу приложения. Действительно, если несколько запросов объединены в процедуру, то не требуется</p> <ul> <li><p>пересылка промежуточных запросов на сервер;</p></li> <li><p>получение промежуточных результатов на клиент и их обработка;</p></li> </ul> <p>Кроме того, хранимые процедуры упрощают процесс разработки и поддержки: изменения надо вносить только на стороне сервера, а не менять запросы во всех приложениях.</p> <h3 id="sec:pg-optimize-sql">Оптимизация конкретных запросов</h3> <p>В этом разделе описываются запросы, для которых по разным причинам нельзя заставить оптимизатор использовать индексы, и которые будут всегда вызывать полный просмотр таблицы. Таким образом, если вам требуется использовать эти запросы в требовательном к быстродействию приложении, то придётся их изменить.</p> <h4 id="select-count-from-огромная-таблица">SELECT count(*) FROM &lt;огромная таблица&gt;</h4> <p>Функция <code>count()</code> работает очень просто: сначала выбираются все записи, удовлетворяющие условию, а потом к полученному набору записей применяется агрегатная функция — считается количество выбранных строк. Информация о видимости записи для текущей транзакции (а конкурентным транзакциям может быть видимо разное количество записей в таблице!) не хранится в индексе, поэтому, даже если использовать для выполнения запроса индекс первичного ключа таблицы, всё равно потребуется чтение записей собственно из файла таблицы.</p> <p><strong>Проблема</strong> Запрос вида</p> <pre><code>SELECT count(*) FROM foo;</code></pre> <p>осуществляет полный просмотр таблицы foo, что весьма долго для таблиц с большим количеством записей.</p> <p><strong>Решение</strong> Простого решения проблемы, к сожалению, нет. Возможны следующие подходы:</p> <ol> <li><p>Если точное число записей не важно, а важен порядок<a href="#fn2" class=footnoteRef id=fnref2><sup>2</sup></a>, то можно использовать информацию о количестве записей в таблице, собранную при выполнении команды ANALYZE:</p> <pre><code>SELECT reltuples FROM pg_class WHERE relname = &#39;foo&#39;;</code></pre></li> <li><p>Если подобные выборки выполняются часто, а изменения в таблице достаточно редки, то можно завести вспомогательную таблицу, хранящую число записей в основной. На основную же таблицу повесить триггер, который будет уменьшать это число в случае удаления записи и увеличивать в случае вставки. Таким образом, для получения количества записей потребуется лишь выбрать одну запись из вспомогательной таблицы;</p></li> <li><p>Вариант предыдущего подхода, но данные во вспомогательной таблице обновляются через определённые промежутки времени (cron);</p></li> </ol> <h4 id="медленный-distinct">Медленный DISTINCT</h4> <p>Текущая реализация <code>DISTINCT</code> для больших таблиц очень медленна. Но возможно использовать <code>GROUP BY</code> взамен <code>DISTINCT</code>. <code>GROUP BY</code> может использовать агрегирующий хэш, что значительно быстрее, чем <code>DISTINCT</code> (актуально до версии 8.4 и ниже).</p> <pre><code>postgres=# select count(*) from (select distinct i from g) a;
 count
-------
 19125
(1 row)

Time: 580,553 ms


postgres=# select count(*) from (select distinct i from g) a;
 count
-------
 19125
(1 row)

Time: 36,281 ms</code></pre> <pre><code>postgres=# select count(*) from (select i from g group by i) a;
 count
-------
 19125
(1 row)

Time: 26,562 ms


postgres=# select count(*) from (select i from g group by i) a;
 count
-------
 19125
(1 row)

Time: 25,270 ms</code></pre> <h3 id="утилиты-для-оптимизации-запросов">Утилиты для оптимизации запросов</h3> <h4 id=pgfouine>pgFouine</h4> <p><a href="http://pgfouine.projects.pgfoundry.org/">pgFouine</a> — это анализатор log-файлов для PostgreSQL, используемый для генерации детальных отчетов из log-файлов PostgreSQL. pgFouine поможет определить, какие запросы следует оптимизировать в первую очередь. pgFouine написан на языке программирования PHP с использованием объектно-ориентированных технологий и легко расширяется для поддержки специализированных отчетов, является свободным программным обеспечением и распространяется на условиях GNU General Public License. Утилита спроектирована таким образом, чтобы обработка очень больших log-файлов не требовала много ресурсов.</p> <p>Для работы с pgFouine сначала нужно сконфигурировать PostgreSQL для создания нужного формата log-файлов:</p> <ul> <li><p>Чтобы включить протоколирование в syslog</p> <pre><code>  log_destination = &#39;syslog&#39;
  redirect_stderr = off
  silent_mode = on
  </code></pre></li> <li><p>Для записи запросов, длящихся дольше n миллисекунд:</p> <pre><code>  log_min_duration_statement = n
  log_duration = off
  log_statement = &#39;none&#39;
  </code></pre></li> </ul> <p>Для записи каждого обработанного запроса установите <code>log_min_duration_statement</code> на 0. Чтобы отключить запись запросов, установите этот параметр на -1.</p> <p>pgFouine — простой в использовании инструмент командной строки. Следующая команда создаёт HTML-отчёт со стандартными параметрами:</p> <pre><code>pgfouine.php -file your/log/file.log &gt; your-report.html</code></pre> <p>С помощью этой строки можно отобразить текстовый отчёт с 10 запросами на каждый экран на стандартном выводе:</p> <pre><code>pgfouine.php -file your/log/file.log -top 10 -format text</code></pre> <p>Более подробно о возможностях, а также много полезных примеров, можно найти на официальном сайта проекта <a href="http://pgfouine.projects.pgfoundry.org/">pgfouine.projects.pgfoundry.org</a>.</p> <h4 id=pgbadger>pgBadger</h4> <p><a href="http://dalibo.github.io/pgbadger/">pgBadger</a> — аналогичная утилита, что и pgFouine, но написанная на Perl. Еще одно большое преимущество проекта в том, что он более активно сейчас разрабатывается (на момент написания этого текста последний релиз pgFouine был в 24.02.2010, а последняя версия pgBadger — 22.02.2016). Установка pgBadger проста:</p> <pre><code>$ tar xzf pgbadger-2.x.tar.gz
$ cd pgbadger-2.x/
$ perl Makefile.PL
$ make &amp;&amp; sudo make install</code></pre> <p>Как и в случае с pgFouine нужно настроить PostgreSQL логи:</p> <pre><code>logging_collector = on
log_min_messages = debug1
log_min_error_statement = debug1
log_min_duration_statement = 0
log_line_prefix = &#39;%t [%p]: [%l-1] user=%u,db=%d &#39;
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on
log_temp_files = 0</code></pre> <p>Парсим логи PostgreSQL через pgBadger:</p> <pre><code>$ ./pgbadger ~/pgsql/master/pg_log/postgresql-2012-08-30_132*
[========================&gt;] Parsed 10485768 bytes of 10485768 (100.00%)
[========================&gt;] Parsed 10485828 bytes of 10485828 (100.00%)
[========================&gt;] Parsed 10485851 bytes of 10485851 (100.00%)
[========================&gt;] Parsed 10485848 bytes of 10485848 (100.00%)
[========================&gt;] Parsed 10485839 bytes of 10485839 (100.00%)
[========================&gt;] Parsed 982536 bytes of 982536 (100.00%)</code></pre> <p>В результате получится HTML файлы, которые содержат статистику по запросам к PostgreSQL. Более подробно о возможностях можно найти на официальном сайта проекта <a href="http://dalibo.github.io/pgbadger/" class=uri>http://dalibo.github.io/pgbadger/</a>.</p> <h4 id=pg_stat_statements>pg_stat_statements</h4> <p>Pg_stat_statements — расширение для сбора статистики выполнения запросов в рамках всего сервера. Преимущество данного расширения в том, что ему не требуется собирать и парсить логи PostgreSQL, как это делает pgFouine и pgBadger. Для начала установим и настроим его:</p> <pre><code>shared_preload_libraries = &#39;pg_stat_statements&#39;
custom_variable_classes = &#39;pg_stat_statements&#39; # данная настройка нужна для PostgreSQL 9.1 и ниже

pg_stat_statements.max = 10000
pg_stat_statements.track = all</code></pre> <p>После внесения этих параметров PostgreSQL потребуется перегрузить. Параметры конфигурации pg_stat_statements:</p> <ol> <li><p><code>pg_stat_statements.max (integer)</code>» — максимальное количество sql запросов, которые будет хранится расширением (удаляются записи с наименьшим количеством вызовов);</p></li> <li><p><code>pg_stat_statements.track (enum)</code>» — какие SQL запросы требуется записывать. Возможные параметры: top (только запросы от приложения/клиента), all (все запросы, например в функциях) и none (отключить сбор статистики);</p></li> <li><p><code>pg_stat_statements.save (boolean)</code>» — следует ли сохранять собранную статистику после остановки PostgreSQL. По умолчанию включено;</p></li> </ol> <p>Далее активируем расширение:</p> <pre><code># CREATE EXTENSION pg_stat_statements;</code></pre> <p>Пример собранной статистики:</p> <pre><code># SELECT query, calls, total_time, rows, 100.0 * shared_blks_hit /
               nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
          FROM pg_stat_statements ORDER BY total_time DESC LIMIT 10;
-[ RECORD 1 ]----------------------------------------------------------------------------
query       | SELECT query, calls, total_time, rows, ? * shared_blks_hit /
            |                nullif(shared_blks_hit + shared_blks_read, ?) AS hit_percent
            |           FROM pg_stat_statements ORDER BY total_time DESC LIMIT ?;
calls       | 3
total_time  | 0.994
rows        | 7
hit_percent | 100.0000000000000000
-[ RECORD 2 ]----------------------------------------------------------------------------
query       | insert into x (i) select generate_series(?,?);
calls       | 2
total_time  | 0.591
rows        | 110
hit_percent | 100.0000000000000000
-[ RECORD 3 ]----------------------------------------------------------------------------
query       | select * from x where i = ?;
calls       | 2
total_time  | 0.157
rows        | 6
hit_percent | 100.0000000000000000
-[ RECORD 4 ]----------------------------------------------------------------------------
query       | SELECT pg_stat_statements_reset();
calls       | 1
total_time  | 0.102
rows        | 1
hit_percent |</code></pre> <p>Для сброса статистики есть команда <code>pg_stat_statements_reset</code>:</p> <pre><code># SELECT pg_stat_statements_reset();
-[ RECORD 1 ]------------+-
pg_stat_statements_reset |

# SELECT query, calls, total_time, rows, 100.0 * shared_blks_hit /
               nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
          FROM pg_stat_statements ORDER BY total_time DESC LIMIT 10;
-[ RECORD 1 ]-----------------------------------
query       | SELECT pg_stat_statements_reset();
calls       | 1
total_time  | 0.175
rows        | 1
hit_percent |</code></pre> <p>Хочется сразу отметить, что расширение только с версии PostgreSQL 9.2 contrib нормализирует SQL запросы. В версиях 9.1 и ниже SQL запросы сохраняются как есть, а значит «select * from table where id = 3» и «select * from table where id = 21» буду разными записями, что почти бесполезно для сбора полезной статистики.</p> <h2 id="заключение">Заключение</h2> <p>К счастью, PostgreSQL не требует особо сложной настройки. В большинстве случаев вполне достаточно будет увеличить объём выделенной памяти, настроить периодическое поддержание базы в порядке и проверить наличие необходимых индексов. Более сложные вопросы можно обсудить в специализированном списке рассылки.</p> <h1 id="индексы">Индексы</h1> <p>Что такое таблица в реляционной СУБД? Это такой список из кортежей (tuple). Каждый кортеж состоит из ячеек (row). Количество ячеек в кортеже и их тип совпадают со схемой колонки, нескольких колонок. Этот список имеют сквозную нумерацию RowId — порядковый номер. Таким образом, таблицы можно осознавать как список пар (RowId, Кортеж).</p> <p>Индексы — это обратные отношения (Кортеж, RowId). Кортеж обязан содержать больше равно одной ячейки (т.е. быть построенным минимум по одной колонке). Для индексов, которые индексируют более одной колонки — они ещё называются составными, и участвуют в отношениях вида «многие-ко-многим» — всё написанное верно в равной степени. Очевидно, если кортеж — не уникален (в колонке существует два одинаковых кортежа), то эти отношения выглядят как (Кортеж, Список RowId) — т.е. кортежу сопоставляется список RowId.</p> <p>Индексы могут использоватся для таких операций в базе данных:</p> <ul> <li><p>Поиск данных — абсолютно все индексы поддерживают поиск значений по равенству. А B-Tree — по произвольным диапазонам;</p></li> <li><p>Like — B-Tree и Bitmap индексы можно использовать для ускорения префиксных Like-предикатов (вида abc%);</p></li> <li><p>Оптимизатор — B-Tree и R-Tree индексы представляют из себя гистограмму произвольной точности;</p></li> <li><p>Join — индексы могут быть использованы для Merge, Index алгоритмов;</p></li> <li><p>Relation — индексы могут быть использованы для операций except/intersect;</p></li> <li><p>Aggregations — индексы позволяют эффективно вычислять некоторые агрегационные функции — COUNT, MIX, MAX, а также их DISTINCT версии;</p></li> <li><p>Grouping — индексы позволяют эффективно вычислять группировки и произвольные агрегационные функции (sort-group алгоритм);</p></li> </ul> <h2 id="типы-индексов">Типы индексов</h2> <p>В зависимости от структуры, используемой в реализации индексов, существенно различаются поддерживаемые операции, их стоимости, а также свойства читаемых данных. Давайте рассмотрим какие существуют типы индексов в PostgreSQL.</p> <h3 id=b-tree>B-Tree</h3> <p>B-Tree (Boeing/Bayer/Balanced/Broad/Bushy-Tree) называют упорядоченное блочное дерево. Узлы в дереве представляют из себя блоки фиксированного размера. У каждого узла фиксированные число детей. Структура B-Tree представлена на рисунке [fig:btree_index].</p> <p>B-Tree для индексов отличается от представленной на Википедии — есть дублированные данных в промежуточных блоках. Для i-ой записи в блоке сохраняется не значение, которое больше максимума i-го поддерева, и меньше минимума (i+1) поддерева, а максимум i-го поддерева. Различия проистекают из того, что википедия приводит пример B-Tree для множества, а нам нужен ассоциативный массив.</p> <p>В индексном B-Tree значения и RowId размещаются совместно на нижнем слое дерева. Каждый узел дерева представляет из себя одну страницу (page) в некотором формате. В начале страницы всегда идёт некоторый заголовок. Для корневого и промежуточного узла в страницах хранятся пары (Значение, Номер страницы). Для листовых — пары (Значение ,RowId) либо (Значение, Список RowId) (в зависимости от свойств значения — уникально или нет). B-Tree деревья имеют крайне маленькую высоту — порядка <span class=LaTeX>$H = \log_m{N}$</span>, где m — количество записей в блоке, N — количество элементов. B-Tree деревья являются упорядоченными — все элементы в любой странице (блоке) дерева лежат последовательно. Предыдущие два свойства позволяют крайне эффективно производить поиск — начиная с первой страницы, половинным делением (binary search) выделяются дети, в которых лежат границы поиска. Таким образом, прочитав всего H, 2H страниц мы находим искомый диапозон. Важным ньюансом является также факт, что страницы в листьях связаны в односвязный либо двусвязный список - это означает, что выполнив поиск, мы можем дальше просто последовательно читать страницы, и эффективность чтения большего объёма данных (длинного диапазона) сравнима с эффективностью чтению данных из таблицы.</p> <p>Сильные стороны B-Tree индексов:</p> <ul> <li><p>сохраняют сортированность данных;</p></li> <li><p>поддерживают поиск по унарным и бинарным предикатам (<code>&lt;a; = b; &gt;c and &lt;d; &lt;e and &gt;f</code>) за O(<span class=LaTeX>$\log_m{N}$</span>), где m — количество записей в блоке, N — количество элементов;</p></li> <li><p>позволяют не сканируя последовательность данных целиком оценить cardinality (количество записей) для всего индекса (а следовательно таблицы), диапазона, причём с произвольной точностью. Посмотрели корневую страницу — получили одну точность. Посмотрели следующий уровень дерева — получили точность получше. Просмотрели дерево до корня — получили точное число записей;</p></li> <li><p>самобалансируемый, для внесения изменения не требуется полного перестроения, происходит не более O(<span class=LaTeX>$\log_m{N}$</span>) дейстий, , где m — количество записей в блоке, N — количество элементов;</p></li> </ul> <p>Слабые стороны B-Tree индексов:</p> <ul> <li><p>занимают много места на диске. Индекс по уникальным Integer-ам к примеру весит в два раза больше аналогичной колонки (т.к. храняться ещё и RowId);</p></li> <li><p>при постоянной записи дерево начинает хранить данные разреженно (сразу после построения они могут лежать очень плотно), и время доступа увеличивается за счёт увеличения объёма дисковой информации. Поэтому B-Tree индексы требуют присмотра и периодического перепостроения (REBUILD);</p></li> </ul> <h3 id=r-tree>R-Tree</h3> <p>R-Tree (Rectangle-Tree) предназначен для хранения пар (X, Y) значений числового типа (например, координат). По способу организации R-Tree очень похоже на B-Tree. Единственное отличие — это информация, записываемая в промежуточные страницы в дереве. Для i-го значения в узле мы B-Tree мы пишем максимум из i-го поддерева, а в R-Tree — минимальный прямоугольник, покрывающий все прямоугольники из ребёнка. Подробней можно увидеть на рисунке [fig:rtree_index].</p> <p>Сильные стороны:</p> <ul> <li><p>поиск произвольных регионов, точек за O(<span class=LaTeX>$\log_m{N}$</span>), где m — количество записей в блоке, N — количество элементов;</p></li> <li><p>позволяет оценить количество точек в некотором регионе без полного сканирования данных;</p></li> </ul> <p>Слабые стороны:</p> <ul> <li><p>существенная избыточность в хранении данных;</p></li> <li><p>медленное обновление данных;</p></li> </ul> <p>В целом, плюсы-минусы очень напоминают B-Tree.</p> <h3 id="hash-индекс">Hash индекс</h3> <p>Hash индекс по сути является ассоциативным хеш-контейнером. Хеш-контейнер — это массив из разряженных значений. Адресуются отдельные элементы этого массива некоторой хеш-функцией которая отображает каждое значение в некоторое целое число. Т.е. результат хеш-функции является порядковым номером элемента в массиве. Элементы массива в хеш-конейтнере называются букетами (bucket). Обычно один букет — одна странца. Хеш-функция отображает более мощное множество в менее мощное, возникают так называемые коллизии — ситуация, когда одному значению хеш-функции соответствует несколько разных значений. В букете хранятся значения, образующие коллизию. Разрешение коллизий происходит посредством поиска среди значений, сохранённых в букете.</p> <p>Сильные стороны:</p> <ul> <li><p>очень быстрый поиск O(1);</p></li> <li><p>стабильность — индекс не нужно перестраивать;</p></li> </ul> <p>Слабые стороны:</p> <ul> <li><p>хеш очень чувствителен к коллизиям хеш-функции. В случае «плохого» распределения данных, большинство записей будет сосредоточено в нескольких букетах, и фактически поиск будет происходить путем разрешения коллизий;</p></li> <li><p>из-за нелинейнойсти хэш-функций данный индекс нельзя сортировать по значению, что приводит к невозможности использования в сравнениях больше/меньше и «IS NULL»;</p></li> <li><p>данный индекс в PostgreSQL транзакционно не безопасен, нужно перестраивать после краха и не реплицируется через потоковую (streaming) репликацию (разработчики обещают это исправить к 10 версии);</p></li> </ul> <h3 id="sec:indexes-bitmap-index">Битовый индекс (bitmap index)</h3> <p>Битовый индекс (bitmap index) — метод битовых индексов заключается в создании отдельных битовых карт (последовательность 0 и 1) для каждого возможного значения столбца, где каждому биту соответствует строка с индексируемым значением, а его значение равное 1 означает, что запись, соответствующая позиции бита содержит индексируемое значение для данного столбца или свойства (<a href="https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%B4_%D0%A5%D0%B0%D1%84%D1%84%D0%BC%D0%B0%D0%BD%D0%B0">алгоритм Хаффмана</a>).</p> <p>Сильные стороны:</p> <ul> <li><p>компактность представления (занимает мало места);</p></li> <li><p>быстрое чтение и поиск по предикату «равно»;</p></li> </ul> <p>Слабые стороны:</p> <ul> <li><p>невозможность изменить способ кодирования значений в процессе обновления данных;</p></li> </ul> <p>У PostgreSQL нет возможности создать постоянный битовый индекс, но база может на лету создавать данные индексы для объединения разных индексов. Чтобы объединить несколько индексов, база сканирует каждый необходимый индекс и готовит битовую карта в памяти с расположением строк таблицы. Битовые карты затем обрабатываются AND/OR операцией по мере требования запроса и после этого выбираются колонки с данными.</p> <h3 id="gist-индекс">GiST индекс</h3> <p>GiST (Generalized Search Tree) — обобщение B-Tree, R-Tree дерево поиска по произвольному предикату. Структура дерева не меняется, по прежнему в каждом не листовом узле хранятся пары (Значения, Номер страницы), а количество детей совпадает с количеством пар в узле. Существенное отличие состоит в организации ключа. B-Tree деревья заточены под поиск диапазонов, и хранят максимумы поддерева-ребёнка. R-Tree — региона на координатной плоскости. GiST предлагает в качестве значений в не листовых узлах хранить ту информацию, которую мы считаем существенной, и которая позволит определить, есть ли интересующие нас значения (удовлетворяющие предикату) в поддереве-ребёнке. Конкретный вид хранимой информации зависит от вида поиска, который мы желаем проводить. Таким образом параметризовав R-Tree и B-Tree дерево предикатами и значениями мы автоматически получаем специализированный под задачу индекс (PostGiST, pg_trgm, hstore, ltree, прочее).</p> <p>Сильные стороны:</p> <ul> <li><p>эффективный поиск;</p></li> </ul> <p>Слабые стороны:</p> <ul> <li><p>большая избыточность;</p></li> <li><p>необходимость специализированной реализации под каждую группу запросов;</p></li> </ul> <p>Остальные плюсы-минусы совпадают с B-Tree и R-Tree индексами.</p> <h3 id="gin-индекс">GIN индекс</h3> <p>GIN (Generalized Inverted Index) — обратный индекс, используемым полнотекстовым поиском PostgreSQL. Это означает, что в структуре индексов с каждой лексемой сопоставляется отсортированный список номеров документов, в которых она встречается. Очевидно, что поиск по такой структуре намного эффективнее, чем при использовании GiST, однако процесс добавления нового документа достаточно длителен.</p> <h3 id="cluster-индекс">Cluster индекс</h3> <p>Не является индексом, поскольку производит кластеризацию таблицы по заданному индексу. Более подробно можно почитать в разделе «[sec:hard-drive-cluster] ».</p> <h3 id="brin-индекс">BRIN индекс</h3> <p>Версия PostgreSQL 9.5 привнесла с собой новый вид индексов — BRIN (Block Range Index, или индекс блоковых зон).</p> <p>В отличие от привычного B-Tree, этот индекс намного эффективнее для очень больших таблиц, и в некоторых ситуациях позволяет заменить собой партицирование (подробно можно почитать в разделе «[sec:partitioning] »). BRIN-индекс имеет смысл применять для таблиц, в которых часть данных уже по своей природе как-то отсортирована. Например, это характерно для логов или для истории заказов магазина, которые пишутся последовательно, а потому уже на физическом уровне упорядочены по дате/номеру, и в то же время таблицы с такими данными обычно разрастаются до гигантских размеров.</p> <p>Под блоковой зоной (Block Range) подразумевается набор страниц, физически расположенных по соседству в таблице. Для каждой такой зоны создается некий идентификатор, отвечающий за «место» этой зоны в таблице. Для лога это может быть дата создания записи. Поиск по такому индексу осуществляется с потерями информации, то есть выбираются все записи, входящие в блоковые зоны с идентификаторами, соответствующими запросу, но среди записей в этих зонах могут попадаться такие, которые на следующем этапе надо будет отфильтровать. Размер индекса при этом очень маленький, и он почти не нагружает базу. Размер индекса обратно пропорционален параметру <code>pages_per_range</code>, отвечающему за количество страниц на зону. В то же время, чем меньше размер зоны, тем меньше «лишних» данных попадёт в результат поиска (надо подходить к этому параметру с умом).</p> <p>Индексы BRIN могут иметь один из нескольких встроенных классов операторов, по которым будет осуществляться разбивка на зоны и присвоение идентификаторов. Например, <code>int8_minmax_ops</code> применяется для операций сравнения целых чисел, а <code>date_minmax_ops</code> для сравнения дат.</p> <h2 id="возможности-индексов">Возможности индексов</h2> <h3 id="функциональный-индекс-functional-index">Функциональный индекс (functional index)</h3> <p>Вы можете построить индекс не только по полю/нескольким полям таблицы, но и по выражению, зависящему от полей. Пусть, например, в вашей таблице foo есть поле <code>foo_name</code>, и выборки часто делаются по условию «первая буква из поля foo_name в любом регистре». Вы можете создать индекс</p> <pre><code>CREATE INDEX foo_name_first_idx ON foo ((lower(substr(foo_name, 1, 1))));</code></pre> <p>и запрос вида</p> <pre><code>SELECT * FROM foo WHERE lower(substr(foo_name, 1, 1)) = &#39;а&#39;;</code></pre> <p>будет его использовать.</p> <h3 id="частичный-индекс-partial-index">Частичный индекс (partial index)</h3> <p>Под частичным индексом понимается индекс с предикатом WHERE. Пусть, например, у вас есть в базе таблица <code>scheta</code> с параметром <code>uplocheno</code> типа boolean. Записей, где <code>uplocheno = false</code> меньше, чем записей с <code>uplocheno = true</code>, а запросы по ним выполняются значительно чаще. Вы можете создать индекс</p> <pre><code>CREATE INDEX scheta_neuplocheno ON scheta (id) WHERE NOT uplocheno;</code></pre> <p>который будет использоваться запросом вида</p> <pre><code>SELECT * FROM scheta WHERE NOT uplocheno AND ...;</code></pre> <p>Достоинство подхода в том, что записи, не удовлетворяющие условию WHERE, просто не попадут в индекс.</p> <h3 id="уникальный-индекс-unique-index">Уникальный индекс (unique index)</h3> <p>Уникальный индекс гарантирует, что таблица не будет иметь более чем одну строку с тем же значением. Это удобно по двум причинам: целостность данных и производительность. Поиск данных с использованием уникального индекса, как правило, очень быстрый.</p> <h3 id="индекс-нескольких-столбцов-multi-column-index">Индекс нескольких столбцов (multi-column index)</h3> <p>В PostgreSQL возможно создавать индексы на несколько столбцов, но нам главное нужно понять когда имеет смысл создавать такой индекс, поскольку планировщик запросов PostgreSQL может комбинировать и использовать несколько индексов в запросе путем создания битового индекса («[sec:indexes-bitmap-index] »). Можно конечно создать индексы, которые охватять все возможные запросы, но за это придется платить производительностью (индексы нужно перестраивать при запросах на модификацию данных). Нужно также помнить, что индексы на несколько столбцов могут использоваться только запросами, которые ссылаются на эти столбцы в индексе в том же порядке. Индекс по столбцам <code>(a, b)</code> может быть использован в запросах, которые содержат <code>a = x and b = y</code> или <code>a = x</code>, но не будет использоваться в запросе вида <code>b = y</code>. Если это подходит под запросы вашего приложения, то данный индекс может быть полезен. В таком случае создание индекса на поле <code>a</code> было бы излишним. Индексы нескольких столбцов с указанием уникальности (<code>unique</code>) может быть также полезен для сохранения целосности данных (т.е. когда набор данных в этих стобцах должен быть уникальным).</p> <h1 id="sec:partitioning">Партиционирование</h1> <h2 id="введение-2">Введение</h2> <p>Партиционирование (partitioning, секционирование) — это разбиение больших структур баз данных (таблицы, индексы) на меньшие кусочки. Звучит сложно, но на практике все просто.</p> <p>Скорее всего у Вас есть несколько огромных таблиц (обычно всю нагрузку обеспечивают всего несколько таблиц СУБД из всех имеющихся). Причем чтение в большинстве случаев приходится только на самую последнюю их часть (т.е. активно читаются те данные, которые недавно появились). Примером тому может служить блог — на первую страницу (это последние 5…10 постов) приходится 40…50% всей нагрузки, или новостной портал (суть одна и та же), или системы личных сообщений, впрочем понятно. Партиционирование таблицы позволяет базе данных делать интеллектуальную выборку — сначала СУБД уточнит, какой партиции соответствует Ваш запрос (если это реально) и только потом сделает этот запрос, применительно к нужной партиции (или нескольким партициям). Таким образом, в рассмотренном случае, Вы распределите нагрузку на таблицу по ее партициям. Следовательно выборка типа <code>SELECT * FROM articles ORDER BY id DESC LIMIT 10</code> будет выполняться только над последней партицией, которая значительно меньше всей таблицы.</p> <p>Итак, партиционирование дает ряд преимуществ:</p> <ul> <li><p>На определенные виды запросов (которые, в свою очередь, создают основную нагрузку на СУБД) мы можем улучшить производительность;</p></li> <li><p>Массовое удаление может быть произведено путем удаления одной или нескольких партиций (<code>DROP TABLE</code> гораздо быстрее, чем массовый <code>DELETE</code>);</p></li> <li><p>Редко используемые данные могут быть перенесены в другое хранилище;</p></li> </ul> <h2 id="теория">Теория</h2> <p>На текущий момент PostgreSQL поддерживает два критерия для создания партиций:</p> <ul> <li><p>Партиционирование по диапазону значений (range) — таблица разбивается на «диапазоны» значений по полю или набору полей в таблице, без перекрытия диапазонов значений, отнесенных к различным партициям. Например, диапазоны дат;</p></li> <li><p>Партиционирование по списку значений (list) — таблица разбивается по спискам ключевых значений для каждой партиции.</p></li> </ul> <p>Чтобы настроить партиционирование таблицы, достаточно выполнить следующие действия:</p> <ul> <li><p>Создается «мастер» таблица, из которой все партиции будут наследоваться. Эта таблица не будет содержать данные. Также не нужно ставить никаких ограничений на таблицу, если конечно они не будут дублироваться на партиции;</p></li> <li><p>Создайте несколько «дочерних» таблиц, которые наследуют от «мастер» таблицы;</p></li> <li><p>Добавить в «дочерние» таблицы значения, по которым они будут партициями. Стоить заметить, что значения партиций не должны пересекаться. Например:</p> <pre><code>CHECK ( outletID BETWEEN 100 AND 200 )
CHECK ( outletID BETWEEN 200 AND 300 )</code></pre> <p>неверно заданы партиции, поскольку непонятно какой партиции принадлежит значение 200;</p></li> <li><p>Для каждой партиции создать индекс по ключевому полю (или нескольким), а также указать любые другие требуемые индексы;</p></li> <li><p>При необходимости, создать триггер или правило для перенаправления данных с «мастер» таблицы в соответствующую партицию;</p></li> <li><p>Убедиться, что параметр <code>constraint_exclusion</code> не отключен в postgresql.conf. Если его не включить, то запросы не будут оптимизированы при работе с партиционированием;</p></li> </ul> <h2 id="практика-использования">Практика использования</h2> <p>Теперь начнем с практического примера. Представим, что в нашей системе есть таблица, в которую мы собираем данные о посещаемости нашего ресурса. На любой запрос пользователя наша система логирует действия в эту таблицу. И, например, в начале каждого месяца (неделю) нам нужно создавать отчет за предыдущий месяц (неделю). При этом логи нужно хранить в течение 3 лет. Данные в такой таблице накапливаются быстро, если система активно используется. И вот, когда в таблице уже миллионы, а то и миллиарды записей, создавать отчеты становится все сложнее (да и чистка старых записей становится нелегким делом). Работа с такой таблицей создает огромную нагрузку на СУБД. Тут нам на помощь и приходит партиционирование.</p> <h3 id="настройка">Настройка</h3> <p>Для примера, мы имеем следующую таблицу:</p> <pre><code>CREATE TABLE my_logs (
    id              SERIAL PRIMARY KEY,
    user_id         INT NOT NULL,
    logdate         TIMESTAMP NOT NULL,
    data            TEXT,
    some_state      INT
);</code></pre> <p>Поскольку нам нужны отчеты каждый месяц, мы будем делить партиции по месяцам. Это поможет нам быстрее создавать отчеты и чистить старые данные.</p> <p>«Мастер» таблица будет <code>my_logs</code>, структуру которой мы указали выше. Далее создадим «дочерние» таблицы (партиции):</p> <pre><code>CREATE TABLE my_logs2010m10 (
    CHECK ( logdate &gt;= DATE &#39;2010-10-01&#39; AND logdate &lt; DATE &#39;2010-11-01&#39; )
) INHERITS (my_logs);
CREATE TABLE my_logs2010m11 (
    CHECK ( logdate &gt;= DATE &#39;2010-11-01&#39; AND logdate &lt; DATE &#39;2010-12-01&#39; )
) INHERITS (my_logs);
CREATE TABLE my_logs2010m12 (
    CHECK ( logdate &gt;= DATE &#39;2010-12-01&#39; AND logdate &lt; DATE &#39;2011-01-01&#39; )
) INHERITS (my_logs);
CREATE TABLE my_logs2011m01 (
    CHECK ( logdate &gt;= DATE &#39;2011-01-01&#39; AND logdate &lt; DATE &#39;2010-02-01&#39; )
) INHERITS (my_logs);</code></pre> <p>Данными командами мы создаем таблицы <code>my_logs2010m10</code>, <code>my_logs2010m11</code> и т.д., которые копируют структуру с «мастер» таблицы (кроме индексов). Также с помощью «CHECK» мы задаем диапазон значений, который будет попадать в эту партицию (хочу опять напомнить, что диапазоны значений партиций не должны пересекаться!). Поскольку партиционирование будет работать по полю <code>logdate</code>, мы создадим индекс на это поле на всех партициях:</p> <pre><code>CREATE INDEX my_logs2010m10_logdate ON my_logs2010m10 (logdate);
CREATE INDEX my_logs2010m11_logdate ON my_logs2010m11 (logdate);
CREATE INDEX my_logs2010m12_logdate ON my_logs2010m12 (logdate);
CREATE INDEX my_logs2011m01_logdate ON my_logs2011m01 (logdate);</code></pre> <p>Далее для удобства создадим функцию, которая будет перенаправлять новые данные с «мастер» таблицы в соответствующую партицию.</p> <pre><code>CREATE OR REPLACE FUNCTION my_logs_insert_trigger()
RETURNS TRIGGER AS $$
BEGIN
    IF ( NEW.logdate &gt;= DATE &#39;2010-10-01&#39; AND
         NEW.logdate &lt; DATE &#39;2010-11-01&#39; ) THEN
        INSERT INTO my_logs2010m10 VALUES (NEW.*);
    ELSIF ( NEW.logdate &gt;= DATE &#39;2010-11-01&#39; AND
            NEW.logdate &lt; DATE &#39;2010-12-01&#39; ) THEN
        INSERT INTO my_logs2010m11 VALUES (NEW.*);
    ELSIF ( NEW.logdate &gt;= DATE &#39;2010-12-01&#39; AND
            NEW.logdate &lt; DATE &#39;2011-01-01&#39; ) THEN
        INSERT INTO my_logs2010m12 VALUES (NEW.*);
    ELSIF ( NEW.logdate &gt;= DATE &#39;2011-01-01&#39; AND
            NEW.logdate &lt; DATE &#39;2011-02-01&#39; ) THEN
        INSERT INTO my_logs2011m01 VALUES (NEW.*);
    ELSE
        RAISE EXCEPTION &#39;Date out of range.  Fix the my_logs_insert_trigger() function!&#39;;
    END IF;
    RETURN NULL;
END;
$$
LANGUAGE plpgsql;</code></pre> <p>В функции ничего особенного нет: идет проверка поля <code>logdate</code>, по которой направляются данные в нужную партицию. При ненахождении требуемой партиции — вызываем ошибку. Теперь осталось создать триггер на «мастер» таблицу для автоматического вызова данной функции:</p> <pre><code>CREATE TRIGGER insert_my_logs_trigger
    BEFORE INSERT ON my_logs
    FOR EACH ROW EXECUTE PROCEDURE my_logs_insert_trigger();</code></pre> <p>Партиционирование настроено и теперь мы готовы приступить к тестированию.</p> <h3 id="тестирование">Тестирование</h3> <p>Для начала добавим данные в нашу таблицу <code>my_logs</code>:</p> <pre><code>INSERT INTO my_logs (user_id,logdate, data, some_state) VALUES(1, &#39;2010-10-30&#39;, &#39;30.10.2010 data&#39;, 1);
INSERT INTO my_logs (user_id,logdate, data, some_state) VALUES(2, &#39;2010-11-10&#39;, &#39;10.11.2010 data2&#39;, 1);
INSERT INTO my_logs (user_id,logdate, data, some_state) VALUES(1, &#39;2010-12-15&#39;, &#39;15.12.2010 data3&#39;, 1);</code></pre> <p>Теперь проверим где они хранятся:</p> <pre><code>partitioning_test=# SELECT * FROM ONLY my_logs;
 id | user_id | logdate | data | some_state
----+---------+---------+------+------------
(0 rows)</code></pre> <p>Как видим, в «мастер» таблицу данные не попали — она чиста. Теперь проверим, а есть ли вообще данные:</p> <pre><code>partitioning_test=# SELECT * FROM my_logs;
 id | user_id |       logdate       |       data       | some_state
----+---------+---------------------+------------------+------------
  1 |       1 | 2010-10-30 00:00:00 | 30.10.2010 data  |          1
  2 |       2 | 2010-11-10 00:00:00 | 10.11.2010 data2 |          1
  3 |       1 | 2010-12-15 00:00:00 | 15.12.2010 data3 |          1
(3 rows)</code></pre> <p>Данные при этом выводятся без проблем. Проверим партиции, правильно ли хранятся данные:</p> <pre><code>partitioning_test=# Select * from my_logs2010m10;
 id | user_id |       logdate       |      data       | some_state
----+---------+---------------------+-----------------+------------
  1 |       1 | 2010-10-30 00:00:00 | 30.10.2010 data |          1
(1 row)

partitioning_test=# Select * from my_logs2010m11;
 id | user_id |       logdate       |       data       | some_state
----+---------+---------------------+------------------+------------
  2 |       2 | 2010-11-10 00:00:00 | 10.11.2010 data2 |          1
(1 row)</code></pre> <p>Данные хранятся на требуемых нам партициях. При этом запросы к таблице <code>my_logs</code> менять не требуется:</p> <pre><code>partitioning_test=# SELECT * FROM my_logs WHERE user_id = 2;
 id | user_id |       logdate       |       data       | some_state
----+---------+---------------------+------------------+------------
  2 |       2 | 2010-11-10 00:00:00 | 10.11.2010 data2 |          1
(1 row)

partitioning_test=# SELECT * FROM my_logs WHERE data LIKE &#39;%0.1%&#39;;
 id | user_id |       logdate       |       data       | some_state
----+---------+---------------------+------------------+------------
  1 |       1 | 2010-10-30 00:00:00 | 30.10.2010 data  |          1
  2 |       2 | 2010-11-10 00:00:00 | 10.11.2010 data2 |          1
(2 rows)</code></pre> <h3 id="управление-партициями">Управление партициями</h3> <p>Обычно при работе с партиционированием старые партиции перестают получать данные и остаются неизменными. Это дает огромное преимущество над работой с данными через партиции. Например, нам нужно удалить старые логи за 2014 год, 10 месяц. Нам достаточно выполнить:</p> <pre><code>DROP TABLE my_logs2014m10;</code></pre> <p>поскольку <code>DROP TABLE</code> работает гораздо быстрее, чем удаление миллионов записей индивидуально через <code>DELETE</code>. Другой вариант, который более предпочтителен, просто удалить партицию из партиционирования, тем самым оставив данные в СУБД, но уже не доступные через «мастер» таблицу:</p> <pre><code>ALTER TABLE my_logs2014m10 NO INHERIT my_logs;</code></pre> <p>Это удобно, если мы хотим эти данные потом перенести в другое хранилище или просто сохранить.</p> <h3 id="важность-constraint_exclusion-для-партиционирования">Важность «constraint_exclusion» для партиционирования</h3> <p>Параметр <code>constraint_exclusion</code> отвечает за оптимизацию запросов, что повышает производительность для партиционированых таблиц. Например, выполним простой запрос:</p> <pre><code>partitioning_test=# SET constraint_exclusion = off;
partitioning_test=# EXPLAIN SELECT * FROM my_logs WHERE logdate &gt; &#39;2010-12-01&#39;;

                                            QUERY PLAN
---------------------------------------------------------------------------------------------------
 Result  (cost=6.81..104.66 rows=1650 width=52)
   -&gt;  Append  (cost=6.81..104.66 rows=1650 width=52)
         -&gt;  Bitmap Heap Scan on my_logs  (cost=6.81..20.93 rows=330 width=52)
               Recheck Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
               -&gt;  Bitmap Index Scan on my_logs_logdate  (cost=0.00..6.73 rows=330 width=0)
                     Index Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
         -&gt;  Bitmap Heap Scan on my_logs2010m10 my_logs  (cost=6.81..20.93 rows=330 width=52)
               Recheck Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
               -&gt;  Bitmap Index Scan on my_logs2010m10_logdate  (cost=0.00..6.73 rows=330 width=0)
                     Index Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
         -&gt;  Bitmap Heap Scan on my_logs2010m11 my_logs  (cost=6.81..20.93 rows=330 width=52)
               Recheck Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
               -&gt;  Bitmap Index Scan on my_logs2010m11_logdate  (cost=0.00..6.73 rows=330 width=0)
                     Index Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
         -&gt;  Bitmap Heap Scan on my_logs2010m12 my_logs  (cost=6.81..20.93 rows=330 width=52)
               Recheck Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
               -&gt;  Bitmap Index Scan on my_logs2010m12_logdate  (cost=0.00..6.73 rows=330 width=0)
                     Index Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
         -&gt;  Bitmap Heap Scan on my_logs2011m01 my_logs  (cost=6.81..20.93 rows=330 width=52)
               Recheck Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
               -&gt;  Bitmap Index Scan on my_logs2011m01_logdate  (cost=0.00..6.73 rows=330 width=0)
                     Index Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
(22 rows)</code></pre> <p>Как видно через команду <code>EXPLAIN</code>, данный запрос сканирует все партиции на наличие данных в них, что не логично, поскольку данное условие <code>logdate &gt; 2010-12-01</code> говорит о том, что данные должны браться только с партиций, где подходит такое условие. А теперь включим <code>constraint_exclusion</code>:</p> <pre><code>partitioning_test=# SET constraint_exclusion = on;
SET
partitioning_test=# EXPLAIN SELECT * FROM my_logs WHERE logdate &gt; &#39;2010-12-01&#39;;
                                            QUERY PLAN
---------------------------------------------------------------------------------------------------
 Result  (cost=6.81..41.87 rows=660 width=52)
   -&gt;  Append  (cost=6.81..41.87 rows=660 width=52)
         -&gt;  Bitmap Heap Scan on my_logs  (cost=6.81..20.93 rows=330 width=52)
               Recheck Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
               -&gt;  Bitmap Index Scan on my_logs_logdate  (cost=0.00..6.73 rows=330 width=0)
                     Index Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
         -&gt;  Bitmap Heap Scan on my_logs2010m12 my_logs  (cost=6.81..20.93 rows=330 width=52)
               Recheck Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
               -&gt;  Bitmap Index Scan on my_logs2010m12_logdate  (cost=0.00..6.73 rows=330 width=0)
                     Index Cond: (logdate &gt; &#39;2010-12-01 00:00:00&#39;::timestamp without time zone)
(10 rows)</code></pre> <p>Как мы видим, теперь запрос работает правильно и сканирует только партиции, что подходят под условие запроса. Но включать <code>constraint_exclusion</code> не желательно для баз, где нет партиционирования, поскольку команда <code>CHECK</code> будет проверяться на всех запросах, даже простых, а значит производительность сильно упадет. Начиная с 8.4 версии PostgreSQL <code>constraint_exclusion</code> может быть «on», «off» и «partition». По умолчанию (и рекомендуется) ставить <code>constraint_exclusion</code> «partition», который будет проверять <code>CHECK</code> только на партиционированых таблицах.</p> <h2 id=pg_partman>Pg_partman</h2> <p>Поскольку реализация партиционирования реализована неполноценно в PostgreSQL (для управления партициями и данными в них приходится писать функции, тригеры и правила), то существует расширение, которое автоматизирует полностью данный процесс. <a href="https://github.com/keithf4/pg_partman">PG Partition Manager</a>, он же pg_partman, это расширение для создания и управления партициями и партициями партиций (sub-partitoning) в PostgreSQL. Поддерживает партицирование по времени (time-based) или по последованности (serial-based). Для партицирования по диапазону значений (range) существует отдельное расширение <a href="https://github.com/moat/range_partitioning">Range Partitioning (range_partitioning)</a>.</p> <p>Текущая реализация поддерживает только INSERT операции, которые перенаправляют данные в нужную партицию. UPDATE операции, которые будут перемещать данные из одной партиции в другую, не поддерживаются. При попытке вставить данные, на которые нет партиции, pg_partman перемещает их в «мастер» (родительскую) таблицу. Данный вариант предпочтительнее, чем создавать автоматически новые партиции, поскольку это может привести к созданию десятков или сотен ненужных дочерных таблиц из-за ошибки в самих данных. Функция <code>check_parent</code> позволят проверить попадение подобных данных в родительскую таблицу и решить, что с ними требуется делать (удалить или использовать <code>partition_data_time/partition_data_id</code> для создания и переноса этих данных в партиции).</p> <p>Данное расширение использует большинство атрибутов родительской таблицы для создания партиций: индексы, внешние ключи (опционально), tablespace, constraints, privileges и ownership. Под такое условие попадают OID и UNLOGGED таблицы.</p> <p>Партициями партиций (sub-partitoning) поддерживаются разных уровней: time-&gt;time, id-&gt;id, time-&gt;id и id-&gt;time. Нет лимитов на создание таких партиций, но стоит помнить, что большое число партиций влияет на производительность родительской таблицы. Если размер партиций станет слишком большим, то придется увеличивать <code>max_locks_per_transaction</code> параметр для базы данных (64 по умолчанию).</p> <p>В PostgreSQL 9.4 появилась возможность создания пользовательских фоновых воркеров и динамически загружать их во время работы базы. Благодаря этому в pg_partman есть собственный фоновый воркер, задача которого запускать <code>run_maintenance</code> функцию каждый заданный промежуток времени. Если у Вас версия PostgreSQL ниже 9.4, то придется воспользоватся внешним планировщиком для выполнения данной функции (например cron). Задача данной функции - проверять и автоматически создавать партиции и опционально чистить старые.</p> <h3 id="пример-использования">Пример использования</h3> <p>Для начала установим данное расширение:</p> <pre><code>$ git clone https://github.com/keithf4/pg_partman.git
$ cd pg_partman/
$ make
$ sudo make install</code></pre> <p>Если не требуется использовать фоновый воркер, то можно собрать без него:</p> <pre><code>$ sudo make NO_BGW=1 install</code></pre> <p>Для работы фонового воркера нужно загружать его на старте PostgreSQL. Для этого потребуется добавить настройки в postgresql.conf:</p> <pre><code>shared_preload_libraries = &#39;pg_partman_bgw&#39;     # (change requires restart)
pg_partman_bgw.interval = 3600
pg_partman_bgw.role = &#39;myrole&#39;
pg_partman_bgw.dbname = &#39;mydatabase&#39;</code></pre> <p>где:</p> <ul> <li><p><code>pg_partman_bgw.dbname</code> — база данных, в которой будет выполняться <code>run_maintenance</code> функция. Если нужно указать больше одной базы, то они указываются через запятую. Без этого параметра воркер не будет работать;</p></li> <li><p><code>pg_partman_bgw.interval</code> — количество секунд между вызовами <code>run_maintenance</code> функции. По умолчанию 3600 (1 час);</p></li> <li><p><code>pg_partman_bgw.role</code> — роль для запуска <code>run_maintenance</code> функции. По умолчанию postgres. Разрешена только одна роль;</p></li> <li><p><code>pg_partman_bgw.analyze</code> — запускать или нет <code>ANALYZE</code> после создания партиций на родительскую таблицу. По умолчанию включено;</p></li> <li><p><code>pg_partman_bgw.jobmon</code> — разрешить или нет использовать <code>pg_jobmon</code> расширение для мониторинга, что партицирование работает без проблем. По умолчанию включено;</p></li> </ul> <p>Далее подключаемся к базе данных и активируем расширение:</p> <pre><code># CREATE SCHEMA partman;
CREATE SCHEMA
# CREATE EXTENSION pg_partman SCHEMA partman;
CREATE EXTENSION</code></pre> <p>Теперь можно приступать к использованию расширения. Создадим и заполним таблицу тестовыми данными:</p> <pre><code># CREATE TABLE users (
    id             serial primary key,
    username       text not null unique,
    password       text,
    created_on     timestamptz not null,
    last_logged_on timestamptz not null
);

# INSERT INTO users (username, password, created_on, last_logged_on)
  SELECT
      md5(random()::text),
      md5(random()::text),
      now() - &#39;1 years&#39;::interval * random(),
      now() - &#39;1 years&#39;::interval * random()
  FROM
      generate_series(1, 10000);</code></pre> <p>Далее активируем расширение для поля <code>created_on</code> с партицией на каждый год:</p> <pre><code># SELECT partman.create_parent(&#39;public.users&#39;, &#39;created_on&#39;, &#39;time&#39;, &#39;yearly&#39;);
 create_parent
---------------
 t
(1 row)</code></pre> <p>Указывание схемы в имени таблицы обязательно, даже если она «public» (первый аргумент функции).</p> <p>Поскольку родительская таблица уже была заполнена данными, перенесем данные из нее в партиции через <code>partition_data_time</code> функцию:</p> <pre><code># SELECT partman.check_parent();
     check_parent
----------------------
 (public.users,10000)
(1 row)

# SELECT partman.partition_data_time(&#39;public.users&#39;, 1000);
 partition_data_time
---------------------
               10000
(1 row)

# SELECT partman.check_parent();
 check_parent
--------------
(0 rows)

# SELECT * FROM ONLY users;
 id | username | password | created_on | last_logged_on
----+----------+----------+------------+----------------
(0 rows)

# \d+ users
                                                          Table &quot;public.users&quot;
     Column     |           Type           |                     Modifiers                      | Storage  | Stats target | Description
----------------+--------------------------+----------------------------------------------------+----------+--------------+-------------
 id             | integer                  | not null default nextval(&#39;users_id_seq&#39;::regclass) | plain    |              |
 username       | text                     | not null                                           | extended |              |
 password       | text                     |                                                    | extended |              |
 created_on     | timestamp with time zone | not null                                           | plain    |              |
 last_logged_on | timestamp with time zone | not null                                           | plain    |              |
Indexes:
    &quot;users_pkey&quot; PRIMARY KEY, btree (id)
    &quot;users_username_key&quot; UNIQUE CONSTRAINT, btree (username)
Triggers:
    users_part_trig BEFORE INSERT ON users FOR EACH ROW EXECUTE PROCEDURE users_part_trig_func()
Child tables: users_p2012,
              users_p2013,
              users_p2014,
              users_p2015,
              users_p2016,
              users_p2017,
              users_p2018,
              users_p2019,
              users_p2020</code></pre> <p>В результате данные в таблице <code>users</code> содержатся в партициях благодаря pg_partman. Более подробно по функционалу расширения, его настройках и ограничениях доступно в <a href="https://github.com/keithf4/pg_partman/blob/master/doc/pg_partman.md">официальной документации</a>.</p> <h2 id=pgslice>Pgslice</h2> <p><a href="https://github.com/ankane/pgslice">Pgslice</a> — утилита для создания и управления партициями в PostgreSQL. Утилита разбивает на «куски» как новую, так и существующию таблицу с данными c нулевым временем простоя («zero downtime»).</p> <p>Утилита написана на <a href="https://www.ruby-lang.org">Ruby</a>, поэтому потребуется сначала установить его. После этого устанавливаем pgslice через rubygems (многие ruby разработчики используют <a href="http://bundler.io/">bundler</a> для лучшего управления зависимостями, но в этой главе это не рассматривается):</p> <pre><code>$ gem install pgslice</code></pre> <p>Создадим и заполним таблицу тестовыми данными:</p> <pre><code># CREATE TABLE users (
    id             serial primary key,
    username       text not null unique,
    password       text,
    created_on     timestamptz not null,
    last_logged_on timestamptz not null
);

# INSERT INTO users (username, password, created_on, last_logged_on)
  SELECT
      md5(random()::text),
      md5(random()::text),
      now() + &#39;1 month&#39;::interval * random(),
      now() + &#39;1 month&#39;::interval * random()
  FROM
      generate_series(1, 10000);</code></pre> <p>Настройки подключения к базе задаются через <code>PGSLICE_URL</code> переменную окружения:</p> <pre><code>$ export PGSLICE_URL=postgres://username:password@localhost/mydatabase</code></pre> <p>Через команду <code>pgslice prep &lt;table&gt; &lt;column&gt; &lt;period&gt;</code> создадим таблицу <code>&lt;table&gt;_intermediate</code> (<code>users_intermediate</code> в примере) с соответствующим триггером для разбиения данных, где <code>&lt;table&gt;</code> - это название таблицы (<code>users</code> в примере), <code>&lt;column&gt;</code> - поле, по которому будут создаваться партиции, а <code>&lt;period&gt;</code> - период данных в партициях (может быть <code>day</code> или <code>month</code>).</p> <pre><code>$ pgslice prep users created_on month
BEGIN;

CREATE TABLE users_intermediate (LIKE users INCLUDING ALL);

CREATE FUNCTION users_insert_trigger()
    RETURNS trigger AS $$
    BEGIN
        RAISE EXCEPTION &#39;Create partitions first.&#39;;
    END;
    $$ LANGUAGE plpgsql;

CREATE TRIGGER users_insert_trigger
    BEFORE INSERT ON users_intermediate
    FOR EACH ROW EXECUTE PROCEDURE users_insert_trigger();

COMMENT ON TRIGGER users_insert_trigger ON users_intermediate is &#39;column:created_on,period:month,cast:timestamptz&#39;;

COMMIT;</code></pre> <p>Теперь можно добавить партиции:</p> <pre><code>$ pgslice add_partitions users --intermediate --past 3 --future 3
BEGIN;

CREATE TABLE users_201611
    (CHECK (created_on &gt;= &#39;2016-11-01 00:00:00 UTC&#39;::timestamptz AND created_on &lt; &#39;2016-12-01 00:00:00 UTC&#39;::timestamptz))
    INHERITS (users_intermediate);

ALTER TABLE users_201611 ADD PRIMARY KEY (id);

...

CREATE OR REPLACE FUNCTION users_insert_trigger()
    RETURNS trigger AS $$
    BEGIN
        IF (NEW.created_on &gt;= &#39;2017-02-01 00:00:00 UTC&#39;::timestamptz AND NEW.created_on &lt; &#39;2017-03-01 00:00:00 UTC&#39;::timestamptz) THEN
            INSERT INTO users_201702 VALUES (NEW.*);
        ELSIF (NEW.created_on &gt;= &#39;2017-03-01 00:00:00 UTC&#39;::timestamptz AND NEW.created_on &lt; &#39;2017-04-01 00:00:00 UTC&#39;::timestamptz) THEN
            INSERT INTO users_201703 VALUES (NEW.*);
        ELSIF (NEW.created_on &gt;= &#39;2017-04-01 00:00:00 UTC&#39;::timestamptz AND NEW.created_on &lt; &#39;2017-05-01 00:00:00 UTC&#39;::timestamptz) THEN
            INSERT INTO users_201704 VALUES (NEW.*);
        ELSIF (NEW.created_on &gt;= &#39;2017-05-01 00:00:00 UTC&#39;::timestamptz AND NEW.created_on &lt; &#39;2017-06-01 00:00:00 UTC&#39;::timestamptz) THEN
            INSERT INTO users_201705 VALUES (NEW.*);
        ELSIF (NEW.created_on &gt;= &#39;2017-01-01 00:00:00 UTC&#39;::timestamptz AND NEW.created_on &lt; &#39;2017-02-01 00:00:00 UTC&#39;::timestamptz) THEN
            INSERT INTO users_201701 VALUES (NEW.*);
        ELSIF (NEW.created_on &gt;= &#39;2016-12-01 00:00:00 UTC&#39;::timestamptz AND NEW.created_on &lt; &#39;2017-01-01 00:00:00 UTC&#39;::timestamptz) THEN
            INSERT INTO users_201612 VALUES (NEW.*);
        ELSIF (NEW.created_on &gt;= &#39;2016-11-01 00:00:00 UTC&#39;::timestamptz AND NEW.created_on &lt; &#39;2016-12-01 00:00:00 UTC&#39;::timestamptz) THEN
            INSERT INTO users_201611 VALUES (NEW.*);
        ELSE
            RAISE EXCEPTION &#39;Date out of range. Ensure partitions are created.&#39;;
        END IF;
        RETURN NULL;
    END;
    $$ LANGUAGE plpgsql;

COMMIT;</code></pre> <p>Через <code>--past</code> и <code>--future</code> опции указывается количество партиций. Далее можно переместить данные в партиции:</p> <pre><code>$ pgslice fill users
/* 1 of 1 */
INSERT INTO users_intermediate (&quot;id&quot;, &quot;username&quot;, &quot;password&quot;, &quot;created_on&quot;, &quot;last_logged_on&quot;)
    SELECT &quot;id&quot;, &quot;username&quot;, &quot;password&quot;, &quot;created_on&quot;, &quot;last_logged_on&quot; FROM users
    WHERE id &gt; 0 AND id &lt;= 10000 AND created_on &gt;= &#39;2016-11-01 00:00:00 UTC&#39;::timestamptz AND created_on &lt; &#39;2017-06-01 00:00:00 UTC&#39;::timestamptz</code></pre> <p>Через <code>--batch-size</code> и <code>--sleep</code> опции можно управлять скоростью переноса данных.</p> <p>После этого можно переключиться на новую таблицу с партициями:</p> <pre><code>$ pgslice swap users
BEGIN;

SET LOCAL lock_timeout = &#39;5s&#39;;

ALTER TABLE users RENAME TO users_retired;

ALTER TABLE users_intermediate RENAME TO users;

ALTER SEQUENCE users_id_seq OWNED BY users.id;

COMMIT;</code></pre> <p>Если требуется, то можно перенести часть данных, что накопилась между переключением таблиц:</p> <pre><code>$ pgslice fill users --swapped</code></pre> <p>В результате таблица <code>users</code> будет работать через партиции:</p> <pre><code>$ psql -c &quot;EXPLAIN SELECT * FROM users&quot;
                               QUERY PLAN
------------------------------------------------------------------------
 Append  (cost=0.00..330.00 rows=13601 width=86)
   -&gt;  Seq Scan on users  (cost=0.00..0.00 rows=1 width=84)
   -&gt;  Seq Scan on users_201611  (cost=0.00..17.20 rows=720 width=84)
   -&gt;  Seq Scan on users_201612  (cost=0.00..17.20 rows=720 width=84)
   -&gt;  Seq Scan on users_201701  (cost=0.00..17.20 rows=720 width=84)
   -&gt;  Seq Scan on users_201702  (cost=0.00..166.48 rows=6848 width=86)
   -&gt;  Seq Scan on users_201703  (cost=0.00..77.52 rows=3152 width=86)
   -&gt;  Seq Scan on users_201704  (cost=0.00..17.20 rows=720 width=84)
   -&gt;  Seq Scan on users_201705  (cost=0.00..17.20 rows=720 width=84)
(9 rows)</code></pre> <p>Старая таблица теперь будет называться <code>&lt;table&gt;_retired</code> (<code>users_retired</code> в примере). Её можно оставить или удалить из базы.</p> <pre><code>$ pg_dump -c -Fc -t users_retired $PGSLICE_URL &gt; users_retired.dump
$ psql -c &quot;DROP users_retired&quot; $PGSLICE_URL</code></pre> <p>Далее только требуется следить за количеством партиций. Для этого команду <code>pgslice add_partitions</code> можно добавить в cron:</p> <pre><code># day
0 0 * * * pgslice add_partitions &lt;table&gt; --future 3 --url ...

# month
0 0 1 * * pgslice add_partitions &lt;table&gt; --future 3 --url ...</code></pre> <h2 id="заключение-1">Заключение</h2> <p>Партиционирование — одна из самых простых и менее безболезненных методов уменьшения нагрузки на СУБД. Именно на этот вариант стоит посмотреть сперва, и если он не подходит по каким либо причинам — переходить к более сложным.</p> <h1 id="репликация">Репликация</h1> <h2 id="введение-3">Введение</h2> <p>Репликация (англ. replication) — механизм синхронизации содержимого нескольких копий объекта (например, содержимого базы данных). Репликация — это процесс, под которым понимается копирование данных из одного источника на множество других и наоборот. При репликации изменения, сделанные в одной копии объекта, могут быть распространены в другие копии. Репликация может быть синхронной или асинхронной.</p> <p>В случае синхронной репликации, если данная реплика обновляется, все другие реплики того же фрагмента данных также должны быть обновлены в одной и той же транзакции. Логически это означает, что существует лишь одна версия данных. В большинстве продуктов синхронная репликация реализуется с помощью триггерных процедур (возможно, скрытых и управляемых системой). Но синхронная репликация имеет тот недостаток, что она создаёт дополнительную нагрузку при выполнении всех транзакций, в которых обновляются какие-либо реплики (кроме того, могут возникать проблемы, связанные с доступностью данных).</p> <p>В случае асинхронной репликации обновление одной реплики распространяется на другие спустя некоторое время, а не в той же транзакции. Таким образом, при асинхронной репликации вводится задержка, или время ожидания, в течение которого отдельные реплики могут быть фактически неидентичными (то есть определение реплика оказывается не совсем подходящим, поскольку мы не имеем дело с точными и своевременно созданными копиями). В большинстве продуктов асинхронная репликация реализуется посредством чтения журнала транзакций или постоянной очереди тех обновлений, которые подлежат распространению. Преимущество асинхронной репликации состоит в том, что дополнительные издержки репликации не связаны с транзакциями обновлений, которые могут иметь важное значение для функционирования всего предприятия и предъявлять высокие требования к производительности. К недостаткам этой схемы относится то, что данные могут оказаться несовместимыми (то есть несовместимыми с точки зрения пользователя). Иными словами, избыточность может проявляться на логическом уровне, а это, строго говоря, означает, что термин контролируемая избыточность в таком случае не применим.</p> <p>Рассмотрим кратко проблему согласованности (или, скорее, несогласованности). Дело в том, что реплики могут становиться несовместимыми в результате ситуаций, которые трудно (или даже невозможно) избежать и последствия которых трудно исправить. В частности, конфликты могут возникать по поводу того, в каком порядке должны применяться обновления. Например, предположим, что в результате выполнения транзакции А происходит вставка строки в реплику X, после чего транзакция B удаляет эту строку, а также допустим, что Y — реплика X. Если обновления распространяются на Y, но вводятся в реплику Y в обратном порядке (например, из-за разных задержек при передаче), то транзакция B не находит в Y строку, подлежащую удалению, и не выполняет своё действие, после чего транзакция А вставляет эту строку. Суммарный эффект состоит в том, что реплика Y содержит указанную строку, а реплика X — нет.</p> <p>В целом задачи устранения конфликтных ситуаций и обеспечения согласованности реплик являются весьма сложными. Следует отметить, что, по крайней мере, в сообществе пользователей коммерческих баз данных термин репликация стал означать преимущественно (или даже исключительно) асинхронную репликацию.</p> <p>Основное различие между репликацией и управлением копированием заключается в следующем: если используется репликация, то обновление одной реплики в конечном счёте распространяется на все остальные автоматически. В режиме управления копированием, напротив, не существует такого автоматического распространения обновлений. Копии данных создаются и управляются с помощью пакетного или фонового процесса, который отделён во времени от транзакций обновления. Управление копированием в общем более эффективно по сравнению с репликацией, поскольку за один раз могут копироваться большие объёмы данных. К недостаткам можно отнести то, что большую часть времени копии данных не идентичны базовым данным, поэтому пользователи должны учитывать, когда именно были синхронизированы эти данные. Обычно управление копированием упрощается благодаря тому требованию, чтобы обновления применялись в соответствии со схемой первичной копии того или иного вида.</p> <p>Для репликации PostgreSQL существует несколько решений, как закрытых, так и свободных. Закрытые системы репликации не будут рассматриваться в этой книге. Вот список свободных решений:</p> <ul> <li><p><a href="http://www.slony.info/">Slony-I</a> — асинхронная Master-Slave репликация, поддерживает каскады(cascading) и отказоустойчивость(failover). Slony-I использует триггеры PostgreSQL для привязки к событиям INSERT/DELETE/UPDATE и хранимые процедуры для выполнения действий;</p></li> <li><p><a href="http://pgpool.projects.postgresql.org/">Pgpool-I/II</a> — это замечательный инструмент для PostgreSQL (лучше сразу работать с II версией). Позволяет делать:</p> <ul> <li><p>репликацию (в том числе, с автоматическим переключением на резервный stand-by сервер);</p></li> <li><p>online-бэкап;</p></li> <li><p>pooling коннектов;</p></li> <li><p>очередь соединений;</p></li> <li><p>балансировку SELECT-запросов на несколько postgresql-серверов;</p></li> <li><p>разбиение запросов для параллельного выполнения над большими объемами данных;</p></li> </ul></li> <li><p><a href="http://bucardo.org/">Bucardo</a> — асинхронная репликация, которая поддерживает Multi-Master и Master-Slave режимы, а также несколько видов синхронизации и обработки конфликтов;</p></li> <li><p><a href="http://skytools.projects.postgresql.org/doc/londiste.ref.html">Londiste</a> — асинхронная Master-Slave репликация. Входит в состав <a href="http://pgfoundry.org/projects/skytools/">Skytools</a>. Проще в использовании, чем Slony-I;</p></li> <li><p><a href="http://www.commandprompt.com/products/mammothreplicator/">Mammoth Replicator</a> — асинхронная Multi-Master репликация;</p></li> <li><p><a href="http://2ndquadrant.com/en/resources/bdr/">BDR (Bi-Directional Replication)</a> — асинхронная Multi-Master репликация;</p></li> <li><p><a href="http://2ndquadrant.com/en/resources/pglogical/">Pglogical</a> — асинхронная Master-Slave репликация;</p></li> </ul> <p>Это, конечно, не весь список свободных систем для репликации, но даже из этого есть что выбрать для PostgreSQL.</p> <h2 id="потоковая-репликация-streaming-replication">Потоковая репликация (Streaming Replication)</h2> <p><a href="https://wiki.postgresql.org/wiki/Streaming_Replication">Потоковая репликация (Streaming Replication, SR)</a> дает возможность непрерывно отправлять и применять WAL (Write-Ahead Log) записи на резервные сервера для создания точной копии текущего. Данная функциональность появилась у PostgreSQL начиная с 9 версии. Этот тип репликации простой, надежный и, вероятней всего, будет использоваться в качестве стандартной репликации в большинстве высоконагруженных приложений, что используют PostgreSQL.</p> <p>Отличительными особенностями решения являются:</p> <ul> <li><p>репликация всего инстанса PostgreSQL;</p></li> <li><p>асинхронный или синхронный механизмы репликации;</p></li> <li><p>простота установки;</p></li> <li><p>мастер база данных может обслуживать огромное количество слейвов из-за минимальной нагрузки;</p></li> </ul> <p>К недостаткам можно отнести:</p> <ul> <li><p>невозможность реплицировать только определенную базу данных из всех на PostgreSQL инстансе;</p></li> </ul> <h3 id="установка">Установка</h3> <p>Для начала нам потребуется PostgreSQL не ниже 9 версии. Все работы, как полагается, будут проводится на Linux.</p> <h3 id="настройка-1">Настройка</h3> <p>Обозначим мастер сервер как <code>masterdb(192.168.0.10)</code> и слейв как <code>slavedb(192.168.0.20)</code>.</p> <h4 id="предварительная-настройка">Предварительная настройка</h4> <p>Для начала позволим определенному пользователю без пароля ходить по ssh. Пусть это будет <code>postgres</code> юзер. Если же нет, то создаем набором команд:</p> <pre><code>$ sudo groupadd userssh
$ sudo useradd -m -g userssh -d /home/userssh -s /bin/bash \
-c &quot;user ssh allow&quot; userssh</code></pre> <p>Дальше выполняем команды от имени пользователя (в данном случае <code>postgres</code>):</p> <pre><code>$ su postgres</code></pre> <p>Генерим RSA-ключ для обеспечения аутентификации в условиях отсутствия возможности использовать пароль:</p> <pre><code>$ ssh-keygen -t rsa -P &quot;&quot;
Generating public/private rsa key pair.
Enter file in which to save the key (/var/lib/postgresql/.ssh/id_rsa):
Created directory &#39;/var/lib/postgresql/.ssh&#39;.
Your identification has been saved in /var/lib/postgresql/.ssh/id_rsa.
Your public key has been saved in /var/lib/postgresql/.ssh/id_rsa.pub.
The key fingerprint is:
16:08:27:97:21:39:b5:7b:86:e1:46:97:bf:12:3d:76 postgres@localhost</code></pre> <p>И добавляем его в список авторизованных ключей:</p> <pre><code>$ cat $HOME/.ssh/id_rsa.pub » $HOME/.ssh/authorized_keys</code></pre> <p>Проверить работоспособность соединения можно просто написав:</p> <pre><code>$ ssh localhost</code></pre> <p>Не забываем предварительно инициализировать <code>sshd</code>:</p> <pre><code>$ $/etc/init.d/sshd start</code></pre> <p>После успешно проделаной операции скопируйте <code>$HOME/.ssh</code> на <code>slavedb</code>. Теперь мы должны иметь возможность без пароля заходить с мастера на слейв и со слейва на мастер через ssh.</p> <p>Также отредактируем <code>pg_hba.conf</code> на мастере и слейве, разрешив им друг к другу доступ без пароля (тут добавляется роль <code>replication</code>):</p> <pre><code>host  replication  all  192.168.0.20/32  trust</code></pre> <pre><code>host  replication  all  192.168.0.10/32  trust</code></pre> <p>Не забываем после этого перегрузить postgresql на обоих серверах.</p> <h4 id="настройка-мастера">Настройка мастера</h4> <p>Для начала настроим masterdb. Установим параметры в <code>postgresql.conf</code> для репликации:</p> <pre><code># To enable read-only queries on a standby server, wal_level must be set to
# &quot;hot_standby&quot;. But you can choose &quot;archive&quot; if you never connect to the
# server in standby mode.
wal_level = hot_standby

# Set the maximum number of concurrent connections from the standby servers.
max_wal_senders = 5

# To prevent the primary server from removing the WAL segments required for
# the standby server before shipping them, set the minimum number of segments
# retained in the pg_xlog directory. At least wal_keep_segments should be
# larger than the number of segments generated between the beginning of
# online-backup and the startup of streaming replication. If you enable WAL
# archiving to an archive directory accessible from the standby, this may
# not be necessary.
wal_keep_segments = 32

# Enable WAL archiving on the primary to an archive directory accessible from
# the standby. If wal_keep_segments is a high enough number to retain the WAL
# segments required for the standby server, this may not be necessary.
archive_mode    = on
archive_command = &#39;cp %p /path_to/archive/%f&#39;</code></pre> <p>Давайте по порядку:</p> <ul> <li><p><code>wal_level = hot_standby</code> — сервер начнет писать в WAL логи так же как и при режиме «archive», добавляя информацию, необходимую для восстановления транзакции (можно также поставить <code>archive</code>, но тогда сервер не может быть слейвом при необходимости);</p></li> <li><p><code>max_wal_senders = 5</code> — максимальное количество слейвов;</p></li> <li><p><code>wal_keep_segments = 32</code> — минимальное количество файлов c WAL сегментами в <code>pg_xlog</code> директории;</p></li> <li><p><code>archive_mode = on</code> — позволяем сохранять WAL сегменты в указанное переменной <code>archive_command</code> хранилище. В данном случае в директорию <code>/path/to/archive/</code>;</p></li> </ul> <p>По умолчанию репликация асинхронная. В версии 9.1 добавили параметр <code>synchronous_standby_names</code>, который включает синхронную репликацию. В данные параметр передается <code>application_name</code>, который используется на слейвах в <code>recovery.conf</code>:</p> <pre><code>restore_command = &#39;cp /mnt/server/archivedir/%f %p&#39;               # e.g. &#39;cp /mnt/server/archivedir/%f %p&#39;
standby_mode = on
primary_conninfo = &#39;host=masterdb port=59121 user=replication password=replication application_name=newcluster&#39;            # e.g. &#39;host=localhost port=5432&#39;
trigger_file = &#39;/tmp/trig_f_newcluster&#39;</code></pre> <p>После изменения параметров перегружаем PostgreSQL сервер. Теперь перейдем к <code>slavedb</code>.</p> <h4 id="subsec:streaming-slave-settings">Настройка слейва</h4> <p>Для начала нам потребуется создать на <code>slavedb</code> точную копию <code>masterdb</code>. Перенесем данные с помощью «Онлайн бэкапа».</p> <p>Переместимся на <code>masterdb</code> сервер и выполним в консоли:</p> <pre><code>$ psql -c &quot;SELECT pg_start_backup(&#39;label&#39;, true)&quot;</code></pre> <p>Теперь нам нужно перенести данные с мастера на слейв. Выполняем на мастере:</p> <pre><code>$ rsync -C -a --delete -e ssh --exclude postgresql.conf --exclude postmaster.pid \
--exclude postmaster.opts --exclude pg_log --exclude pg_xlog \
--exclude recovery.conf master_db_datadir/ slavedb_host:slave_db_datadir/</code></pre> <p>где</p> <ul> <li><p><code>master_db_datadir</code> — директория с postgresql данными на masterdb;</p></li> <li><p><code>slave_db_datadir</code> — директория с postgresql данными на slavedb;</p></li> <li><p><code>slavedb_host</code> — хост slavedb(в нашем случае - 192.168.1.20);</p></li> </ul> <p>После копирования данных с мастера на слейв, остановим онлайн бэкап. Выполняем на мастере:</p> <pre><code>$ psql -c &quot;SELECT pg_stop_backup()&quot;</code></pre> <p>Для версии PostgreSQL 9.1+ можно воспользоватся командой <code>pg_basebackup</code> (копирует базу на <code>slavedb</code> подобным образом):</p> <pre><code>$ pg_basebackup -R -D /srv/pgsql/standby --host=192.168.0.10 --port=5432</code></pre> <p>Устанавливаем такие же данные в конфиге <code>postgresql.conf</code>, что и у мастера (чтобы при падении мастера слейв мог его заменить). Так же установим дополнительный параметр:</p> <pre><code>hot_standby = on</code></pre> <p>Внимание! Если на мастере поставили <code>wal_level = archive</code>, тогда параметр оставляем по умолчанию (<code>hot_standby = off</code>).</p> <p>Далее на <code>slavedb</code> в директории с данными PostgreSQL создадим файл <code>recovery.conf</code> с таким содержимым:</p> <pre><code># Specifies whether to start the server as a standby. In streaming replication,
# this parameter must to be set to on.
standby_mode          = &#39;on&#39;

# Specifies a connection string which is used for the standby server to connect
# with the primary.
primary_conninfo      = &#39;host=192.168.0.10 port=5432 user=postgres&#39;

# Specifies a trigger file whose presence should cause streaming replication to
# end (i.e., failover).
trigger_file = &#39;/path_to/trigger&#39;

# Specifies a command to load archive segments from the WAL archive. If
# wal_keep_segments is a high enough number to retain the WAL segments
# required for the standby server, this may not be necessary. But
# a large workload can cause segments to be recycled before the standby
# is fully synchronized, requiring you to start again from a new base backup.
restore_command = &#39;scp masterdb_host:/path_to/archive/%f &quot;%p&quot;&#39;</code></pre> <p>где</p> <ul> <li><p><code>standby_mode='on'</code> — указываем серверу работать в режиме слейв;</p></li> <li><p><code>primary_conninfo</code> — настройки соединения слейва с мастером;</p></li> <li><p><code>trigger_file</code> — указываем триггер файл, при наличии которого будет остановлена репликация;</p></li> <li><p><code>restore_command</code> — команда, которой будут восстанавливаться WAL логи. В нашем случае через scp копируем с masterdb (<code>masterdb_host</code> - хост masterdb);</p></li> </ul> <p>Теперь можем запустить PostgreSQL на <code>slavedb</code>.</p> <h4 id="тестирование-репликации">Тестирование репликации</h4> <p>В результате можем посмотреть отставание слейвов от мастера с помощью таких команд:</p> <pre><code>$ psql -c &quot;SELECT pg_current_xlog_location()&quot; -h192.168.0.10 (masterdb)
 pg_current_xlog_location
--------------------------
 0/2000000
(1 row)

$ psql -c &quot;select pg_last_xlog_receive_location()&quot; -h192.168.0.20 (slavedb)
 pg_last_xlog_receive_location
-------------------------------
 0/2000000
(1 row)

$ psql -c &quot;select pg_last_xlog_replay_location()&quot; -h192.168.0.20 (slavedb)
 pg_last_xlog_replay_location
------------------------------
 0/2000000
(1 row)</code></pre> <p>Начиная с версии 9.1 добавили дополнительные view для просмотра состояния репликации. Теперь master знает все состояния slaves:</p> <pre><code># SELECT * from pg_stat_replication ;
  procpid | usesysid |   usename   | application_name | client_addr | client_hostname | client_port |        backend_start         |   state   | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state
 ---------+----------+-------------+------------------+-------------+-----------------+-------------+------------------------------+-----------+---------------+----------------+----------------+-----------------+---------------+------------
    17135 |    16671 | replication | newcluster       | 127.0.0.1   |                 |       43745 | 2011-05-22 18:13:04.19283+02 | streaming | 1/30008750    | 1/30008750     | 1/30008750     | 1/30008750      |             1 | sync</code></pre> <p>Также с версии 9.1 добавили view <code>pg_stat_database_conflicts</code>, с помощью которой на слейв базах можно просмотреть сколько запросов было отменено и по каким причинам:</p> <pre><code># SELECT * from pg_stat_database_conflicts ;
  datid |  datname  | confl_tablespace | confl_lock | confl_snapshot | confl_bufferpin | confl_deadlock
 -------+-----------+------------------+------------+----------------+-----------------+----------------
      1 | template1 |                0 |          0 |              0 |               0 |              0
  11979 | template0 |                0 |          0 |              0 |               0 |              0
  11987 | postgres  |                0 |          0 |              0 |               0 |              0
  16384 | marc      |                0 |          0 |              1 |               0 |              0</code></pre> <p>Еще проверить работу репликации можно с помощью утилиты <code>ps</code>:</p> <pre><code>$ ps -ef | grep sender
postgres  6879  6831  0 10:31 ?        00:00:00 postgres: wal sender process postgres 127.0.0.1(44663) streaming 0/2000000

[slavedb] $ ps -ef | grep receiver
postgres  6878  6872  1 10:31 ?        00:00:01 postgres: wal receiver process   streaming 0/2000000</code></pre> <p>Давайте проверим реприкацию и выполним на мастере:</p> <pre><code>$ psql test_db
test_db=# create table test3(id int not null primary key,name varchar(20));
NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index &quot;test3_pkey&quot; for table &quot;test3&quot;
CREATE TABLE
test_db=# insert into test3(id, name) values(&#39;1&#39;, &#39;test1&#39;);
INSERT 0 1
test_db=#</code></pre> <p>Теперь проверим на слейве результат:</p> <pre><code>$ psql test_db
test_db=# select * from test3;
 id | name
----+-------
  1 | test1
(1 row)</code></pre> <p>Как видим, таблица с данными успешно скопирована с мастера на слейв. Более подробно по настройке данной репликации можно почитать из <a href="https://wiki.postgresql.org/wiki/Streaming_Replication">официальной wiki</a>.</p> <h3 id="общие-задачи">Общие задачи</h3> <h4 id="переключение-на-слейв-при-падении-мастера">Переключение на слейв при падении мастера</h4> <p>Достаточно создать триггер файл (<code>trigger_file</code>) на слейве, который перестанет читать данные с мастера.</p> <h4 id="остановка-репликации-на-слейве">Остановка репликации на слейве</h4> <p>Создать триггер файл (<code>trigger_file</code>) на слейве. Также с версии 9.1 добавили функции <code>pg_xlog_replay_pause()</code> и <code>pg_xlog_replay_resume()</code> для остановки и возобновления репликации.</p> <h4 id="перезапуск-репликации-после-сбоя">Перезапуск репликации после сбоя</h4> <p>Повторяем операции из раздела «». Хочется заметить, что мастер при этом не нуждается в остановке при выполнении данной задачи.</p> <h4 id="перезапуск-репликации-после-сбоя-слейва">Перезапуск репликации после сбоя слейва</h4> <p>Перезагрузить PostgreSQL на слейве после устранения сбоя.</p> <h4 id="повторно-синхронизировать-репликации-на-слейве">Повторно синхронизировать репликации на слейве</h4> <p>Это может потребоваться, например, после длительного отключения от мастера. Для этого останавливаем PostgreSQL на слейве и повторяем операции из раздела «».</p> <h3 id=repmgr>Repmgr</h3> <p><a href="http://www.repmgr.org/">Repmgr</a> — набор инструментов для управления потоковой репликацией и восстановления после сбоя кластера PostgreSQL серверов. Он автоматизирует настройку резервных серверов, мониторинг репликации, а также помогает выполнять задачи администрированию кластера, такие как отказоустойчивость (failover) или переключение мастера-слейва (слейв становится мастером, а мастер - слейвом). Repmgr работает с версии PostgreSQL 9.3 и выше.</p> <p>Repmgr состоит из двух утилит:</p> <ul> <li><p><code>repmgr</code> — инструмент командной строки (cli), который используется для административных задач, таких как:</p> <ul> <li><p>создание слейвов;</p></li> <li><p>переключение слейва в режим мастера;</p></li> <li><p>переключение между собой мастер и слейв серверов;</p></li> <li><p>отображение состояния кластера;</p></li> </ul></li> <li><p><code>repmgrd</code> — демон, который мониторит кластер серверов и выполняет такие задачи:</p> <ul> <li><p>мониторинг и логирование эффективности репликации;</p></li> <li><p>автоматическое переключение слейва в мастер при обнаружении проблем у текущего мастера (failover);</p></li> <li><p>посылка сообщений о событиях в кластере через заданые пользователем скрипты;</p></li> </ul></li> </ul> <h4 id="пример-использования-автоматическое-переключение-слейва-в-мастер">Пример использования: автоматическое переключение слейва в мастер</h4> <p>Для использования failover потребуется добавить <code>repmgr_funcs</code> в <code>postgresql.conf</code>:</p> <pre><code>shared_preload_libraries = &#39;repmgr_funcs&#39;</code></pre> <p>И добавить настройки в <code>repmgr.conf</code>:</p> <pre><code>failover=automatic
promote_command=&#39;repmgr standby promote -f /etc/repmgr.conf --log-to-file&#39;
follow_command=&#39;repmgr standby follow -f /etc/repmgr.conf --log-to-file&#39;</code></pre> <p>Для демонстрации автоматического failover, настроен кластер с тремя узлами репликации (один мастер и два слейв сервера), так что таблица <code>repl_nodes</code> выглядит следующим образом:</p> <pre><code># SELECT id, type, upstream_node_id, priority, active FROM repmgr_test.repl_nodes ORDER BY id;
 id |  type   | upstream_node_id | priority | active
----+---------+------------------+----------+--------
  1 | master  |                  |      100 | t
  2 | standby |                1 |      100 | t
  3 | standby |                1 |      100 | t
(3 rows)</code></pre> <p>После запуска <code>repmgrd</code> демона на каждом сервере в режиме ожидания, убеждаемся что он мониторит кластер:</p> <pre><code>checking cluster configuration with schema &#39;repmgr_test&#39;
[2016-01-05 13:15:40] [INFO] checking node 2 in cluster &#39;test&#39;
[2016-01-05 13:15:40] [INFO] reloading configuration file and updating repmgr tables
[2016-01-05 13:15:40] [INFO] starting continuous standby node monitoring</code></pre> <p>Теперь остановим мастер базу:</p> <pre><code>pg_ctl -D /path/to/node1/data -m immediate stop</code></pre> <p><code>repmgrd</code> автоматически замечает падение мастера и переключает один из слейвов в мастер:</p> <pre><code>connection to upstream has been lost, trying to recover... 15 seconds before failover decision
[2016-01-06 18:33:03] [WARNING] connection to upstream has been lost, trying to recover... 10 seconds before failover decision
[2016-01-06 18:33:08] [WARNING] connection to upstream has been lost, trying to recover... 5 seconds before failover decision
...
[2016-01-06 18:33:18] [NOTICE] this node is the best candidate to be the new master, promoting...
...
[2016-01-06 18:33:20] [NOTICE] STANDBY PROMOTE successful</code></pre> <p>Также переключает оставшийся слейв на новый мастер:</p> <pre><code>connection to upstream has been lost, trying to recover... 15 seconds before failover decision
[2016-01-06 18:33:03] [WARNING] connection to upstream has been lost, trying to recover... 10 seconds before failover decision
[2016-01-06 18:33:08] [WARNING] connection to upstream has been lost, trying to recover... 5 seconds before failover decision
...
[2016-01-06 18:33:23] [NOTICE] node 2 is the best candidate for new master, attempting to follow...
[2016-01-06 18:33:23] [INFO] changing standby&#39;s master
...
[2016-01-06 18:33:25] [NOTICE] node 3 now following new upstream node 2</code></pre> <p>Таблица <code>repl_nodes</code> будет обновлена, чтобы отразить новую ситуацию — старый мастер <code>node1</code> помечен как неактивный, и слейв <code>node3</code> теперь работает от нового мастера <code>node2</code>:</p> <pre><code># SELECT id, type, upstream_node_id, priority, active from repl_nodes ORDER BY id;
 id |  type   | upstream_node_id | priority | active
----+---------+------------------+----------+--------
  1 | master  |                  |      100 | f
  2 | master  |                  |      100 | t
  3 | standby |                2 |      100 | t
(3 rows)</code></pre> <p>В таблицу <code>repl_events</code> будут добавлены записи того, что произошло с каждым сервером во время failover:</p> <pre><code># SELECT node_id, event, successful, details from repmgr_test.repl_events where event_timestamp&gt;=&#39;2016-01-06 18:30&#39;;
 node_id |          event           | successful |                         details
---------+--------------------------+------------+----------------------------------------------------------
       2 | standby_promote          | t          | node 2 was successfully promoted to master
       2 | repmgrd_failover_promote | t          | node 2 promoted to master; old master 1 marked as failed
       3 | repmgrd_failover_follow  | t          | node 3 now following new upstream node 2
(3 rows)</code></pre> <h4 id="заключение-2">Заключение</h4> <p>Более подробно по функционалу, его настройках и ограничениях доступно в <a href="https://github.com/2ndQuadrant/repmgr/blob/master/README.md">официальном репозитории</a>.</p> <h3 id=patroni>Patroni</h3> <p><a href="https://github.com/zalando/patroni">Patroni</a> — это демон на Python, позволяющий автоматически обслуживать кластеры PostgreSQL с потоковой репликацией.</p> <p>Особенности:</p> <ul> <li><p>Использует потоковую PostgreSQL репликацию (асинхронная и синхронная репликация);</p></li> <li><p>Интеграция с <a href="https://kubernetes.io/">Kubernetes</a>;</p></li> <li><p>Поддержания актуальности кластера и выборов мастера используются распределенные <a href="https://en.wikipedia.org/wiki/Distributed_control_system">DCS</a> хранилища (поддерживаются <a href="https://zookeeper.apache.org/">Zookeeper</a>, <a href="https://coreos.com/etcd">Etcd</a> или <a href="https://www.consul.io/">Consul</a>);</p></li> <li><p>Автоматическое «service discovery» и динамическая реконфигурация кластера;</p></li> <li><p>Состояние кластера можно получить как запросами в DCS, так и напрямую к Patroni через HTTP запросы;</p></li> </ul> <p>Информация по настройке и использованию Patroni находится в <a href="https://patroni.readthedocs.io/en/latest/">официальной документации</a> проекта.</p> <h3 id=stolon>Stolon</h3> <p><a href="https://github.com/sorintlab/stolon">Stolon</a> — это демон на Go, позволяющий автоматически обслуживать кластеры PostgreSQL с потоковой репликацией.</p> <p>Особенности:</p> <ul> <li><p>Использует потоковую PostgreSQL репликацию (асинхронная и синхронная репликация);</p></li> <li><p>Интеграция с <a href="https://kubernetes.io/">Kubernetes</a>;</p></li> <li><p>Поддержания актуальности кластера и выборов мастера используются распределенные <a href="https://en.wikipedia.org/wiki/Distributed_control_system">DCS</a> хранилища (поддерживаются <a href="https://coreos.com/etcd">Etcd</a> или <a href="https://www.consul.io/">Consul</a>);</p></li> <li><p>Автоматическое «service discovery» и динамическая реконфигурация кластера;</p></li> <li><p>Состояние кластера можно получить как запросами в DCS, так и через <code>stolonctl</code> клиент;</p></li> </ul> <p>Stolon состоит из 3 основных компонентов:</p> <ul> <li><p>keeper (хранитель): управляет экземпляром PostgreSQL;</p></li> <li><p>sentinel: обнаруживает и контролирует keeper-ров и вычисляет оптимальное состояние кластера;</p></li> <li><p>proxy: точка доступа клиента, обеспечивает подключение к верному PostgreSQL мастеру в кластере;</p></li> </ul> <p>Информация по настройке и использованию Stolon находится в <a href="https://github.com/sorintlab/stolon/blob/master/doc/README.md">официальной документации</a> проекта.</p> <h2 id="sec:bdr">PostgreSQL Bi-Directional Replication (BDR)</h2> <p><a href="https://2ndquadrant.com/en/resources/bdr/">BDR (Bi-Directional Replication)</a> это новая функциональность добавленая в ядро PostgreSQL которая предоставляет расширенные средства для репликации. На данный момент это реализовано в виде небольшого патча и модуля для 9.4 версии. Заявлено что полностью будет только в PostgreSQL 9.6 (разработчики решили не заниматься поддержкой патча для 9.5, а сосредоточиться на добавление патчей в сам PostgreSQL). BDR позволяет создавать географически распределенные асинхронные мульти-мастер конфигурации используя для этого встроенную логическую потоковую репликацию LLSR (Logical Log Streaming Replication).</p> <p>BDR не является инструментом для кластеризации, т.к. здесь нет каких-либо глобальных менеджеров блокировок или координаторов транзакций. Каждый узел не зависит от других, что было бы невозможно в случае использования менеджеров блокировки. Каждый из узлов содержит локальную копию данных идентичную данным на других узлах. Запросы также выполняются только локально. При этом каждый из узлов внутренне консистентен в любое время, целиком же группа серверов является согласованной в конечном счете (eventually consistent). Уникальность BDR заключается в том что она непохожа ни на встроенную потоковую репликацию, ни на существующие trigger-based решения (Londiste, Slony, Bucardo).</p> <p>Самым заметным отличием от потоковой репликации является то, что BDR (LLSR) оперирует базами (per-database replication), а классическая PLSR реплицирует целиком инстанс (per-cluster replication), т.е. все базы внутри инстанса. Существующие ограничения и особенности:</p> <ul> <li><p>Все изменения данных вызываемые <code>INSERT/DELETE/UPDATE</code> реплицируются (<code>TRUNCATE</code> на момент написания статьи пока не реализован);</p></li> <li><p>Большинство операции изменения схемы (DDL) реплицируются успешно. Неподдерживаемые DDL фиксируются модулем репликации и отклоняются с выдачей ошибкой (на момент написания не работал <code>CREATE TABLE ... AS</code>);</p></li> <li><p>Определения таблиц, типов, расширений и т.п. должны быть идентичными между upstream и downstream мастерами;</p></li> <li><p>Действия которые отражаются в WAL, но непредставляются в виде логических изменений не реплицируются на другой узел (запись полных страниц, вакуумация таблиц и т.п.). Таким образом логическая потоковая репликация (LLSR) избавлена от некоторой части накладных расходов которые присутствуют в физической потоковой репликации PLSR (тем не менее это не означает что LLSR требуется меньшая пропускная способность сети чем для PLSR);</p></li> </ul> <p>Небольшое примечание: временная остановка репликации осуществляется выключением downstream мастера. Однако стоит отметить что остановленная реплика приводит к тому что upstream мастер продолжит накапливать WAL журналы что в свою очередь может привести к неконтролируемому расходу пространства на диске. Поэтому крайне не рекомендуется надолго выключать реплику. Удаление реплики навсегда осуществляется через удаление конфигурации BDR на downstream сервере с последующим перезапуском downstream мастера. Затем нужно удалить соответствующий слот репликации на upstream мастере с помощью функции <code>pg_drop_replication_slot('slotname')</code>. Доступные слоты можно просмотреть с помощью функции <code>pg_get_replication_slots</code>.</p> <p>На текущий момент собрать BDR можно из исходников по <a href="https://wiki.postgresql.org/wiki/BDR_Quick_Start">данному мануалу</a>. С официальным принятием данных патчей в ядро PostgreSQL данный раздел про BDR будет расширен и дополнен.</p> <h2 id="sec:pglogical">Pglogical</h2> <p><a href="https://2ndquadrant.com/en/resources/pglogical/">Pglogical</a> — это расширение для PostgreSQL, которое использует логическое декодирование через publish/subscribe модель. Данное расширение базируется на BDR проекте ([sec:bdr] ). Расширение работает только начиная с версии PostgreSQL 9.4 и выше (из-за логического декодирования). Для разных вариаций обнаружения и разрешения конфликтов требуется версия 9.5 и выше.</p> <p>Используются следующие термины для описания pglogical:</p> <ul> <li><p>Nodes(ноды, узлы) — экземпляры баз данных PostgreSQL;</p></li> <li><p>Provider и subscriber — роли узлов. Provider выполняют выдачу данных и изменений для subscriber-ов;</p></li> <li><p>Replication set (набор для репликации) — коллекция таблиц и последовательностей для репликации;</p></li> </ul> <p>Сценарии использования pglogical:</p> <ul> <li><p>Обновление между версиями PostgreSQL (например c 9.4 на 9.5);</p></li> <li><p>Полная репликация базы данных;</p></li> <li><p>Выборочная репликация таблиц;</p></li> <li><p>Сбор данных с нескольких баз данных в одну;</p></li> </ul> <p>Архитектурные детали:</p> <ul> <li><p>Pglogical работает на уровне каждой базы данных, а не на весь сервер;</p></li> <li><p>Provider (publisher) может «кормить» несколько subscriber-ов без дополнительных накладных расходов записи на диск;</p></li> <li><p>Один subscriber может объединить изменения из нескольких provider-ов и использовать систему обнаружения и разрешения конфликтов между изменениями;</p></li> <li><p>Каскадная репликация осуществляется в виде переадресации изменений;</p></li> </ul> <h3 id="установка-и-настройка">Установка и настройка</h3> <p>Установить pglogical можно по <a href="https://2ndquadrant.com/en/resources/pglogical/pglogical-installation-instructions/">данной документации</a>. Далее требуется настроить логический декодинг в PostgreSQL:</p> <pre><code>wal_level = &#39;logical&#39;
max_worker_processes = 10   # one per database needed on provider node
                            # one per node needed on subscriber node
max_replication_slots = 10  # one per node needed on provider node
max_wal_senders = 10        # one per node needed on provider node
shared_preload_libraries = &#39;pglogical&#39;</code></pre> <p>Если используется PostgreSQL 9.5+ и требуется механизмы разрешения конфликтов, то требуется добавить дополнительные опции:</p> <pre><code>track_commit_timestamp = on # needed for last/first update wins conflict resolution
                            # property available in PostgreSQL 9.5+</code></pre> <p>В <code>pg_hba.conf</code> нужно разрешить replication соеденения с локального хоста для пользователя с привилегией репликации. После перезапуска базы нужно активировать расширение на всех нодах:</p> <pre><code>CREATE EXTENSION pglogical;</code></pre> <p>Далее на master (мастер) создаем provider (процесс, который будет выдавать изменения для subscriber-ов) ноду:</p> <pre><code>SELECT pglogical.create_node(
    node_name := &#39;provider1&#39;,
    dsn := &#39;host=providerhost port=5432 dbname=db&#39;
);</code></pre> <p>И добавляем все таблицы в <code>public</code> схеме:</p> <pre><code>SELECT pglogical.replication_set_add_all_tables(&#39;default&#39;, ARRAY[&#39;public&#39;]);</code></pre> <p>Далее переходим на slave (слейв) и создаем subscriber ноду:</p> <pre><code>SELECT pglogical.create_node(
    node_name := &#39;subscriber1&#39;,
    dsn := &#39;host=thishost port=5432 dbname=db&#39;
);</code></pre> <p>После этого создаем «подписку» на provider ноду, которая начнет синхронизацию и репликацию в фоне:</p> <pre><code>SELECT pglogical.create_subscription(
    subscription_name := &#39;subscription1&#39;,
    provider_dsn := &#39;host=providerhost port=5432 dbname=db&#39;
);</code></pre> <p>Если все проделано верно, subscriber через определенный интервал времени subscriber нода должна получить точную копию всех таблиц в <code>public</code> схеме с master хоста.</p> <h3 id="разрешение-конфликтов">Разрешение конфликтов</h3> <p>Если используется схема, где subscriber нода подписана на данные из нескольких provider-ов, или же на subscriber дополнительно производятся локальные изменения данных, могут возникать конфликты для новых изменений. В pglogical встроен механизм для обнаружения и разрешения конфликтов. Настройка данного механизма происходит через <code>pglogical.conflict_resolution</code> ключ. Поддерживаются следующие значения:</p> <ul> <li><p><code>error</code> - репликация остановится на ошибке, если обнаруживается конфликт и потребуется ручное действие для его разрешения;</p></li> <li><p><code>apply_remote</code> - всегда применить изменения, который конфликтуют с локальными данными. Значение по умолчанию;</p></li> <li><p><code>keep_local</code> - сохранить локальную версию данных и игнорировать конфликтующие изменения, которые исходят от provider-а;</p></li> <li><p><code>last_update_wins</code> - версия данных с самым новым коммитом (newest commit timestamp) будет сохранена;</p></li> <li><p><code>first_update_wins</code> - версия данных с самым старым коммитом (oldest commit timestamp) будет сохранена;</p></li> </ul> <p>Когда опция <code>track_commit_timestamp</code> отключена, единственное допустимое значение для <code>pglogical.conflict_resolution</code> может быть <code>apply_remote</code>. Поскольку <code>track_commit_timestamp</code> не доступен в PostgreSQL 9.4, данная опция установлена по умолчанию в <code>apply_remote</code>.</p> <h3 id="ограничения-и-недостатки">Ограничения и недостатки</h3> <ul> <li><p>Для работы требуется суперпользователь;</p></li> <li><p><code>UNLOGGED</code> и <code>TEMPORARY</code> таблицы не реплицируются;</p></li> <li><p>Для каждой базы данных нужно настроить отдельный provider и subscriber;</p></li> <li><p>Требуется primary key или <a href="http://www.postgresql.org/docs/current/static/sql-altertable.html#SQL-CREATETABLE-REPLICA-IDENTITY">replica identity</a> для репликации;</p></li> <li><p>Разрешен только один уникальный индекс/ограничение/основной ключ на таблицу (из-за возможных конфликтов). Возможно использовать больше, но только в случае если subscriber читает только с одного provider и не производится локальных изменений данных на нем;</p></li> <li><p>Автоматическая репликация DDL не поддерживается. У pglogical есть команда <code>pglogical.replicate_ddl_command</code> для запуска DDL на provider и subscriber;</p></li> <li><p>Ограничения на foreign ключи не выполняются на subscriber-рах;</p></li> <li><p>При использовании <code>TRUNCATE ... CASCADE</code> будет выполнен <code>CASCADE</code> только на provider;</p></li> <li><p>Последовательности реплицируются периодически, а не в режиме реального времени;</p></li> </ul> <h2 id="sec:slonyI">Slony-I</h2> <p><a href="http://slony.info/">Slony</a> это система репликации реального времени, позволяющая организовать синхронизацию нескольких серверов PostgreSQL по сети. Slony использует триггеры PostgreSQL для привязки к событиям INSERT/DELETE/UPDATE и хранимые процедуры для выполнения действий.</p> <p>Система Slony с точки зрения администратора состоит из двух главных компонент: репликационного демона <code>slony</code> и административной консоли <code>slonik</code>. Администрирование системы сводится к общению со <code>slonik</code>-ом, демон <code>slon</code> только следит за собственно процессом репликации.</p> <p>Все команды slonik принимает на свой stdin. До начала выполнения скрипт slonik-a проверяется на соответствие синтаксису, если обнаруживаются ошибки, скрипт не выполняется, так что можно не волноваться если slonik сообщает о syntax error, ничего страшного не произошло. И он ещё ничего не сделал. Скорее всего.</p> <h3 id="установка-1">Установка</h3> <p>Установка на Ubuntu производится простой командой:</p> <pre><code>$ sudo aptitude install slony1-2-bin</code></pre> <h3 id="subsec:slonyI-settings">Настройка</h3> <p>Рассмотрим установку на гипотетическую базу данных customers. Исходные данные:</p> <ul> <li><p><code>customers</code> — база данных;</p></li> <li><p><code>master_host</code> — хост master базы;</p></li> <li><p><code>slave_host</code> — хост slave базы;</p></li> <li><p><code>customers_rep</code> — имя кластера;</p></li> </ul> <h4 id="subsec:slonyI-settings-1">Подготовка master базы</h4> <p>Для начала нужно создать пользователя в базе, под которым будет действовать Slony. По умолчанию, и отдавая должное системе, этого пользователя обычно называют slony.</p> <pre><code>$ createuser -a -d slony
$ psql -d template1 -c &quot;ALTER USER slony WITH PASSWORD &#39;slony_user_password&#39;;&quot;</code></pre> <p>Также на каждом из узлов лучше завести системного пользователя slony, чтобы запускать от его имени репликационного демона slon. В дальнейшем подразумевается, что он (и пользователь и slon) есть на каждом из узлов кластера.</p> <h4 id="subsec:slonyI-settings-2">Подготовка slave базы</h4> <p>Здесь рассматривается, что серверы кластера соединены посредством сети. Необходимо чтобы с каждого из серверов можно было установить соединение с PostgreSQL на master хосте, и наоборот. То есть, команда:</p> <pre><code>anyuser@customers_slave$ psql -d customers \
-h customers_master.com -U slony</code></pre> <p>должна подключать нас к мастер-серверу (после ввода пароля, желательно).</p> <p>Теперь устанавливаем на slave-хост сервер PostgreSQL. Следующего обычно не требуется, сразу после установки Postgres «up and ready», но в случае каких-то ошибок можно начать «с чистого листа», выполнив следующие команды (предварительно сохранив конфигурационные файлы и остановив postmaster):</p> <pre><code>pgsql@customers_slave$ rm -rf $PGDATA
pgsql@customers_slave$ mkdir $PGDATA
pgsql@customers_slave$ initdb -E UTF8 -D $PGDATA
pgsql@customers_slave$ createuser -a -d slony
pgsql@customers_slave$ psql -d template1 -c &quot;alter \
user slony with password &#39;slony_user_password&#39;;&quot;</code></pre> <p>Далее запускаем postmaster. Обычно требуется определённый владелец для реплицируемой БД. В этом случае необходимо создать его тоже:</p> <pre><code>pgsql@customers_slave$ createuser -a -d customers_owner
pgsql@customers_slave$ psql -d template1 -c &quot;alter \
user customers_owner with password &#39;customers_owner_password&#39;;&quot;</code></pre> <p>Эти две команды можно запускать с <code>customers_master</code>, к командной строке в этом случае нужно добавить <code>-h customers_slave</code>, чтобы все операции выполнялись на slave.</p> <p>На slave, как и на master, также нужно установить Slony.</p> <h4 id="инициализация-бд-и-plpgsql-на-slave">Инициализация БД и plpgsql на slave</h4> <p>Следующие команды выполняются от пользователя slony. Скорее всего для выполнения каждой из них потребуется ввести пароль (<code>slony_user_password</code>):</p> <pre><code>slony@customers_master$ createdb -O customers_owner \
-h customers_slave.com customers
slony@customers_master$ createlang -d customers \
-h customers_slave.com plpgsql</code></pre> <p>Внимание! Все таблицы, которые будут добавлены в replication set должны иметь primary key. Если какая-то из таблиц не удовлетворяет этому условию, задержитесь на этом шаге и дайте каждой таблице primary key командой <code>ALTER TABLE ADD PRIMARY KEY</code>. Если столбца который мог бы стать primary key не находится, добавьте новый столбец типа serial (<code>ALTER TABLE ADD COLUMN</code>), и заполните его значениями. Настоятельно НЕ рекомендую использовать <code>table add key</code> slonik-a.</p> <p>Далее создаём таблицы и всё остальное на slave базе:</p> <pre><code>slony@customers_master$ pg_dump -s customers | \
psql -U slony -h customers_slave.com customers</code></pre> <p><code>pg_dump -s</code> сдампит только структуру нашей БД.</p> <p><code>pg_dump -s customers</code> должен пускать без пароля, а вот для <code>psql -U slony -h customers_slave.com customers</code> придётся набрать пароль (<code>slony_user_pass</code>). Важно: подразумевается что сейчас на мастер-хосте ещё не установлен Slony (речь не про <code>make install</code>), то есть в БД нет таблиц <code>sl_*</code>, триггеров и прочего.</p> <h4 id="инициализация-кластера">Инициализация кластера</h4> <p>Сейчас мы имеем два сервера PostgreSQL которые свободно «видят» друг друга по сети, на одном из них находится мастер-база с данными, на другом — только структура базы. Далее мастер-хосте запускаем скрипт:</p> <pre><code>#!/bin/sh

CLUSTER=customers_rep

DBNAME1=customers
DBNAME2=customers

HOST1=customers_master.com
HOST2=customers_slave.com

PORT1=5432
PORT2=5432

SLONY_USER=slony

slonik «EOF
cluster name = $CLUSTER;
node 1 admin conninfo = &#39;dbname=$DBNAME1 host=$HOST1 port=$PORT1
user=slony password=slony_user_password&#39;;
node 2 admin conninfo = &#39;dbname=$DBNAME2 host=$HOST2
port=$PORT2 user=slony password=slony_user_password&#39;;
init cluster ( id = 1, comment = &#39;Customers DB
replication cluster&#39; );

echo &#39;Create set&#39;;

create set ( id = 1, origin = 1, comment = &#39;Customers
DB replication set&#39; );

echo &#39;Adding tables to the subscription set&#39;;

echo &#39; Adding table public.customers_sales...&#39;;
set add table ( set id = 1, origin = 1, id = 4, full qualified
name = &#39;public.customers_sales&#39;, comment = &#39;Table public.customers_sales&#39; );
echo &#39; done&#39;;

echo &#39; Adding table public.customers_something...&#39;;
set add table ( set id = 1, origin = 1, id = 5, full qualified
name = &#39;public.customers_something,
comment = &#39;Table public.customers_something );
echo &#39; done&#39;;

echo &#39;done adding&#39;;
store node ( id = 2, comment = &#39;Node 2, $HOST2&#39; );
echo &#39;stored node&#39;;
store path ( server = 1, client = 2, conninfo = &#39;dbname=$DBNAME1 host=$HOST1
port=$PORT1 user=slony password=slony_user_password&#39; );
echo &#39;stored path&#39;;
store path ( server = 2, client = 1, conninfo = &#39;dbname=$DBNAME2 host=$HOST2
port=$PORT2 user=slony password=slony_user_password&#39; );

store listen ( origin = 1, provider = 1, receiver = 2 );
store listen ( origin = 2, provider = 2, receiver = 1 );
EOF</code></pre> <p>Здесь инициализируется кластер, создается replication set, включаются в него две таблицы. Нужно перечислить все таблицы, которые нужно реплицировать. Replication set запоминается раз и навсегда. Чтобы добавить узел в схему репликации не нужно заново инициализировать set. Если в набор добавляется или удаляется таблица нужно переподписать все узлы. То есть сделать <code>unsubscribe</code> и <code>subscribe</code> заново.</p> <h4 id="подписываем-slave-узел-на-replication-set">Подписываем slave-узел на replication set</h4> <p>Далее запускаем на слейве:</p> <pre><code>#!/bin/sh

CLUSTER=customers_rep

DBNAME1=customers
DBNAME2=customers

HOST1=customers_master.com
HOST2=customers_slave.com

PORT1=5432
PORT2=5432

SLONY_USER=slony

slonik «EOF
cluster name = $CLUSTER;
node 1 admin conninfo = &#39;dbname=$DBNAME1 host=$HOST1
port=$PORT1 user=slony password=slony_user_password&#39;;
node 2 admin conninfo = &#39;dbname=$DBNAME2 host=$HOST2
port=$PORT2 user=slony password=slony_user_password&#39;;

echo&#39;subscribing&#39;;
subscribe set ( id = 1, provider = 1, receiver = 2, forward = no);

EOF</code></pre> <h4 id="старт-репликации">Старт репликации</h4> <p>Теперь, на обоих узлах необходимо запустить демона репликации.</p> <pre><code>slony@customers_master$ slon customers_rep \
&quot;dbname=customers user=slony&quot;</code></pre> <p>и</p> <pre><code>slony@customers_slave$ slon customers_rep \
&quot;dbname=customers user=slony&quot;</code></pre> <p>Cлоны обменяются сообщениями и начнут передачу данных. Начальное наполнение происходит с помощью <code>COPY</code> команды, слейв база в это время полностью блокируется.</p> <h3 id="общие-задачи-1">Общие задачи</h3> <h4 id="добавление-ещё-одного-узла-в-работающую-схему-репликации">Добавление ещё одного узла в работающую схему репликации</h4> <p>Требуется выполнить [subsec:slonyI-settings-1] и [subsec:slonyI-settings-2] этапы. Новый узел имеет id = 3. Находится на хосте <code>customers_slave3.com</code>, «видит» мастер-сервер по сети и мастер может подключиться к его PostgreSQL. После дублирования структуры (п [subsec:slonyI-settings].2) делается следующее:</p> <pre><code>slonik «EOF
cluster name = customers_slave;
node 3 admin conninfo = &#39;dbname=customers host=customers_slave3.com
port=5432 user=slony password=slony_user_pass&#39;;
uninstall node (id = 3);
echo &#39;okay&#39;;
EOF</code></pre> <p>Это нужно чтобы удалить схему, триггеры и процедуры, которые были сдублированы вместе с таблицами и структурой БД. Инициализировать кластер не надо. Вместо этого записываем информацию о новом узле в сети:</p> <pre><code>#!/bin/sh

CLUSTER=customers_rep

DBNAME1=customers
DBNAME3=customers

HOST1=customers_master.com
HOST3=customers_slave3.com

PORT1=5432
PORT2=5432

SLONY_USER=slony

slonik «EOF
cluster name = $CLUSTER;
node 1 admin conninfo = &#39;dbname=$DBNAME1 host=$HOST1
port=$PORT1 user=slony password=slony_user_pass&#39;;
node 3 admin conninfo = &#39;dbname=$DBNAME3
host=$HOST3 port=$PORT2 user=slony password=slony_user_pass&#39;;

echo &#39;done adding&#39;;

store node ( id = 3, comment = &#39;Node 3, $HOST3&#39; );
echo &#39;sored node&#39;;
store path ( server = 1, client = 3, conninfo = &#39;dbname=$DBNAME1
host=$HOST1 port=$PORT1 user=slony password=slony_user_pass&#39; );
echo &#39;stored path&#39;;
store path ( server = 3, client = 1, conninfo = &#39;dbname=$DBNAME3
host=$HOST3 port=$PORT2 user=slony password=slony_user_pass&#39; );

echo &#39;again&#39;;
store listen ( origin = 1, provider = 1, receiver = 3 );
store listen ( origin = 3, provider = 3, receiver = 1 );

EOF</code></pre> <p>Новый узел имеет id 3, потому что 2 уже работает. Подписываем новый узел 3 на replication set:</p> <pre><code>#!/bin/sh

CLUSTER=customers_rep

DBNAME1=customers
DBNAME3=customers

HOST1=customers_master.com
HOST3=customers_slave3.com

PORT1=5432
PORT2=5432

SLONY_USER=slony

slonik «EOF
cluster name = $CLUSTER;
node 1 admin conninfo = &#39;dbname=$DBNAME1 host=$HOST1
port=$PORT1 user=slony password=slony_user_pass&#39;;
node 3 admin conninfo = &#39;dbname=$DBNAME3 host=$HOST3
port=$PORT2 user=slony password=slony_user_pass&#39;;

echo&#39;subscribing&#39;;
subscribe set ( id = 1, provider = 1, receiver = 3, forward = no);

EOF</code></pre> <p>Теперь запускаем slon на новом узле, так же как и на остальных. Перезапускать slon на мастере не надо.</p> <pre><code>slony@customers_slave3$ slon customers_rep \
&quot;dbname=customers user=slony&quot;</code></pre> <p>Репликация должна начаться как обычно.</p> <h3 id="устранение-неисправностей">Устранение неисправностей</h3> <h4 id="ошибка-при-добавлении-узла-в-систему-репликации">Ошибка при добавлении узла в систему репликации</h4> <p>Периодически, при добавлении новой машины в кластер возникает следующая ошибка: на новой ноде всё начинает жужжать и работать, имеющиеся же отваливаются с примерно следующей диагностикой:</p> <pre><code>%slon customers_rep &quot;dbname=customers user=slony_user&quot;
CONFIG main: slon version 1.0.5 starting up
CONFIG main: local node id = 3
CONFIG main: loading current cluster configuration
CONFIG storeNode: no_id=1 no_comment=&#39;CustomersDB
replication cluster&#39;
CONFIG storeNode: no_id=2 no_comment=&#39;Node 2,
node2.example.com&#39;
CONFIG storeNode: no_id=4 no_comment=&#39;Node 4,
node4.example.com&#39;
CONFIG storePath: pa_server=1 pa_client=3
pa_conninfo=&quot;dbname=customers
host=mainhost.com port=5432 user=slony_user
password=slony_user_pass&quot; pa_connretry=10
CONFIG storeListen: li_origin=1 li_receiver=3
li_provider=1
CONFIG storeSet: set_id=1 set_origin=1
set_comment=&#39;CustomersDB replication set&#39;
WARN remoteWorker_wakeup: node 1 - no worker thread
CONFIG storeSubscribe: sub_set=1 sub_provider=1 sub_forward=&#39;f&#39;
WARN remoteWorker_wakeup: node 1 - no worker thread
CONFIG enableSubscription: sub_set=1
WARN remoteWorker_wakeup: node 1 - no worker thread
CONFIG main: configuration complete - starting threads
CONFIG enableNode: no_id=1
CONFIG enableNode: no_id=2
CONFIG enableNode: no_id=4
ERROR remoteWorkerThread_1: &quot;begin transaction; set
transaction isolation level
serializable; lock table &quot;_customers_rep&quot;.sl_config_lock; select
&quot;_customers_rep&quot;.enableSubscription(1, 1, 4);
notify &quot;_customers_rep_Event&quot;; notify &quot;_customers_rep_Confirm&quot;;
insert into &quot;_customers_rep&quot;.sl_event (ev_origin, ev_seqno,
ev_timestamp, ev_minxid, ev_maxxid, ev_xip,
ev_type , ev_data1, ev_data2, ev_data3, ev_data4 ) values
(&#39;1&#39;, &#39;219440&#39;,
&#39;2005-05-05 18:52:42.708351&#39;, &#39;52501283&#39;, &#39;52501292&#39;,
&#39;&#39;&#39;52501283&#39;&#39;&#39;, &#39;ENABLE_SUBSCRIPTION&#39;,
&#39;1&#39;, &#39;1&#39;, &#39;4&#39;, &#39;f&#39;); insert into &quot;_customers_rep&quot;.
sl_confirm (con_origin, con_received,
con_seqno, con_timestamp) values (1, 3, &#39;219440&#39;,
CURRENT_TIMESTAMP); commit transaction;&quot;
PGRES_FATAL_ERROR ERROR: insert or update on table
&quot;sl_subscribe&quot; violates foreign key
constraint &quot;sl_subscribe-sl_path-ref&quot;
DETAIL: Key (sub_provider,sub_receiver)=(1,4)
is not present in table &quot;sl_path&quot;.
INFO remoteListenThread_1: disconnecting from
&#39;dbname=customers host=mainhost.com
port=5432 user=slony_user password=slony_user_pass&#39;
%</code></pre> <p>Это означает что в служебной таблице <code>_&lt;имя кластера&gt;.sl_path</code>, например <code>_customers_rep.sl_path</code> на уже имеющихся узлах отсутствует информация о новом узле. В данном случае, id нового узла 4, пара (1,4) в <code>sl_path</code> отсутствует. Чтобы это устранить, нужно выполнить на каждом из имеющихся узлов приблизительно следующий запрос:</p> <pre><code>$ psql -d customers -h _every_one_of_slaves -U slony
customers=# insert into _customers_rep.sl_path
values (&#39;1&#39;,&#39;4&#39;,&#39;dbname=customers host=mainhost.com
port=5432 user=slony_user password=slony_user_password,&#39;10&#39;);</code></pre> <p>Если возникают затруднения, то можно посмотреть на служебные таблицы и их содержимое. Они не видны обычно и находятся в рамках пространства имён <code>_&lt;имя кластера&gt;</code>, например <code>_customers_rep</code>.</p> <h4 id="что-делать-если-репликация-со-временем-начинает-тормозить">Что делать если репликация со временем начинает тормозить</h4> <p>В процессе эксплуатации может наблюдатся как со временем растёт нагрузка на master-сервере, в списке активных бекендов — постоянные SELECT-ы со слейвов. В <code>pg_stat_activity</code> видны примерно такие запросы:</p> <pre><code>select ev_origin, ev_seqno, ev_timestamp, ev_minxid, ev_maxxid, ev_xip,
ev_type, ev_data1, ev_data2, ev_data3, ev_data4, ev_data5, ev_data6,
ev_data7, ev_data8 from &quot;_customers_rep&quot;.sl_event e where
(e.ev_origin = &#39;2&#39; and e.ev_seqno &gt; &#39;336996&#39;) or
(e.ev_origin = &#39;3&#39; and e.ev_seqno &gt; &#39;1712871&#39;) or
(e.ev_origin = &#39;4&#39; and e.ev_seqno &gt; &#39;721285&#39;) or
(e.ev_origin = &#39;5&#39; and e.ev_seqno &gt; &#39;807715&#39;) or
(e.ev_origin = &#39;1&#39; and e.ev_seqno &gt; &#39;3544763&#39;) or
(e.ev_origin = &#39;6&#39; and e.ev_seqno &gt; &#39;2529445&#39;) or
(e.ev_origin = &#39;7&#39; and e.ev_seqno &gt; &#39;2512532&#39;) or
(e.ev_origin = &#39;8&#39; and e.ev_seqno &gt; &#39;2500418&#39;) or
(e.ev_origin = &#39;10&#39; and e.ev_seqno &gt; &#39;1692318&#39;)
order by e.ev_origin, e.ev_seqno;</code></pre> <p>где <code>_customers_rep</code> — имя схемы из примера. Таблица <code>sl_event</code> почему-то разрастается со временем, замедляя выполнение этих запросов до неприемлемого времени. Удаляем ненужные записи:</p> <pre><code>delete from _customers_rep.sl_event where
ev_timestamp&lt;NOW()-&#39;1 DAY&#39;::interval;</code></pre> <p>Производительность должна вернуться к изначальным значениям. Возможно имеет смысл почистить таблицы <code>_customers_rep.sl_log_*</code> где вместо звёздочки подставляются натуральные числа, по-видимому по количеству репликационных сетов, так что <code>_customers_rep.sl_log_1</code> точно должна существовать.</p> <h2 id="sec:londiste">Londiste</h2> <p><a href="http://pgfoundry.org/projects/skytools">Londiste</a> представляет собой движок для организации репликации, написанный на языке Python. Основные принципы: надежность и простота использования. Из-за этого данное решение имеет меньше функциональности, чем Slony-I. Londiste использует в качестве транспортного механизма очередь PgQ (описание этого более чем интересного проекта остается за рамками данной главы, поскольку он представляет интерес скорее для низкоуровневых программистов баз данных, чем для конечных пользователей — администраторов СУБД PostgreSQL). Отличительными особенностями решения являются:</p> <ul> <li><p>возможность потабличной репликации;</p></li> <li><p>начальное копирование ничего не блокирует;</p></li> <li><p>возможность двухстороннего сравнения таблиц;</p></li> <li><p>простота установки;</p></li> </ul> <p>К недостаткам можно отнести:</p> <ul> <li><p>триггерная репликация, что ухудшает производительность базы;</p></li> </ul> <h3 id="установка-2">Установка</h3> <p>Установка будет проводиться на Debian сервере. Поскольку Londiste — это часть Skytools, то нам нужно ставить этот пакет:</p> <pre><code>% sudo aptitude install skytools</code></pre> <p>В некоторых системнах пакет может содержатся версия 2.x, которая не поддерживает каскадную репликацию, отказоустойчивость(failover) и переключение между серверами (switchover). По этой причине она не будет расматриваться. Скачать самую последнюю версию пакета можно с <a href="http://pgfoundry.org/projects/skytools">официального сайта</a>. На момент написания главы последняя версия была 3.2. Начнем установку:</p> <pre><code>$ wget http://pgfoundry.org/frs/download.php/3622/skytools-3.2.tar.gz
$ tar zxvf skytools-3.2.tar.gz
$ cd skytools-3.2/
# пакеты для сборки deb
$ sudo aptitude install build-essential autoconf \
automake autotools-dev dh-make \
debhelper devscripts fakeroot xutils lintian pbuilder \
python-all-dev python-support xmlto asciidoc \
libevent-dev libpq-dev libtool
# python-psycopg нужен для работы Londiste
$ sudo aptitude install python-psycopg2 postgresql-server-dev-all
# данной командой собираем deb пакет
$ make deb
$ cd ../
# ставим skytools
$ dpkg -i *.deb</code></pre> <p>Для других систем можно собрать Skytools командами:</p> <pre><code>$ ./configure
$ make
$ make install</code></pre> <p>Далее проверяем правильность установки:</p> <pre><code>$ londiste3 -V
londiste3, Skytools version 3.2
$ pgqd -V
bad switch: usage: pgq-ticker [switches] config.file
Switches:
  -v        Increase verbosity
  -q        No output to console
  -d        Daemonize
  -h        Show help
  -V        Show version
 --ini      Show sample config file
  -s        Stop - send SIGINT to running process
  -k        Kill - send SIGTERM to running process
  -r        Reload - send SIGHUP to running process</code></pre> <h3 id="настройка-2">Настройка</h3> <p>Обозначения:</p> <ul> <li><p><code>master-host</code> — мастер база данных;</p></li> <li><p><code>slave1-host</code>, <code>slave2-host</code>, <code>slave3-host</code>, <code>slave4-host</code> — слейв базы данных;</p></li> <li><p><code>l3simple</code> — название реплицируемой базы данных;</p></li> </ul> <h4 id="конфигурация-репликаторов">Конфигурация репликаторов</h4> <p>Сначала создается конфигурационный файл для master базы (конфиг будет <code>/etc/skytools/master-londiste.ini</code>):</p> <pre><code>job_name = master_l3simple
db = dbname=l3simple
queue_name = replika
logfile = /var/log/skytools/master_l3simple.log
pidfile = /var/pid/skytools/master_l3simple.pid

# Задержка между проверками наличия активности
# (новых пакетов данных) в секундах
loop_delay = 0.5</code></pre> <p>Инициализируем Londiste для master базы:</p> <pre><code>$ londiste3 /etc/skytools/master-londiste.ini create-root master-node &quot;dbname=l3simple host=master-host&quot;
INFO plpgsql is installed
INFO Installing pgq
INFO   Reading from /usr/share/skytools3/pgq.sql
INFO pgq.get_batch_cursor is installed
INFO Installing pgq_ext
INFO   Reading from /usr/share/skytools3/pgq_ext.sql
INFO Installing pgq_node
INFO   Reading from /usr/share/skytools3/pgq_node.sql
INFO Installing londiste
INFO   Reading from /usr/share/skytools3/londiste.sql
INFO londiste.global_add_table is installed
INFO Initializing node
INFO Location registered
INFO Node &quot;master-node&quot; initialized for queue &quot;replika&quot; with type &quot;root&quot;
INFO Done</code></pre> <p>где <code>master-server</code> — это имя провайдера (мастера базы).</p> <p>Теперь запустим демон:</p> <pre><code>$ londiste3 -d /etc/skytools/master-londiste.ini worker
$ tail -f /var/log/skytools/master_l3simple.log
INFO {standby: 1}
INFO {standby: 1}</code></pre> <p>Если нужно перегрузить демон (например при изменении конфигурации), то можно воспользоватся параметром <code>-r</code>:</p> <pre><code>$ londiste3 /etc/skytools/master-londiste.ini -r</code></pre> <p>Для остановки демона есть параметр <code>-s</code>:</p> <pre><code>$ londiste3 /etc/skytools/master-londiste.ini -s</code></pre> <p>или если потребуется «убить» (<code>kill</code>) демон:</p> <pre><code>$ londiste3 /etc/skytools/master-londiste.ini -k</code></pre> <p>Для автоматизации данного процесса skytools3 имеет встроенный демон, который запускает все воркеры из директории <code>/etc/skytools/</code>. Сама конфигурация демона находится в <code>/etc/skytools.ini</code>. Что бы запустить все демоны londiste достаточно выполнить:</p> <pre><code>$ /etc/init.d/skytools3 start
INFO Starting master_l3simple</code></pre> <p>Перейдем к slave базе. Для начала нужно создать базу данных:</p> <pre><code>$ psql -h slave1-host -U postgres
# CREATE DATABASE l3simple;</code></pre> <p>Подключение должно быть «trust» (без паролей) между master и slave базами данных.</p> <p>Далее создадим конфиг для slave базы (<code>/etc/skytools/slave1-londiste.ini</code>):</p> <pre><code>job_name = slave1_l3simple
db = dbname=l3simple
queue_name = replika
logfile = /var/log/skytools/slave1_l3simple.log
pidfile = /var/pid/skytools/slave1_l3simple.pid

# Задержка между проверками наличия активности
# (новых пакетов данных) в секундах
loop_delay = 0.5</code></pre> <p>Инициализируем Londiste для slave базы:</p> <pre><code>$ londiste3 /etc/skytools/slave1-londiste.ini create-leaf slave1-node &quot;dbname=l3simple host=slave1-host&quot; --provider=&quot;dbname=l3simple host=master-host&quot;</code></pre> <p>Теперь можем запустить демон:</p> <pre><code>$ londiste3 -d /etc/skytools/slave1-londiste.ini worker</code></pre> <p>Или же через главный демон:</p> <pre><code>$ /etc/init.d/skytools3 start
INFO Starting master_l3simple
INFO Starting slave1_l3simple</code></pre> <h4 id="создаём-конфигурацию-для-pgq-ticker">Создаём конфигурацию для PgQ ticker</h4> <p>Londiste требуется PgQ ticker для работы с мастер базой данных, который может быть запущен и на другой машине. Но, конечно, лучше его запускать на той же, где и master база данных. Для этого мы настраиваем специальный конфиг для ticker демона (конфиг будет <code>/etc/skytools/pgqd.ini</code>):</p> <pre><code>logfile = /var/log/skytools/pgqd.log
pidfile = /var/pid/skytools/pgqd.pid</code></pre> <p>Запускаем демон:</p> <pre><code>$ pgqd -d /etc/skytools/pgqd.ini
$ tail -f /var/log/skytools/pgqd.log
LOG Starting pgqd 3.2
LOG auto-detecting dbs ...
LOG l3simple: pgq version ok: 3.2</code></pre> <p>Или же через глобальный демон:</p> <pre><code>$ /etc/init.d/skytools3 restart
INFO Starting master_l3simple
INFO Starting slave1_l3simple
INFO Starting pgqd
LOG Starting pgqd 3.2</code></pre> <p>Теперь можно увидеть статус кластера:</p> <pre><code>$ londiste3 /etc/skytools/master-londiste.ini status
Queue: replika   Local node: slave1-node

master-node (root)
  |                           Tables: 0/0/0
  |                           Lag: 44s, Tick: 5
  +--: slave1-node (leaf)
                              Tables: 0/0/0
                              Lag: 44s, Tick: 5

$ londiste3 /etc/skytools/master-londiste.ini members
Member info on master-node@replika:
node_name        dead             node_location
---------------  ---------------  --------------------------------
master-node      False            dbname=l3simple host=master-host
slave1-node      False            dbname=l3simple host=slave1-host</code></pre> <p>Но репликация еще не запущенна: требуется добавить таблицы в очередь, которые мы хотим реплицировать. Для этого используем команду <code>add-table</code>:</p> <pre><code>$ londiste3 /etc/skytools/master-londiste.ini add-table --all
$ londiste3 /etc/skytools/slave1-londiste.ini add-table --all --create-full</code></pre> <p>В данном примере используется параметр <code>--all</code>, который означает все таблицы, но вместо него вы можете перечислить список конкретных таблиц, если не хотите реплицировать все. Если имена таблиц отличаются на master и slave, то можно использовать <code>--dest-table</code> параметр при добавлении таблиц на slave базе. Также, если вы не перенесли струкруру таблиц заранее с master на slave базы, то это можно сделать автоматически через <code>--create</code> параметр (или <code>--create-full</code>, если нужно перенести полностью всю схему таблицы).</p> <p>Подобным образом добавляем последовательности (<code>sequences</code>) для репликации:</p> <pre><code>$ londiste3 /etc/skytools/master-londiste.ini add-seq --all
$ londiste3 /etc/skytools/slave1-londiste.ini add-seq --all</code></pre> <p>Но последовательности должны на slave базе созданы заранее (тут не поможет <code>--create-full</code> для таблиц). Поэтому иногда проще перенести точную копию структуры master базы на slave:</p> <pre><code>$ pg_dump -s -npublic l3simple | psql -hslave1-host l3simple</code></pre> <p>Далее проверяем состояние репликации:</p> <pre><code>$ londiste3 /etc/skytools/master-londiste.ini status
Queue: replika   Local node: master-node

master-node (root)
  |                           Tables: 4/0/0
  |                           Lag: 18s, Tick: 12
  +--: slave1-node (leaf)
                              Tables: 0/4/0
                              Lag: 18s, Tick: 12</code></pre> <p>Как можно заметить, возле «Table» содержится три цифры (x/y/z). Каждая обозначает:</p> <ul> <li><p>x - количество таблиц в состоянии «ok» (replicated). На master базе указывает, что она в норме, а на slave базах - таблица синхронизирована с master базой;</p></li> <li><p>y - количество таблиц в состоянии half (initial copy, not finnished), у master должно быть 0, а у slave базах это указывает количество таблиц в процессе копирования;</p></li> <li><p>z - количество таблиц в состоянии ignored (table not replicated locally), у master должно быть 0, а у slave базах это количество таблиц, которые не добавлены для репликации с мастера (т.е. master отдает на репликацию эту таблицу, но slave их просто не забирает).</p></li> </ul> <p>Через небольшой интервал времени все таблицы должны синхронизироватся:</p> <pre><code>$ londiste3 /etc/skytools/master-londiste.ini status
Queue: replika   Local node: master-node

master-node (root)
  |                           Tables: 4/0/0
  |                           Lag: 31s, Tick: 20
  +--: slave1-node (leaf)
                              Tables: 4/0/0
                              Lag: 31s, Tick: 20</code></pre> <p>Дополнительно Londiste позволяет просмотреть состояние таблиц и последовательностей на master и slave базах:</p> <pre><code>$ londiste3 /etc/skytools/master-londiste.ini tables
Tables on node
table_name               merge_state      table_attrs
-----------------------  ---------------  ---------------
public.pgbench_accounts  ok
public.pgbench_branches  ok
public.pgbench_history   ok
public.pgbench_tellers   ok

$ londiste3 /etc/skytools/master-londiste.ini seqs
Sequences on node
seq_name                        local            last_value
------------------------------  ---------------  ---------------
public.pgbench_history_hid_seq  True             33345</code></pre> <h4 id="проверка">Проверка</h4> <p>Для проверки будем использовать <code>pgbench</code> утилиту. Запустим добавление данных в таблицу и смотрим в логи одновлеменно:</p> <pre><code>$ pgbench -T 10 -c 5 l3simple
$ tail -f /var/log/skytools/slave1_l3simple.log
INFO {count: 1508, duration: 0.307, idle: 0.0026}
INFO {count: 1572, duration: 0.3085, idle: 0.002}
INFO {count: 1600, duration: 0.3086, idle: 0.0026}
INFO {count: 36, duration: 0.0157, idle: 2.0191}</code></pre> <p>Как видно по логам slave база успешно реплицируется с master базой.</p> <h4 id="каскадная-репликация">Каскадная репликация</h4> <p>Каскадная репликация позволяет реплицировать данные с одного слейва на другой. Создадим конфиг для второго slave (конфиг будет <code>/etc/skytools/slave2-londiste.ini</code>):</p> <pre><code>job_name = slave2_l3simple
db = dbname=l3simple host=slave2-host
queue_name = replika
logfile = /var/log/skytools/slave2_l3simple.log
pidfile = /var/pid/skytools/slave2_l3simple.pid

# Задержка между проверками наличия активности
# (новых пакетов данных) в секундах
loop_delay = 0.5</code></pre> <p>Для создания slave, от которого можно реплицировать другие базы данных используется команда <code>create-branch</code> вместо <code>create-leaf</code> (root, корень - master нода, предоставляет информацию для репликации; branch, ветка - нода с копией данных, с которой можно реплицировать; leaf, лист - нода с копией данными, но реплицировать с нее уже не возможно):</p> <pre><code>$ psql -hslave2-host -d postgres -c &quot;CREATE DATABASE l3simple;&quot;
$ pg_dump -s -npublic l3simple | psql -hslave2-host l3simple
$ londiste3 /etc/skytools/slave2-londiste.ini create-branch slave2-node &quot;dbname=l3simple host=slave2-host&quot; --provider=&quot;dbname=l3simple host=master-host&quot;
INFO plpgsql is installed
INFO Installing pgq
INFO   Reading from /usr/share/skytools3/pgq.sql
INFO pgq.get_batch_cursor is installed
INFO Installing pgq_ext
INFO   Reading from /usr/share/skytools3/pgq_ext.sql
INFO Installing pgq_node
INFO   Reading from /usr/share/skytools3/pgq_node.sql
INFO Installing londiste
INFO   Reading from /usr/share/skytools3/londiste.sql
INFO londiste.global_add_table is installed
INFO Initializing node
INFO Location registered
INFO Location registered
INFO Subscriber registered: slave2-node
INFO Location registered
INFO Location registered
INFO Location registered
INFO Node &quot;slave2-node&quot; initialized for queue &quot;replika&quot; with type &quot;branch&quot;
INFO Done</code></pre> <p>Далее добавляем все таблицы и последовательности:</p> <pre><code>$ londiste3 /etc/skytools/slave2-londiste.ini add-table --all
$ londiste3 /etc/skytools/slave2-londiste.ini add-seq --all</code></pre> <p>И запускаем новый демон:</p> <pre><code>$ /etc/init.d/skytools3 start
INFO Starting master_l3simple
INFO Starting slave1_l3simple
INFO Starting slave2_l3simple
INFO Starting pgqd
LOG Starting pgqd 3.2</code></pre> <p>Повторим вышеперечисленные операции для slave3 и slave4, только поменяем provider для них:</p> <pre><code>$ londiste3 /etc/skytools/slave3-londiste.ini create-branch slave3-node &quot;dbname=l3simple host=slave3-host&quot; --provider=&quot;dbname=l3simple host=slave2-host&quot;
$ londiste3 /etc/skytools/slave4-londiste.ini create-branch slave4-node &quot;dbname=l3simple host=slave4-host&quot; --provider=&quot;dbname=l3simple host=slave3-host&quot;</code></pre> <p>В результате получаем такую картину с кластером:</p> <pre><code>$ londiste3 /etc/skytools/slave4-londiste.ini status
Queue: replika   Local node: slave4-node

master-node (root)
  |                                Tables: 4/0/0
  |                                Lag: 9s, Tick: 49
  +--: slave1-node (leaf)
  |                                Tables: 4/0/0
  |                                Lag: 9s, Tick: 49
  +--: slave2-node (branch)
     |                             Tables: 4/0/0
     |                             Lag: 9s, Tick: 49
     +--: slave3-node (branch)
        |                          Tables: 4/0/0
        |                          Lag: 9s, Tick: 49
        +--: slave4-node (branch)
                                   Tables: 4/0/0
                                   Lag: 9s, Tick: 49</code></pre> <p>Londiste позволяет «на лету» изменять топологию кластера. Например, изменим «provider» для slave4:</p> <pre><code>$ londiste3 /etc/skytools/slave4-londiste.ini change-provider --provider=&quot;dbname=l3simple host=slave2-host&quot;
$ londiste3 /etc/skytools/slave4-londiste.ini status
Queue: replika   Local node: slave4-node

master-node (root)
  |                                Tables: 4/0/0
  |                                Lag: 12s, Tick: 56
  +--: slave1-node (leaf)
  |                                Tables: 4/0/0
  |                                Lag: 12s, Tick: 56
  +--: slave2-node (branch)
     |                             Tables: 4/0/0
     |                             Lag: 12s, Tick: 56
     +--: slave3-node (branch)
     |                             Tables: 4/0/0
     |                             Lag: 12s, Tick: 56
     +--: slave4-node (branch)
                                   Tables: 4/0/0
                                   Lag: 12s, Tick: 56</code></pre> <p>Также топологию можно менять с сторони репликатора через команду <code>takeover</code>:</p> <pre><code>$ londiste3 /etc/skytools/slave3-londiste.ini takeover slave4-node
$ londiste3 /etc/skytools/slave4-londiste.ini status
Queue: replika   Local node: slave4-node

master-node (root)
  |                                Tables: 4/0/0
  |                                Lag: 9s, Tick: 49
  +--: slave1-node (leaf)
  |                                Tables: 4/0/0
  |                                Lag: 9s, Tick: 49
  +--: slave2-node (branch)
     |                             Tables: 4/0/0
     |                             Lag: 9s, Tick: 49
     +--: slave3-node (branch)
        |                          Tables: 4/0/0
        |                          Lag: 9s, Tick: 49
        +--: slave4-node (branch)
                                   Tables: 4/0/0
                                   Lag: 9s, Tick: 49</code></pre> <p>Через команду <code>drop-node</code> можно удалить slave из кластера:</p> <pre><code>$ londiste3 /etc/skytools/slave4-londiste.ini drop-node slave4-node
$ londiste3 /etc/skytools/slave3-londiste.ini status
Queue: replika   Local node: slave3-node

master-node (root)
  |                                Tables: 4/0/0
  |                                Lag: 9s, Tick: 49
  +--: slave1-node (leaf)
  |                                Tables: 4/0/0
  |                                Lag: 9s, Tick: 49
  +--: slave2-node (branch)
     |                             Tables: 4/0/0
     |                             Lag: 9s, Tick: 49
     +--: slave3-node (branch)
                                   Tables: 4/0/0
                                   Lag: 9s, Tick: 49</code></pre> <p>Команда <code>tag-dead</code> может использоваться, что бы указать slave как не живой (прекратить на него репликацию), а через команду <code>tag-alive</code> его можно вернуть в кластер.</p> <h3 id="общие-задачи-2">Общие задачи</h3> <h4 id="проверка-состояния-слейвов">Проверка состояния слейвов</h4> <p>Данный запрос на мастере дает некоторую информацию о каждой очереди и слейве:</p> <pre><code># SELECT queue_name, consumer_name, lag, last_seen FROM pgq.get_consumer_info();
 queue_name |     consumer_name      |       lag       |    last_seen
------------+------------------------+-----------------+-----------------
 replika    | .global_watermark      | 00:03:37.108259 | 00:02:33.013915
 replika    | slave1_l3simple        | 00:00:32.631509 | 00:00:32.533911
 replika    | .slave1-node.watermark | 00:03:37.108259 | 00:03:05.01431</code></pre> <p>где <code>lag</code> столбец показывает отставание от мастера в синхронизации, <code>last_seen</code> — время последней запроса от слейва. Значение этого столбца не должно быть больше, чем 60 секунд для конфигурации по умолчанию.</p> <h4 id="удаление-очереди-всех-событий-из-мастера">Удаление очереди всех событий из мастера</h4> <p>При работе с Londiste может потребоваться удалить все ваши настройки для того, чтобы начать все заново. Для PGQ, чтобы остановить накопление данных, используйте следующие API:</p> <pre><code>SELECT pgq.unregister_consumer(&#39;queue_name&#39;, &#39;consumer_name&#39;);</code></pre> <h4 id="добавление-столбца-в-таблицу">Добавление столбца в таблицу</h4> <p>Добавляем в следующей последовательности:</p> <ol> <li><p>добавить поле на все слейвы;</p></li> <li><p>BEGIN; – на мастере;</p></li> <li><p>добавить поле на мастере;</p></li> <li><p>COMMIT;</p></li> </ol> <h4 id="удаление-столбца-из-таблицы">Удаление столбца из таблицы</h4> <ol> <li><p>BEGIN; – на мастере;</p></li> <li><p>удалить поле на мастере;</p></li> <li><p>COMMIT;</p></li> <li><p>Проверить <code>lag</code>, когда londiste пройдет момент удаления поля;</p></li> <li><p>удалить поле на всех слейвах;</p></li> </ol> <p>Хитрость тут в том, чтобы удалить поле на слейвах только тогда, когда больше нет событий в очереди на это поле.</p> <h3 id="устранение-неисправностей-1">Устранение неисправностей</h3> <h4 id="londiste-пожирает-процессор-и-lag-растет">Londiste пожирает процессор и lag растет</h4> <p>Это происходит, например, если во время сбоя забыли перезапустить ticker. Или когда сделали большой <code>UPDATE</code> или <code>DELETE</code> в одной транзакции, но теперь что бы реализовать каждое событие в этом запросе создаются транзакции на слейвах …</p> <p>Следующий запрос позволяет подсчитать, сколько событий пришло в <code>pgq.subscription</code> в колонках <code>sub_last_tick</code> и <code>sub_next_tick</code>.</p> <pre><code>SELECT count(*)
  FROM pgq.event_1,
    (SELECT tick_snapshot
      FROM pgq.tick
      WHERE tick_id BETWEEN 5715138 AND 5715139
    ) as t(snapshots)
WHERE txid_visible_in_snapshot(ev_txid, snapshots);</code></pre> <p>На практике это было более чем 5 миллионов и 400 тысяч событий. Чем больше событий с базы данных требуется обработать Londiste, тем больше ему требуется памяти для этого. Возможно сообщить Londiste не загружать все события сразу. Достаточно добавить в INI конфиг PgQ ticker следующую настройку:</p> <pre><code>pgq_lazy_fetch = 500</code></pre> <p>Теперь Londiste будет брать максимум 500 событий в один пакет запросов. Остальные попадут в следующие пакеты запросов.</p> <h2 id=bucardo>Bucardo</h2> <p><a href="https://bucardo.org/wiki/Bucardo">Bucardo</a> — асинхронная master-master или master-slave репликация PostgreSQL, которая написана на Perl. Система очень гибкая, поддерживает несколько видов синхронизации и обработки конфликтов.</p> <h3 id="установка-3">Установка</h3> <p>Установка будет проводиться на Debian сервере. Сначала нужно установить <code>DBIx::Safe Perl</code> модуль.</p> <pre><code>$ apt-get install libdbix-safe-perl</code></pre> <p>Для других систем можно поставить из <a href="http://search.cpan.org/CPAN/authors/id/T/TU/TURNSTEP/">исходников</a>:</p> <pre><code>$ tar xvfz dbix_safe.tar.gz
$ cd DBIx-Safe-1.2.5
$ perl Makefile.PL
$ make
$ make test
$ sudo make install</code></pre> <p>Теперь ставим сам Bucardo. <a href="http://bucardo.org/wiki/Bucardo#Obtaining_Bucardo">Скачиваем</a> его и инсталлируем:</p> <pre><code>$ wget http://bucardo.org/downloads/Bucardo-5.4.1.tar.gz
$ tar xvfz Bucardo-5.4.1.tar.gz
$ cd Bucardo-5.4.1
$ perl Makefile.PL
$ make
$ sudo make install</code></pre> <p>Для работы Bucardo потребуется установить поддержку pl/perl языка в PostgreSQL.</p> <pre><code>$ sudo aptitude install postgresql-plperl-9.5</code></pre> <p>и дополнительные пакеты для Perl (DBI, DBD::Pg, Test::Simple, boolean):</p> <pre><code>$ sudo aptitude install libdbd-pg-perl libboolean-perl</code></pre> <p>Теперь можем приступать к настройке репликации.</p> <h3 id="настройка-3">Настройка</h3> <h4 id="инициализация-bucardo">Инициализация Bucardo</h4> <p>Запускаем установку Bucardo:</p> <pre><code>$ bucardo install</code></pre> <p>Во время установки будут показаны настройки подключения к PostgreSQL, которые можно будет изменить:</p> <pre><code>This will install the bucardo database into an existing Postgres cluster.
Postgres must have been compiled with Perl support,
and you must connect as a superuser

We will create a new superuser named &#39;bucardo&#39;,
and make it the owner of a new database named &#39;bucardo&#39;

Current connection settings:
1. Host:          &lt;none&gt;
2. Port:          5432
3. User:          postgres
4. Database:      postgres
5. PID directory: /var/run/bucardo</code></pre> <p>После подтверждения настроек, Bucardo создаст пользователя <code>bucardo</code> и базу данных <code>bucardo</code>. Данный пользователь должен иметь право логиниться через Unix socket, поэтому лучше заранее дать ему такие права в <code>pg_hda.conf</code>.</p> <p>После успешной установки можно проверить конфигурацию через команду <code>bucardo show all</code>:</p> <pre><code>$ bucardo show all
autosync_ddl              = newcol
bucardo_initial_version   = 5.0.0
bucardo_vac               = 1
bucardo_version           = 5.0.0
ctl_checkonkids_time      = 10
ctl_createkid_time        = 0.5
ctl_sleep                 = 0.2
default_conflict_strategy = bucardo_latest
default_email_from        = nobody@example.com
default_email_host        = localhost
default_email_to          = nobody@example.com
...</code></pre> <h4 id="настройка-баз-данных">Настройка баз данных</h4> <p>Теперь нужно настроить базы данных, с которыми будет работать Bucardo. Обозначим базы как <code>master_db</code> и <code>slave_db</code>. Реплицировать будем <code>simple_database</code> базу. Сначала настроим мастер базу:</p> <pre><code>$ bucardo add db master_db dbname=simple_database host=master_host
Added database &quot;master_db&quot;</code></pre> <p>Данной командой указали базу данных и дали ей имя <code>master_db</code> (для того, что в реальной жизни <code>master_db</code> и <code>slave_db</code> имеют одинаковое название базы <code>simple_database</code> и их нужно отличать в Bucardo).</p> <p>Дальше добавляем <code>slave_db</code>:</p> <pre><code>$ bucardo add db slave_db dbname=simple_database port=5432 host=slave_host</code></pre> <h4 id="настройка-репликации">Настройка репликации</h4> <p>Теперь требуется настроить синхронизацию между этими базами данных. Делается это командой <code>sync</code>:</p> <pre><code>$ bucardo add sync delta dbs=master_db:source,slave_db:target conflict_strategy=bucardo_latest tables=all
Added sync &quot;delta&quot;
Created a new relgroup named &quot;delta&quot;
Created a new dbgroup named &quot;delta&quot;
  Added table &quot;public.pgbench_accounts&quot;
  Added table &quot;public.pgbench_branches&quot;
  Added table &quot;public.pgbench_history&quot;
  Added table &quot;public.pgbench_tellers&quot;</code></pre> <p>Данная команда устанавливает Bucardo триггеры в PostgreSQL для master-slave репликации. Значения параметров:</p> <ul> <li><p><code>dbs</code> — список баз данных, которые следует синхронизировать. Значение <code>source</code> или <code>target</code> указывает, что это master или slave база данных соответственно (их может быть больше одной);</p></li> <li><p><code>conflict_strategy</code> — для работы в режиме master-master нужно указать как Bucardo должен решать конфликты синхронизации. Существуют следующие стратегии:</p> <ul> <li><p><code>bucardo_source</code> — при конфликте мы копируем данные с source;</p></li> <li><p><code>bucardo_target</code> — при конфликте мы копируем данные с target;</p></li> <li><p><code>bucardo_skip</code> — конфликт мы просто не реплицируем. Не рекомендуется для продакшен систем;</p></li> <li><p><code>bucardo_random</code> — каждая БД имеет одинаковый шанс, что её изменение будет взято для решение конфликта;</p></li> <li><p><code>bucardo_latest</code> — запись, которая была последней изменена решает конфликт;</p></li> <li><p><code>bucardo_abort</code> — синхронизация прерывается;</p></li> </ul></li> <li><p><code>tables</code> — таблицы, которые требуется синхронизировать. Через «all» указываем все;</p></li> </ul> <p>Для master-master репликации требуется выполнить:</p> <pre><code>$ bucardo add sync delta dbs=master_db:source,slave_db:source conflict_strategy=bucardo_latest tables=all</code></pre> <p>Пример для создания master-master и master-slave репликации:</p> <pre><code>$ bucardo add sync delta dbs=master_db1:source,master_db2:source,slave_db1:target,slave_db2:target conflict_strategy=bucardo_latest tables=all</code></pre> <p>Для проверки состояния репликации:</p> <pre><code>$ bucardo status
PID of Bucardo MCP: 12122
 Name    State    Last good    Time     Last I/D    Last bad    Time
=======+========+============+========+===========+===========+=======
 delta | Good   | 13:28:53   | 13m 6s | 3685/7384 | none      |</code></pre> <h4 id="запускостановка-репликации">Запуск/Остановка репликации</h4> <p>Запуск репликации:</p> <pre><code>$ bucardo start</code></pre> <p>Остановка репликации:</p> <pre><code>$ bucardo stop</code></pre> <h3 id="общие-задачи-3">Общие задачи</h3> <h4 id="просмотр-значений-конфигурации">Просмотр значений конфигурации</h4> <pre><code>$ bucardo show all</code></pre> <h4 id="изменения-значений-конфигурации">Изменения значений конфигурации</h4> <pre><code>$ bucardo set name=value</code></pre> <p>Например:</p> <pre><code>$ bucardo_ctl set syslog_facility=LOG_LOCAL3</code></pre> <h4 id="перегрузка-конфигурации">Перегрузка конфигурации</h4> <pre><code>$ bucardo reload_config</code></pre> <p>Более подробную информацию можно найти на <a href="http://bucardo.org/">официальном сайте</a>.</p> <h3 id="репликация-в-другие-типы-баз-данных">Репликация в другие типы баз данных</h3> <p>Начиная с версии 5.0 Bucardo поддерживает репликацию в другие источники данных: drizzle, mongo, mysql, oracle, redis и sqlite (тип базы задается при использовании команды <code>bucardo add db</code> через ключ «type», который по умолчанию postgres). Давайте рассмотрим пример с redis базой. Для начала потребуется установить redis адаптер для Perl (для других баз устанавливаются соответствующие):</p> <pre><code>$ aptitude install libredis-perl</code></pre> <p>Далее зарегистрируем redis базу в Bucardo:</p> <pre><code>$ bucardo add db R dbname=simple_database type=redis
Added database &quot;R&quot;</code></pre> <p>Создадим группу баз данных под названием <code>pg_to_redis</code>:</p> <pre><code>$ bucardo add dbgroup pg_to_redis master_db:source slave_db:source R:target
Created dbgroup &quot;pg_to_redis&quot;
Added database &quot;master_db&quot; to dbgroup &quot;pg_to_redis&quot; as source
Added database &quot;slave_db&quot; to dbgroup &quot;pg_to_redis&quot; as source
Added database &quot;R&quot; to dbgroup &quot;pg_to_redis&quot; as target</code></pre> <p>И создадим репликацию:</p> <pre><code>$ bucardo add sync pg_to_redis_sync tables=all dbs=pg_to_redis status=active
Added sync &quot;pg_to_redis_sync&quot;
  Added table &quot;public.pgbench_accounts&quot;
  Added table &quot;public.pgbench_branches&quot;
  Added table &quot;public.pgbench_history&quot;
  Added table &quot;public.pgbench_tellers&quot;</code></pre> <p>После перезапуска Bucardo данные с PostgreSQL таблиц начнуть реплицироватся в Redis:</p> <pre><code>$ pgbench -T 10 -c 5 simple_database
$ redis-cli monitor
&quot;HMSET&quot; &quot;pgbench_history:6&quot; &quot;bid&quot; &quot;2&quot; &quot;aid&quot; &quot;36291&quot; &quot;delta&quot; &quot;3716&quot; &quot;mtime&quot; &quot;2014-07-11 14:59:38.454824&quot; &quot;hid&quot; &quot;4331&quot;
&quot;HMSET&quot; &quot;pgbench_history:2&quot; &quot;bid&quot; &quot;1&quot; &quot;aid&quot; &quot;65179&quot; &quot;delta&quot; &quot;2436&quot; &quot;mtime&quot; &quot;2014-07-11 14:59:38.500896&quot; &quot;hid&quot; &quot;4332&quot;
&quot;HMSET&quot; &quot;pgbench_history:14&quot; &quot;bid&quot; &quot;2&quot; &quot;aid&quot; &quot;153001&quot; &quot;delta&quot; &quot;-264&quot; &quot;mtime&quot; &quot;2014-07-11 14:59:38.472706&quot; &quot;hid&quot; &quot;4333&quot;
&quot;HMSET&quot; &quot;pgbench_history:15&quot; &quot;bid&quot; &quot;1&quot; &quot;aid&quot; &quot;195747&quot; &quot;delta&quot; &quot;-1671&quot; &quot;mtime&quot; &quot;2014-07-11 14:59:38.509839&quot; &quot;hid&quot; &quot;4334&quot;
&quot;HMSET&quot; &quot;pgbench_history:3&quot; &quot;bid&quot; &quot;2&quot; &quot;aid&quot; &quot;147650&quot; &quot;delta&quot; &quot;3237&quot; &quot;mtime&quot; &quot;2014-07-11 14:59:38.489878&quot; &quot;hid&quot; &quot;4335&quot;
&quot;HMSET&quot; &quot;pgbench_history:15&quot; &quot;bid&quot; &quot;1&quot; &quot;aid&quot; &quot;39521&quot; &quot;delta&quot; &quot;-2125&quot; &quot;mtime&quot; &quot;2014-07-11 14:59:38.526317&quot; &quot;hid&quot; &quot;4336&quot;
&quot;HMSET&quot; &quot;pgbench_history:14&quot; &quot;bid&quot; &quot;2&quot; &quot;aid&quot; &quot;60105&quot; &quot;delta&quot; &quot;2555&quot; &quot;mtime&quot; &quot;2014-07-11 14:59:38.616935&quot; &quot;hid&quot; &quot;4337&quot;
&quot;HMSET&quot; &quot;pgbench_history:15&quot; &quot;bid&quot; &quot;2&quot; &quot;aid&quot; &quot;186655&quot; &quot;delta&quot; &quot;930&quot; &quot;mtime&quot; &quot;2014-07-11 14:59:38.541296&quot; &quot;hid&quot; &quot;4338&quot;
&quot;HMSET&quot; &quot;pgbench_history:15&quot; &quot;bid&quot; &quot;1&quot; &quot;aid&quot; &quot;101406&quot; &quot;delta&quot; &quot;668&quot; &quot;mtime&quot; &quot;2014-07-11 14:59:38.560971&quot; &quot;hid&quot; &quot;4339&quot;
&quot;HMSET&quot; &quot;pgbench_history:15&quot; &quot;bid&quot; &quot;2&quot; &quot;aid&quot; &quot;126329&quot; &quot;delta&quot; &quot;-4236&quot; &quot;mtime&quot; &quot;2014-07-11 14:59:38.5907&quot; &quot;hid&quot; &quot;4340&quot;
&quot;DEL&quot; &quot;pgbench_tellers:20&quot;</code></pre> <p>Данные в Redis хранятся в виде хешей:</p> <pre><code>$ redis-cli &quot;HGETALL&quot; &quot;pgbench_history:15&quot;
 1) &quot;bid&quot;
 2) &quot;2&quot;
 3) &quot;aid&quot;
 4) &quot;126329&quot;
 5) &quot;delta&quot;
 6) &quot;-4236&quot;
 7) &quot;mtime&quot;
 8) &quot;2014-07-11 14:59:38.5907&quot;
 9) &quot;hid&quot;
10) &quot;4340&quot;</code></pre> <p>Также можно проверить состояние репликации:</p> <pre><code>$ bucardo status
PID of Bucardo MCP: 4655
 Name               State    Last good    Time     Last I/D    Last bad    Time
==================+========+============+========+===========+===========+========
 delta            | Good   | 14:59:39   | 8m 15s | 0/0       | none      |
 pg_to_redis_sync | Good   | 14:59:40   | 8m 14s | 646/2546  | 14:59:39  | 8m 15s</code></pre> <p>Теперь данные из redis могут использоваться для приложения в виде быстрого кэш хранилища.</p> <h2 id="заключение-3">Заключение</h2> <p>Репликация — одна из важнейших частей крупных приложений, которые работают на PostgreSQL. Она помогает распределять нагрузку на базу данных, делать фоновый бэкап одной из копий без нагрузки на центральный сервер, создавать отдельный сервер для логирования или аналитики, прочее.</p> <p>В главе было рассмотрено несколько видов репликации PostgreSQL. Нельзя четко сказать какая лучше всех. Потоковая репликация — один из самых лучших вариантов для поддержки идентичных кластеров баз данных. Slony-I — громоздкая и сложная в настройке система, но имеющая в своем арсенале множество функций, таких как отказоустойчивости (failover) и переключение между серверами (switchover). В тоже время Londiste имея в своем арсенале подобный функционал, может похвастатся еще компактностью и простой в установке. Bucardo — система которая может быть или master-master, или master-slave репликацией.</p> <h1 id="шардинг">Шардинг</h1> <h2 id="введение-4">Введение</h2> <p>Шардинг — разделение данных на уровне ресурсов. Концепция шардинга заключается в логическом разделении данных по различным ресурсам исходя из требований к нагрузке.</p> <p>Рассмотрим пример. Пусть у нас есть приложение с регистрацией пользователей, которое позволяет писать друг другу личные сообщения. Допустим оно очень популярно и много людей им пользуются ежедневно. Естественно, что таблица с личными сообщениями будет намного больше всех остальных таблиц в базе (скажем, будет занимать 90% всех ресурсов). Зная это, мы можем подготовить для этой (только одной!) таблицы выделенный сервер помощнее, а остальные оставить на другом (послабее). Теперь мы можем идеально подстроить сервер для работы с одной специфической таблицей, постараться уместить ее в память, возможно, дополнительно партиционировать ее и т.д. Такое распределение называется вертикальным шардингом.</p> <p>Что делать, если наша таблица с сообщениями стала настолько большой, что даже выделенный сервер под нее одну уже не спасает? Необходимо делать горизонтальный шардинг — т.е. разделение одной таблицы по разным ресурсам. Как это выглядит на практике? На разных серверах у нас будет таблица с одинаковой структурой, но разными данными. Для нашего случая с сообщениями, мы можем хранить первые 10 миллионов сообщений на одном сервере, вторые 10 - на втором и т.д. Т.е. необходимо иметь критерий шардинга — какой-то параметр, который позволит определять, на каком именно сервере лежат те или иные данные.</p> <p>Обычно, в качестве параметра шардинга выбирают ID пользователя (<code>user_id</code>) — это позволяет делить данные по серверам равномерно и просто. Т.о. при получении личных сообщений пользователей алгоритм работы будет такой:</p> <ul> <li><p>Определить, на каком сервере БД лежат сообщения пользователя исходя из <code>user_id</code>;</p></li> <li><p>Инициализировать соединение с этим сервером;</p></li> <li><p>Выбрать сообщения;</p></li> </ul> <p>Задачу определения конкретного сервера можно решать двумя путями:</p> <ul> <li><p>Хранить в одном месте хеш-таблицу с соответствиями «пользователь=сервер». Тогда, при определении сервера, нужно будет выбрать сервер из этой таблицы. В этом случае узкое место — это большая таблица соответствия, которую нужно хранить в одном месте. Для таких целей очень хорошо подходят базы данных «ключ=значение»;</p></li> <li><p>Определять имя сервера с помощью числового (буквенного) преобразования. Например, можно вычислять номер сервера, как остаток от деления на определенное число (количество серверов, между которыми Вы делите таблицу). В этом случае узкое место — это проблема добавления новых серверов — придется делать перераспределение данных между новым количеством серверов;</p></li> </ul> <p>Естественно, делая горизонтальный шардинг, Вы ограничиваете себя в возможности выборок, которые требуют пересмотра всей таблицы (например, последние посты в блогах людей будет достать невозможно, если таблица постов шардится). Такие задачи придется решать другими подходами. Например, для описанного примера, можно при появлении нового поста, заносить его ID в общий стек, размером в 100 элементом.</p> <p>Горизонтальный шардинг имеет одно явное преимущество — он бесконечно масштабируем. Для создания шардинга PostgreSQL существует несколько решений:</p> <ul> <li><p><a href="http://postgres-xc.sourceforge.net/">Postgres-XC</a></p></li> <li><p><a href="http://www.greenplum.com/products/greenplum-database">Greenplum Database</a></p></li> <li><p><a href="https://github.com/citusdata/citus">Citus</a></p></li> <li><p><a href="http://plproxy.projects.postgresql.org/doc/tutorial.html">PL/Proxy</a></p></li> <li><p><a href="https://launchpad.net/stado">Stado (sequel to GridSQL)</a></p></li> </ul> <h2 id="sec:plproxy">PL/Proxy</h2> <p><a href="https://plproxy.github.io/">PL/Proxy</a> представляет собой прокси-язык для удаленного вызова процедур и партицирования данных между разными базами. Основная идея его использования заключается в том, что появляется возможность вызывать функции, расположенные в удаленных базах, а также свободно работать с кластером баз данных (например, вызвать функцию на всех узлах кластера, или на случайном узле, или на каком-то одном определенном).</p> <p>Чем PL/Proxy может быть полезен? Он существенно упрощает горизонтальное масштабирование системы. Становится удобным разделять таблицу с пользователями, например, по первой латинской букве имени — на 26 узлов. При этом приложение, которое работает непосредственно с прокси-базой, ничего не будет замечать: запрос на авторизацию, например, сам будет направлен прокси-сервером на нужный узел. То есть администратор баз данных может проводить масштабирование системы практически независимо от разработчиков приложения.</p> <p>PL/Proxy позволяет полностью решить проблемы масштабирования OLTP систем. В систему легко вводится резервирование с failover-ом не только по узлам, но и по самим прокси-серверам, каждый из которых работает со всеми узлами.</p> <p>Недостатки и ограничения:</p> <ul> <li><p>все запросы и вызовы функций вызываются в autocommit-режиме на удаленных серверах;</p></li> <li><p>в теле функции разрешен только один <code>SELECT</code>. При необходимости нужно писать отдельную процедуру;</p></li> <li><p>при каждом вызове прокси-сервер стартует новое соединение к бакенд-серверу. В высоконагруженных системах целесообразно использовать менеджер для кеширования соединений к бакенд-серверам (для этой цели идеально подходит PgBouncer);</p></li> <li><p>изменение конфигурации кластера (например количества партиций) требует перезапуска прокси-сервера;</p></li> </ul> <h3 id="установка-4">Установка</h3> <ol> <li><p>Скачать <a href="https://github.com/plproxy/plproxy">PL/Proxy</a> и распаковать;</p></li> <li><p>Собрать PL/Proxy командами <code>make</code> и <code>make install</code>;</p></li> </ol> <p>Так же можно установить PL/Proxy из репозитория пакетов. Например в Ubuntu Server достаточно выполнить команду для PostgreSQL 9.6:</p> <pre><code>$ sudo aptitude install postgresql-9.6-plproxy</code></pre> <h3 id="настройка-4">Настройка</h3> <p>Для примера настройки используется 3 сервера PostgreSQL. 2 сервера пусть будут <code>node1</code> и <code>node2</code>, а главный, что будет проксировать запросы на два других — <code>proxy</code>. Для корректной работы pl/proxy рекомендуется использовать количество нод равное степеням двойки. База данных будет называться <code>plproxytest</code>, а таблица в ней — <code>users</code>.</p> <p>Для начала настроим <code>node1</code> и <code>node2</code>. Команды, написанные ниже, нужно выполнять на каждой ноде. Сначала создадим базу данных plproxytest (если её ещё нет):</p> <pre><code>CREATE DATABASE plproxytest
     WITH OWNER = postgres
       ENCODING = &#39;UTF8&#39;;</code></pre> <p>Добавляем табличку <code>users</code>:</p> <pre><code>CREATE TABLE public.users
  (
   username character varying(255),
   email character varying(255)
  )
  WITH (OIDS=FALSE);
ALTER TABLE public.users OWNER TO postgres;</code></pre> <p>Теперь создадим функцию для добавления данных в таблицу <code>users</code>:</p> <pre><code>CREATE OR REPLACE FUNCTION public.insert_user(i_username text,
i_emailaddress   text)
RETURNS integer AS
$BODY$
INSERT INTO public.users (username, email) VALUES ($1,$2);
    SELECT 1;
$BODY$
  LANGUAGE &#39;sql&#39; VOLATILE;
ALTER FUNCTION public.insert_user(text, text) OWNER TO postgres;</code></pre> <p>С настройкой нод закончено. Приступим к серверу proxy. Как и на всех нодах, на главном сервере (<code>proxy</code>) должна присутствовать база данных:</p> <pre><code>CREATE DATABASE plproxytest
     WITH OWNER = postgres
       ENCODING = &#39;UTF8&#39;;</code></pre> <p>Теперь надо указать серверу что эта база данных управляется с помощью pl/proxy:</p> <pre><code>CREATE OR REPLACE FUNCTION public.plproxy_call_handler()
  RETURNS language_handler AS
&#39;$libdir/plproxy&#39;, &#39;plproxy_call_handler&#39;
  LANGUAGE &#39;c&#39; VOLATILE
COST 1;
ALTER FUNCTION public.plproxy_call_handler()
OWNER TO postgres;
-- language
CREATE LANGUAGE plproxy HANDLER plproxy_call_handler;
CREATE LANGUAGE plpgsql;</code></pre> <p>Также, для того что бы сервер знал где и какие ноды у него есть, надо создать 3 сервисные функции, которые pl/proxy будет использовать в своей работе. Первая функция — конфиг для кластера баз данных. Тут указываются параметры через key-value:</p> <pre><code>CREATE OR REPLACE FUNCTION public.get_cluster_config
(IN cluster_name text,   OUT &quot;key&quot; text, OUT val text)
  RETURNS SETOF record AS
$BODY$
BEGIN
  -- lets use same config for all clusters
  key := &#39;connection_lifetime&#39;;
  val := 30*60; -- 30m
  RETURN NEXT;
  RETURN;
END;
$BODY$
  LANGUAGE &#39;plpgsql&#39; VOLATILE
  COST 100
  ROWS 1000;
ALTER FUNCTION public.get_cluster_config(text)
OWNER TO postgres;</code></pre> <p>Вторая важная функция, код которой надо будет подправить. В ней надо будет указать DSN нод:</p> <pre><code>CREATE OR REPLACE FUNCTION
public.get_cluster_partitions(cluster_name text)
  RETURNS SETOF text AS
$BODY$
BEGIN
  IF cluster_name = &#39;usercluster&#39; THEN
    RETURN NEXT &#39;dbname=plproxytest host=node1 user=postgres&#39;;
    RETURN NEXT &#39;dbname=plproxytest host=node2 user=postgres&#39;;
    RETURN;
  END IF;
  RAISE EXCEPTION &#39;Unknown cluster&#39;;
END;
$BODY$
  LANGUAGE &#39;plpgsql&#39; VOLATILE
  COST 100
  ROWS 1000;
ALTER FUNCTION public.get_cluster_partitions(text)
OWNER TO postgres;</code></pre> <p>И последняя:</p> <pre><code>CREATE OR REPLACE FUNCTION
public.get_cluster_version(cluster_name text)
  RETURNS integer AS
$BODY$
BEGIN
  IF cluster_name = &#39;usercluster&#39; THEN
    RETURN 1;
  END IF;
  RAISE EXCEPTION &#39;Unknown cluster&#39;;
END;
$BODY$
  LANGUAGE &#39;plpgsql&#39; VOLATILE
  COST 100;
ALTER FUNCTION public.get_cluster_version(text)
OWNER TO postgres;</code></pre> <p>Ну и собственно самая главная функция, которая будет вызываться уже непосредственно в приложении:</p> <pre><code>CREATE OR REPLACE FUNCTION
public.insert_user(i_username text, i_emailaddress text)
  RETURNS integer AS
$BODY$
  CLUSTER &#39;usercluster&#39;;
  RUN ON hashtext(i_username);
$BODY$
  LANGUAGE &#39;plproxy&#39; VOLATILE
  COST 100;
ALTER FUNCTION public.insert_user(text, text)
OWNER TO postgres;</code></pre> <p>Все готово. Подключаемся к серверу proxy и заносим данные в базу:</p> <pre><code>SELECT insert_user(&#39;Sven&#39;,&#39;sven@somewhere.com&#39;);
SELECT insert_user(&#39;Marko&#39;, &#39;marko@somewhere.com&#39;);
SELECT insert_user(&#39;Steve&#39;,&#39;steve@somewhere.com&#39;);</code></pre> <p>Пробуем извлечь данные. Для этого напишем новую серверную функцию:</p> <pre><code>CREATE OR REPLACE FUNCTION
public.get_user_email(i_username text)
 RETURNS SETOF text AS
$BODY$
 CLUSTER &#39;usercluster&#39;;
 RUN ON hashtext(i_username) ;
 SELECT email FROM public.users
 WHERE username = i_username;
$BODY$
 LANGUAGE &#39;plproxy&#39; VOLATILE
 COST 100
 ROWS 1000;
ALTER FUNCTION public.get_user_email(text)
OWNER TO postgres;</code></pre> <p>И попробуем её вызвать:</p> <pre><code>SELECT plproxy.get_user_email(&#39;Steve&#39;);</code></pre> <p>Если потом подключиться к каждой ноде отдельно, то можно четко увидеть, что данные <code>users</code> разбросаны по таблицам каждой ноды.</p> <h3 id="все-ли-так-просто">Все ли так просто?</h3> <p>Как видно на тестовом примере ничего сложного в работе с pl/proxy нет. Но в реальной жизни все не так просто. Представьте что у вас 16 нод. Это же надо как-то синхронизировать код функций. А что если ошибка закрадётся — как её оперативно исправлять?</p> <p>Этот вопрос был задан и на конференции Highload++ 2008, на что Аско Ойя ответил что соответствующие средства уже реализованы внутри самого Skype, но ещё не достаточно готовы для того что бы отдавать их на суд сообществу opensource.</p> <p>Вторая проблема, которая не дай бог коснётся вас при разработке такого рода системы, это проблема перераспределения данных в тот момент, когда нам захочется добавить ещё нод в кластер. Планировать эту масштабную операцию придётся очень тщательно, подготовив все сервера заранее, занеся данные и потом в один момент подменив код функции <code>get_cluster_partitions</code>.</p> <h2 id="sec:postgres-x2">Postgres-X2</h2> <p>Postgres-X2 – система для создания мульти-мастер кластеров, работающих в синхронном режиме – все узлы всегда содержат актуальные данные. Postgres-X2 поддерживает опции для увеличения масштабирования кластера как при преобладании операций записи, так и при основной нагрузке на чтение данных: поддерживается выполнение транзакций с распараллеливанием на несколько узлов, за целостностью транзакций в пределах всего кластера отвечает специальный узел GTM (Global Transaction Manager).</p> <p>Измерение производительности показало, что КПД кластера Postgres-X2 составляет примерно 64%, т.е. кластер из 10 серверов позволяет добиться увеличения производительности системы в целом в 6.4 раза, относительно производительности одного сервера (цифры приблизительные).</p> <p>Система не использует в своей работе триггеры и представляет собой набор дополнений и патчей к PostgreSQL, дающих возможность в прозрачном режиме обеспечить работу в кластере стандартных приложений, без их дополнительной модификации и адаптации (полная совместимость с PostgreSQL API). Кластер состоит из одного управляющего узла (GTM), предоставляющего информацию о состоянии транзакций, и произвольного набора рабочих узлов, каждый из которых в свою очередь состоит из координатора и обработчика данных (обычно эти элементы реализуются на одном сервере, но могут быть и разделены).</p> <p>Хоть Postgres-X2 и выглядит похожим на MultiMaster, но он им не является. Все сервера кластера должны быть соединены сетью с минимальными задержками, никакое географически-распределенное решение с разумной производительностью построить на нем невозможно (это важный момент).</p> <h3 id="sec:postgres-x2-architecture">Архитектура</h3> <p>Рис. [fig:postgres-x21] показывает архитектуру Postgres-X2 с тремя её основными компонентами:</p> <ol> <li><p>Глобальный менеджер транзакций (GTM) — собирает и обрабатывает информацию о транзакциях в Postgres-X2, решает вопросы глобального идентификатора транзакции по операциям (для поддержания согласованного представления базы данных на всех узлах). Он обеспечивает поддержку других глобальных данных, таких как последовательности и временные метки. Он хранит данные пользователя, за исключением управляющей информации;</p></li> <li><p>Координаторы (coordinators) — обеспечивают точку подключения для клиента (приложения). Они несут ответственность за разбор и выполнение запросов от клиентов и возвращение результатов (при необходимости). Они не хранят пользовательские данные, а собирают их из обработчиков данных (datanodes) с помощью запросов SQL через PostgreSQL интерфейс. Координаторы также обрабатывают данные, если требуется, и даже управляют двухфазной фиксацией. Координаторы используются также для разбора запросов, составления планов запросов, поиска данных и т.д;</p></li> <li><p>Обработчики данных (datanodes) — обеспечивают сохранение пользовательских данных. Datanodes выполняют запросы от координаторов и возвращают им полученный результат;</p></li> </ol> <h3 id="установка-5">Установка</h3> <p>Установить Postgres-X2 можно из <a href="http://postgres-x2.github.io/">исходников</a>.</p> <h3 id="распределение-данных-и-масштабируемость">Распределение данных и масштабируемость</h3> <p>Postgres-X2 предусматривает два способа хранения данных в таблицах:</p> <ol> <li><p>Распределенные таблицы (distributed tables, рис. [fig:postgres-x22]): данные по таблице распределяются на указанный набор обработчиков данных с использованием указанной стратегии (hash, round-robin, modulo). Каждая запись в таблице находится только на одном обработчике данных. Параллельно могут быть записаны или прочитаны данные с различных обработчиков данных. За счет этого значительно улучшена производительность на запись и чтение;</p></li> <li><p>Реплицированные таблицы (replicated tables, рис. [fig:postgres-x23]): данные по таблице реплицируется (клонируются) на указанный набор обработчиков данных. Каждая запись в таблице находится на всех обработчиках данных (которые были указаны) и любые изменения дублируются на все обработчики данных. Так как все данные доступны на любом обработчике данных, координатор может собрать все данные из одного узла, что позволяет направить различные запросы на различные обработчики данных. Таким образом создается балансировка нагрузки и увеличения пропускной способности на чтение;</p></li> </ol> <h3 id="таблицы-и-запросы-к-ним">Таблицы и запросы к ним</h3> <p>После установки работа с Postgres-X2 ведется как с обыкновенным PostgreSQL. Подключаться для работы с данными нужно только к координаторам (по умолчанию координатор работает на порту 5432). Для начала создадим распределенные таблицы:</p> <pre><code>CREATE TABLE
users_with_hash (id SERIAL, type INT, ...)
DISTRIBUTE by HASH(id);

CREATE TABLE
users_with_modulo (id SERIAL, type INT, ...)
DISTRIBUTE by MODULO(id);

CREATE TABLE
users_with_rrobin (id SERIAL, type INT, ...)
DISTRIBUTE by ROUNDROBIN;</code></pre> <p>На листинге [lst:postgres-x22] создано 3 распределенные таблицы:</p> <ol> <li><p>Таблица <code>users_with_hash</code> распределяется по хешу значения из указанного поля в таблице (тут указано поле id) по обработчикам данных. Вот как распределились первые 15 значений:</p> <pre><code># координатор
$ psql
# SELECT id, type from users_with_hash ORDER BY id;
 id   | type
-------+-------
     1 |   946
     2 |   153
     3 |   484
     4 |   422
     5 |   785
     6 |   906
     7 |   973
     8 |   699
     9 |   434
    10 |   986
    11 |   135
    12 |  1012
    13 |   395
    14 |   667
    15 |   324

# первый обработчик данных
$ psql -p15432
# SELECT id, type from users_with_hash ORDER BY id;
  id  | type
------+-------
    1 |   946
    2 |   153
    5 |   785
    6 |   906
    8 |   699
    9 |   434
   12 |  1012
   13 |   395
   15 |   324

# второй обработчик данных
$ psql -p15433
# SELECT id, type from users_with_hash ORDER BY id;
 id   | type
-------+-------
     3 |   484
     4 |   422
     7 |   973
    10 |   986
    11 |   135
    14 |   667</code></pre></li> <li><p>Таблица <code>users_with_modulo</code> распределяется по модулю значения из указанного поля в таблице (тут указано поле id) по обработчикам данных. Вот как распределились первые 15 значений:</p> <pre><code># координатор
$ psql
# SELECT id, type from users_with_modulo ORDER BY id;
 id   | type
-------+-------
     1 |   883
     2 |   719
     3 |    29
     4 |   638
     5 |   363
     6 |   946
     7 |   440
     8 |   331
     9 |   884
    10 |   199
    11 |    78
    12 |   791
    13 |   345
    14 |   476
    15 |   860

# первый обработчик данных
$ psql -p15432
# SELECT id, type from users_with_modulo ORDER BY id;
  id   | type
-------+-------
     2 |   719
     4 |   638
     6 |   946
     8 |   331
    10 |   199
    12 |   791
    14 |   476

# второй обработчик данных
$ psql -p15433
# SELECT id, type from users_with_modulo ORDER BY id;
  id  | type
------+-------
    1 |   883
    3 |    29
    5 |   363
    7 |   440
    9 |   884
   11 |    78
   13 |   345
   15 |   860</code></pre></li> <li><p>Таблица <code>users_with_rrobin</code> распределяется циклическим способом(round-robin) по обработчикам данных. Вот как распределились первые 15 значений:</p> <pre><code># координатор
$ psql
# SELECT id, type from users_with_rrobin ORDER BY id;
 id   | type
-------+-------
     1 |   890
     2 |   198
     3 |   815
     4 |   446
     5 |    61
     6 |   337
     7 |   948
     8 |   446
     9 |   796
    10 |   422
    11 |   242
    12 |   795
    13 |   314
    14 |   240
    15 |   733

# первый обработчик данных
$ psql -p15432
# SELECT id, type from users_with_rrobin ORDER BY id;
  id   | type
-------+-------
     2 |   198
     4 |   446
     6 |   337
     8 |   446
    10 |   422
    12 |   795
    14 |   240

# второй обработчик данных
$ psql -p15433
# SELECT id, type from users_with_rrobin ORDER BY id;
  id  | type
------+-------
    1 |   890
    3 |   815
    5 |    61
    7 |   948
    9 |   796
   11 |   242
   13 |   314
   15 |   733</code></pre></li> </ol> <p>Теперь создадим реплицированную таблицу:</p> <pre><code>CREATE TABLE
users_replicated (id SERIAL, type INT, ...)
DISTRIBUTE by REPLICATION;</code></pre> <p>Естественно данные идентичны на всех обработчиках данных:</p> <pre><code># SELECT id, type from users_replicated  ORDER BY id;
  id   | type
-------+-------
     1 |    75
     2 |   262
     3 |   458
     4 |   779
     5 |   357
     6 |    51
     7 |   249
     8 |   444
     9 |   890
    10 |   810
    11 |   809
    12 |   166
    13 |   605
    14 |   401
    15 |    58</code></pre> <p>Рассмотрим как выполняются запросы для таблиц. Выберем все записи из распределенной таблицы:</p> <pre><code># EXPLAIN VERBOSE SELECT * from users_with_modulo ORDER BY id;
                                      QUERY PLAN
--------------------------------------------------------------------------------------
 Sort  (cost=49.83..52.33 rows=1000 width=8)
   Output: id, type
   Sort Key: users_with_modulo.id
   -&gt;  Result  (cost=0.00..0.00 rows=1000 width=8)
         Output: id, type
         -&gt;  Data Node Scan on users_with_modulo  (cost=0.00..0.00 rows=1000 width=8)
               Output: id, type
               Node/s: dn1, dn2
               Remote query: SELECT id, type FROM ONLY users_with_modulo WHERE true
(9 rows)</code></pre> <p>Как видно на листинге [lst:postgres-x26] координатор собирает данные из обработчиков данных, а потом собирает их вместе.</p> <p>Подсчет суммы с группировкой по полю из распределенной таблицы:</p> <pre><code># EXPLAIN VERBOSE SELECT sum(id) from users_with_modulo GROUP BY type;
                                                                      QUERY PLAN
------------------------------------------------------------------------------------------------------------------------------------------------------
 HashAggregate  (cost=5.00..5.01 rows=1 width=8)
   Output: pg_catalog.sum((sum(users_with_modulo.id))), users_with_modulo.type
   -&gt;  Materialize  (cost=0.00..0.00 rows=0 width=0)
         Output: (sum(users_with_modulo.id)), users_with_modulo.type
         -&gt;  Data Node Scan on &quot;__REMOTE_GROUP_QUERY__&quot;  (cost=0.00..0.00 rows=1000 width=8)
               Output: sum(users_with_modulo.id), users_with_modulo.type
               Node/s: dn1, dn2
               Remote query: SELECT sum(group_1.id), group_1.type  FROM (SELECT id, type FROM ONLY users_with_modulo WHERE true) group_1 GROUP BY 2
(8 rows)</code></pre> <p>JOIN между и с участием реплицированных таблиц, а также JOIN между распределенными по одному и тому же полю в таблицах будет выполняются на обработчиках данных. Но JOIN с участием распределенных таблиц по другим ключам будут выполнены на координаторе и скорее всего это будет медленно (листинг [lst:postgres-x28]).</p> <pre><code># EXPLAIN VERBOSE SELECT * from users_with_modulo, users_with_hash WHERE users_with_modulo.id = users_with_hash.id;
                                            QUERY PLAN
--------------------------------------------------------------------------------------------------
 Nested Loop  (cost=0.00..0.01 rows=1 width=16)
   Output: users_with_modulo.id, users_with_modulo.type, users_with_hash.id, users_with_hash.type
   Join Filter: (users_with_modulo.id = users_with_hash.id)
   -&gt;  Data Node Scan on users_with_modulo  (cost=0.00..0.00 rows=1000 width=8)
         Output: users_with_modulo.id, users_with_modulo.type
         Node/s: dn1, dn2
         Remote query: SELECT id, type FROM ONLY users_with_modulo WHERE true
   -&gt;  Data Node Scan on users_with_hash  (cost=0.00..0.00 rows=1000 width=8)
         Output: users_with_hash.id, users_with_hash.type
         Node/s: dn1, dn2
         Remote query: SELECT id, type FROM ONLY users_with_hash WHERE true
(11 rows)</code></pre> <p>Пример выборки данных из реплицированной таблицы:</p> <pre><code># EXPLAIN VERBOSE SELECT * from users_replicated;
                                 QUERY PLAN
----------------------------------------------------------------------------
 Data Node Scan on &quot;__REMOTE_FQS_QUERY__&quot;  (cost=0.00..0.00 rows=0 width=0)
   Output: users_replicated.id, users_replicated.type
   Node/s: dn1
   Remote query: SELECT id, type FROM users_replicated
(4 rows)</code></pre> <p>Как видно из запроса для выборки данных используется один обработчик данных, а не все (что логично).</p> <h3 id="высокая-доступность-ha">Высокая доступность (HA)</h3> <p>По архитектуре у Postgres-X2 всегда есть согласованность данных. По <a href="http://en.wikipedia.org/wiki/CAP_theorem">теореме CAP</a> в такой системе тяжело обеспечить высокую доступность. Для достижения высокой доступности в распределенных системах требуется избыточность данных, резервные копии и автоматическое восстановление. В Postgres-X2 избыточность данных может быть достигнута с помощью PostgreSQL потоковой (streaming) репликации с hot-standby для обработчиков данных. Каждый координатор способен записывать и читать данные независимо от другого, поэтому координаторы способны заменять друг друга. Поскольку GTM отдельный процесс и может стать точкой отказа, лучше создать GTM-standby как резервную копию. Ну а вот для автоматического восстановления придется использовать сторонние утилиты.</p> <h3 id="ограничения">Ограничения</h3> <ol> <li><p>Postgres-X2 базируется на PostgreSQL 9.3;</p></li> <li><p>Нет системы репартиционирования при добавлении или удалении нод;</p></li> <li><p>Нет глобальных <code>UNIQUE</code> на распределенных таблицах;</p></li> <li><p>Не поддерживаются foreign keys между нодами поскольку такой ключ должен вести на данные расположенные на том же обработчике данных;</p></li> <li><p>Не поддерживаются курсоры;</p></li> <li><p>Не поддерживается <code>INSERT ... RETURNING</code>;</p></li> <li><p>Невозможно удаление и добавление нод в кластер без полной реинициализации кластера;</p></li> </ol> <h3 id="заключение-4">Заключение</h3> <p>Postgres-X2 очень перспективное решение для создание кластера на основе PostgreSQL. И хоть это решение имеет ряд недостатков, нестабильно (очень часты случаи падения координаторов при тяжелых запросах) и еще очень молодое, со временем это решение может стать стандартом для масштабирования систем на PostgreSQL.</p> <h2 id="sec:postgres-xl">Postgres-XL</h2> <p>Postgres-XL – система для создания мульти-мастер кластеров, работающих в синхронном режиме – все узлы всегда содержат актуальные данные. Проект построен на основе кодовой базы Postgres-X2, поэтому артитектурный подход полностью идентичен (глобальный менеджер транзакций (GTM), координаторы (coordinators) и обработчики данных (datanodes)). Более подробно про архитектуру можно почитать в «[sec:postgres-x2-architecture] » разделе. Поэтому рассмотрим только отличие Postgres-X2 и Postgres-XL.</p> <h3 id="postgres-x2-и-postgres-xl">Postgres-X2 и Postgres-XL</h3> <p>Одно из главных отличий Postgres-XL от Postgres-X2 является улучшенный механизм массово-параллельной архитектуры (massive parallel processing, MPP). Чтобы понять разницу, давайте рассмотрим как Postgres-X2 и Postgres-XL будет обрабатывать разные SQL запросы. Оба этих кластера будут содержать три таблицы <code>T1</code>, <code>T2</code> и <code>R1</code>. <code>T1</code> имеет колонки <code>a1</code> и <code>a2</code>, <code>T2</code> — <code>b1</code> и <code>b2</code>. <code>T1</code> распределена в кластере по <code>a1</code> полю и <code>T2</code> распределена по <code>b1</code> полю. <code>R1</code> таблица имеет колонки <code>c1</code> и <code>c2</code> и реплицируется в кластере (<code>DISTRIBUTE by REPLICATION</code>).</p> <p>Для начала, простой запрос вида <code>SELECT * FROM T1</code> будет паралельно выполняться на нодах как у Postgres-X2, так и у Postgres-XL. Другой пример запроса <code>SELECT * FROM T1 INNER JOIN R1 ON T1.a1 = R1.c1</code> будет также выполняться паралельно обоими кластерами, потому что будет передан («pushed down») на обработчики данных (datanodes) для выполнения и координатор (coordinators) будет только агрегировать (собирать) результаты запросов. Это будет работать благодаря тому, что <code>R1</code> таблица дублицируется на каждом обработчике данных. Этот тип запросов будет работать хорошо, когда <code>T1</code> является <a href="https://en.wikipedia.org/wiki/Fact_table">таблицей фактов</a> (основной таблицей хранилища данных), в то время как <code>R1</code> — <a href="https://en.wikipedia.org/wiki/Dimension_(data_warehouse)#Dimension_table">таблицей измерений</a> (содержит атрибуты событий, сохраненных в таблице фактов).</p> <p>Теперь рассмотрим другой вариант SQL запроса:</p> <pre><code># SELECT * FROM T1 INNER JOIN T2 ON T1.a1 = T2.b2</code></pre> <p>Данный запрос делает <code>JOIN</code> по распределенной колонке <code>a1</code> в таблице <code>T1</code> и по НЕ распределенной колонке <code>b2</code> в таблице <code>T2</code>. В кластере, который состоит из 4 обработчиков данных, колонка в таблице <code>T1</code> на первом из них потенциально требуется объединить с колонками таблицы <code>T2</code> на всех обработчиках данных в кластере.</p> <p>У Postgres-X2 в данном случае обработчики данных отправляют все данные по заданому условию в запросе к координатору, который и занимается объединением данных с таблиц. В данном примере отсутствует условие <code>WHERE</code>, что значит, что все обработчики данных отправят все содержимое таблиц <code>T1</code> и <code>T2</code> на координатор, который и будет делать <code>JOIN</code> данных. В данной операции будет отсутствовать паралельное выполнение <code>JOIN</code> запроса и будут дополнительные накладные расходы на доставку всех данных к координатору. Поэтому в данном случае Postgres-X2 фактически будет медленее, чем выполнение подобного запроса на обычном PostgreSQL сервере (особенно, если таблицы очень большие).</p> <p>Postgres-XL будет обрабатывать подобный запрос по-другому. Условие <code>T1.a1 = T2.b2</code> говорит о том, что мы объединяем колонку <code>b2</code> с колонкой <code>a1</code>, которая является ключом распределения для таблицы <code>T1</code>. Поэтому, выбрав значения поля <code>b2</code>, кластер будет точно знать для каких обработчиков данных требуется полученый результат для объединения с таблицей <code>T1</code> (поскольку возможно применить хеш функцию распределения на полученые значения). Поэтому каждый обработчик данных считает с другого обработчика данных требуемые данные по таблице <code>T2</code> для объединения со своей таблицей <code>T1</code> без участия координатора. Данная возможность прямой коммуникации обработчиков данных с другими обработчиками данных позволяет распараллеливать более сложные запросы в Postgres-XL.</p> <p>Postgres-XL имеет также другие улучшения производительности (более оптимально обрабатываются последовательности, прочее).</p> <h3 id="заключение-5">Заключение</h3> <p>Postgres-XL - еще одно перспективное решение для создания кластера на основе Postgres-X2. Разработчики данного решения больше нацелены на улучшение производительности и стабильности кластера, вместо добавления нового функционала.</p> <h2 id="sec:citus">Citus</h2> <p><a href="https://www.citusdata.com/">Citus</a> — горизонтально масштабируемый PostgreSQL кластер. Citus использует механизм расширений PostgreSQL вместо того, что бы использовать модифицированную версию базы (как это делает «[sec:postgres-x2] » или «[sec:postgres-xl] »), что позволяет использовать новые версии PostgreSQL с новыми возможностями, сохраняя при этом совместимость с существующими PostgreSQL инструментами. Кластер предоставляет пользователям результаты запросов в режиме «реального времени» для большого и растущего обьема данных (благодаря параллелизации запросов между нодами). Примеры использования:</p> <ul> <li><p>аналитика и вывод данных в реальном времени на графики;</p></li> <li><p>хранение большого набора данных для архива и создание отчетов по ним;</p></li> <li><p>анализ и сегментация большого обьема данных;</p></li> </ul> <p>Нагрузки, которые требуют большой поток данных между узлами кластера, как правило, не будет хорошо работать с Citus кластером. Например:</p> <ul> <li><p>традиционные хранилища данных с длительными и в свободном формате SQL запросами (data warehousing);</p></li> <li><p>множественные распределенные транзакции между несколькими шардами;</p></li> <li><p>запросы, которые возвращают данные по тяжелым <a href="https://ru.wikipedia.org/wiki/ETL">ETL</a> запросам;</p></li> </ul> <h3 id="архитектура">Архитектура</h3> <p>На верхнем уровне Citus кластер распределяет данные по PostgreSQL экземплярам. Входящие SQL запросы затем обрабатываются параллельно через эти сервера.</p> <p>При разворачивании кластера один из экземпляров PostgreSQL выбирается в качестве мастер (master) ноды. Затем остальные добавляются как PostgreSQL воркеры (worker) в конфигурационном файле мастер ноды. После этого все взаимодействие с кластером ведется через мастер ноду с помощью стандартных PostgreSQL интерфейсов. Все данные распределены по воркерам. Мастер хранит только метаданные о воркерах.</p> <p>Citus использует модульную архитектуру для блоков данных, которая похожа на <a href="https://ru.wikipedia.org/wiki/Hadoop#HDFS">HDFS</a> (Hadoop Distributed File System), но использует PostgreSQL таблицы вместо файлов. Каждая из этих таблиц представляет собой горизонтальный раздел или логический «шард» (shard). Каждый шард дублируется, по крайней мере, на двух воркерах (можно настроить на более высокое значение). В результате, потеря одной машины не влияет на доступность данных. Логическая архитектура шардинга в Citus также позволяет добавлять новые воркеры, чтобы увеличить пропускную способность и вычислительную мощность кластера.</p> <p>Citus мастер содержит таблицы метаданных для отслеживания всех воркеров и расположение шардов базы данных на них. Эти таблицы также ведут статистику, такую как размер и минимальное/максимальное значений в шардах, которые помогают распределению SQL запросов Citus планировщику. Таблицы метаданных небольшие (обычно несколько мегабайт), и могут быть дублированы и быстро восстановлены, если с мастером когда-либо произойдет сбой. Подробнее о таблицах метаданных можно глянуть <a href="https://docs.citusdata.com/en/latest/reference/metadata_tables.html">в документации</a>.</p> <p>Когда кластер получает SQL запрос, Citus мастер делит его на более мелкие фрагменты запросов, где каждый фрагмент может выполняться независимо на воркере. Это позволяет Citus распределять каждый запрос в кластере, используя вычислительные мощности всех задействованных узлов, а также отдельных ядер на каждом узле. Мастер затем поручает воркерам выполнить запрос, осуществляет контроль за их исполнением, объединяет результаты по запросам и возвращает конечный результат пользователю. Для того, чтобы гарантировать, что все запросы выполняются в масштабируемой манере, мастер также применяет оптимизации, которые сводят к минимуму объем данных, передаваемых по сети.</p> <p>Citus кластер может легко переносить сбои воркеров из-за своей логической шардинг архитектуры. Если воркер терпит неудачу во время выполнения запроса, Citus завершает запрос, направляя неудачные части запроса другим воркерам, которые имеют копию данных. Если воркер находится в нерабочем состоянии (сервер упал), пользователь может легко произвести ребалансировку кластера, чтобы поддерживать тот же уровень доступности.</p> <h3 id="установка-6">Установка</h3> <p>Установка Citus кластера не требует особых усилий. Для использования в боевом окружении лучше изучить <a href="https://docs.citusdata.com/en/latest/installation/production.html">данную документацию</a>. Для проверки, что кластер работает и мастер видит воркеры можно выполнить команду <code>master_get_active_worker_nodes</code>, которая покажет список воркеров:</p> <pre><code>postgres=# select * from master_get_active_worker_nodes();
 node_name | node_port
-----------+-----------
 localhost |      9702
 localhost |      9701
(2 rows)</code></pre> <h3 id="распределенные-таблицы">Распределенные таблицы</h3> <p>Каждая распределенная таблица в Citus содержит стоблец, который должен быть выбран в качестве значения для распределения по шардам (возможно выбрать только один столбец). Это информирует базу данных как хранить статистику и распределять запросы по кластеру. Как правило, требуется выбрать столбец, который является наиболее часто используемым в запросах <code>WHERE</code>. В таком случае запросы, которые в фильтре используют данный столбец, будут выполняться на шардах, которые выбираются по условию фильтрации. Это помогает значительно уменьшить количество вычислений на шардах.</p> <p>Следующим шагом после выбора столбца на распределения будет определение правильного метода распределения данных в таблицу. В целом, существует два шаблона таблиц: распределенные по времени (время создания заказа, запись логов, прочее) и распределение по идентификатору (ID пользователя, ID приложения, прочее). Citus поддерживает оба метода распределения: append и hash соответственно.</p> <p>Append метод подходит для таблиц, в которые записываются данные по времени (упорядочены по времени). Такой тип таблиц отлично справляется с запросами, которые использут фильтры с диапазонами значений по распределенному столбцу (<code>BETWEEN x AND y</code>). Это обьясняется тем, что мастер хранит диапазоны значений, которые хранятся на шардах, и планировщик может эффективно выбирать шарды, которые содержат данные для SQL запроса.</p> <p>Hash метод распределения подходит для неупорядоченного столбца (user UUID) или по данным, которые могут записываться в любом порядке. В таком случае Citus кластер будет хранить минимальные и максимальные значения для хеш функций на всех шардах. Эта модель лучше подходит для SQL запросов, включающих фильтры на основе равенства по колонке распределения (<code>user_uuid='a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11'</code>).</p> <h4 id="hash-распределение">Hash распределение</h4> <p>Для примера создадим и распределим таблицу по hash методу.</p> <pre><code># CREATE TABLE github_events
(
    event_id bigint,
    event_type text,
    event_public boolean,
    repo_id bigint,
    payload jsonb,
    repo jsonb,
    actor jsonb,
    org jsonb,
    created_at timestamp
);</code></pre> <p>Далее укажем Citus кластеру использовать <code>repo_id</code> с hash распределением для <code>github_events</code> таблицы.</p> <pre><code># SELECT master_create_distributed_table(&#39;github_events&#39;, &#39;repo_id&#39;, &#39;hash&#39;);</code></pre> <p>И создадим шарды для таблицы:</p> <pre><code># SELECT master_create_worker_shards(&#39;github_events&#39;, 16, 1);</code></pre> <p>Данный метод принимает два аргумента в дополнение к имени таблицы: количество шардов и коэффициент репликации. Этот пример позволит создать в общей сложности шестнадцать шардов, где каждый будет владеть частью символического пространства хэша, а данные будут реплицироваться на один воркер.</p> <p>Далее мы можем заполнить таблицу данными:</p> <pre><code>$ wget http://examples.citusdata.com/github_archive/github_events-2015-01-01-{0..5}.csv.gz
$ gzip -d github_events-2015-01-01-*.gz</code></pre> <pre><code># \COPY github_events FROM &#39;github_events-2015-01-01-0.csv&#39; WITH (format CSV)
# INSERT INTO github_events VALUES (2489373118,&#39;PublicEvent&#39;,&#39;t&#39;,24509048,&#39;{}&#39;,&#39;{&quot;id&quot;: 24509048, &quot;url&quot;: &quot;https://api.github.com/repos/SabinaS/csee6868&quot;, &quot;name&quot;: &quot;SabinaS/csee6868&quot;}&#39;,&#39;{&quot;id&quot;: 2955009, &quot;url&quot;: &quot;https://api.github.com/users/SabinaS&quot;, &quot;login&quot;: &quot;SabinaS&quot;, &quot;avatar_url&quot;: &quot;https://avatars.githubusercontent.com/u/2955009?&quot;, &quot;gravatar_id&quot;: &quot;&quot;}&#39;,NULL,&#39;2015-01-01 00:09:13&#39;);</code></pre> <p>Теперь мы можем обновлять и удалять данные с таблицы:</p> <pre><code># UPDATE github_events SET org = NULL WHERE repo_id = 24509048;
# DELETE FROM github_events WHERE repo_id = 24509048;</code></pre> <p>Для работы <code>UPDATE</code> и <code>DELETE</code> запросов требуется, что бы он «затрагивал» один шард. Это означает, что условие <code>WHERE</code> должно содержать условие, что ограничит выполнение запроса на один шард по распределенному столбцу. Для обновления или удаления данных на нескольких шардах требуется использовать команду <code>master_modify_multiple_shards</code>:</p> <pre><code># SELECT master_modify_multiple_shards(
  &#39;DELETE FROM github_events WHERE repo_id IN (24509048, 24509049)&#39;);</code></pre> <p>Для удаления таблицы достаточно выполнить <code>DROP TABLE</code> на мастере:</p> <pre><code># DROP TABLE github_events;</code></pre> <h4 id="append-распределение">Append распределение</h4> <p>Для примера создадим и распределим таблицу по append методу.</p> <pre><code># CREATE TABLE github_events
(
    event_id bigint,
    event_type text,
    event_public boolean,
    repo_id bigint,
    payload jsonb,
    repo jsonb,
    actor jsonb,
    org jsonb,
    created_at timestamp
);</code></pre> <p>Далее укажем Citus кластеру использовать <code>created_at</code> с append распределением для <code>github_events</code> таблицы.</p> <pre><code># SELECT master_create_distributed_table(&#39;github_events&#39;, &#39;created_at&#39;, &#39;append&#39;);</code></pre> <p>После этого мы можем использовать таблицу и загружать в нее данные:</p> <pre><code># SET citus.shard_max_size TO &#39;64MB&#39;;
# \copy github_events from &#39;github_events-2015-01-01-0.csv&#39; WITH (format CSV)</code></pre> <p>По умолчанию команда <code>\copy</code> требует два конфигурационных параметра для работы: <code>citus.shard_max_size</code> и <code>citus.shard_replication_factor</code>.</p> <ul> <li><p><code>citus.shard_max_size</code> параметр указывает максимальный размер шарда при использовании команды <code>\copy</code> (1Гб по умолчанию). Если файл больше данного параметра, то команда автоматически разобьет файл по нескольким шардам;</p></li> <li><p><code>citus.shard_replication_factor</code> параметр количество воркеров, на которые шарды будут реплицироваться (2 по умолчанию);</p></li> </ul> <p>По умолчанию команда <code>\copy</code> создает каждый раз новый шард для данных. Если требуется добавлять данные в один и тот же шард, существуют команды <code>master_create_empty_shard</code>, которая вернет идентификатор на новый шард, и команда <code>master_append_table_to_shard</code> для добавления данных в этот шард по идентификатору.</p> <p>Для удаления старых данных можно использовать команду <code>master_apply_delete_command</code>, которая удаляет старые шарды, которые попадают в переданное условие на удаление:</p> <pre><code># SELECT * from master_apply_delete_command(&#39;DELETE FROM github_events WHERE created_at &gt;= &#39;&#39;2015-01-01 00:00:00&#39;&#39;&#39;);
 master_apply_delete_command
-----------------------------
                           3
(1 row)</code></pre> <p>Для удаления таблицы достаточно выполнить <code>DROP TABLE</code> на мастере:</p> <pre><code># DROP TABLE github_events;</code></pre> <h3 id="ребалансировка-кластера">Ребалансировка кластера</h3> <p>Логическая архитектура шардинга Citus позволяет масштабировать кластер без каких-либо простоев (no downtime!). Для добавления нового воркера достаточно добавить его в <code>pg_worker_list.conf</code> и вызвать на мастере <code>pg_reload_conf</code> для загрузки новой конфигурации:</p> <pre><code># SELECT pg_reload_conf();</code></pre> <p>После этого Citus автоматически начнет использовать данный воркер для новых распределенных таблиц. Если требуется ребалансировать существующие таблицы на новый воркер, то для этого есть команда <code>rebalance_table_shards</code>, но, к сожалению, она доступна только в Citus Enterprise (платное решение).</p> <h3 id="ограничения-1">Ограничения</h3> <p>Модель расширения PostgreSQL в Citus позволяет использовать доступные типы данных (JSON, JSONB, другие) и другие расширения в кластере. Но не весь спектр SQL запросов доступен для распределенных таблиц. На текущий момент распределенные таблицы не поддерживают:</p> <ul> <li><p>Оконные функции (window functions);</p></li> <li><p>Общие табличные выражения (CTE);</p></li> <li><p><code>UNION</code> операции (<code>UNION/INTERSECT/EXCEPT</code>);</p></li> <li><p>Транзакционная семантика для запросов, которые распределены по нескольким шардам;</p></li> </ul> <h3 id="заключение-6">Заключение</h3> <p>Citus кластер достаточно гибкое и мощное решение для горизонтального масштабирования PostgreSQL. Зрелость данного решения показывает его использование такими игроками на рынке, как CloudFlare, Heap и многими другими.</p> <h2 id="sec:greenplum">Greenplum Database</h2> <p><a href="http://greenplum.org/">Greenplum Database (GP)</a> — реляционная СУБД, имеющая массово-параллельную (massive parallel processing) архитектуру без разделения ресурсов (shared nothing). Для подробного понимания принципов работы Greenplum необходимо обозначить основные термины:</p> <ul> <li><p>Master instance («мастер») — инстанс PostgreSQL, являющийся одновременно координатором и входной точкой для пользователей в кластере;</p></li> <li><p>Master host («сервер-мастер») — сервер, на котором работает master instance;</p></li> <li><p>Secondary master instance — инстанс PostgreSQL, являющийся резервным мастером, включается в работу в случае недоступности основного мастера (переключение происходит вручную);</p></li> <li><p>Primary segment instance («сегмент») — инстанс PostgreSQL, являющийся одним из сегментов. Именно сегменты непосредственно хранят данные, выполняют с ними операции и отдают результаты мастеру (в общем случае). По сути сегмент — самый обычный инстанс PostgreSQL 8.3.23 с настроенной репликацией в своё зеркало на другом сервере;</p></li> <li><p>Mirror segment instance («зеркало») — инстанс PostgreSQL, являющийся зеркалом одного из primary сегментов, автоматически принимает на себя роль primary в случае падения оного. Greenplum поддерживает только 1-to-1 репликацию сегментов: для каждого из primary может быть только одно зеркало;</p></li> <li><p>Segment host («сервер-сегмент») — сервер, на котором работает один или несколько сегментов и/или зеркал;</p></li> </ul> <p>В общем случае кластер GP состоит из нескольких серверов-сегментов, одного сервера-мастера, и одного сервера-секондари-мастера, соединённых между собой одной или несколькими быстрыми (10g, infiniband) сетями, обычно обособленными (interconnect) (рис [fig:greenplum_arch1]).</p> <p>Использование нескольких interconnect-сетей позволяет, во-первых, повысить пропускную способность канала взаимодействия сегментов между собой, и во-вторых, обеспечить отказоустойчивость кластера (в случае отказа одной из сетей весь трафик перераспределяется между оставшимися).</p> <p>При выборе числа серверов-сегментов важно правильно выбрать соотношение кластера «число процессоров/Тб данных» в зависимости от планируемого профиля нагрузки на БД — чем больше процессорных ядер приходится на единицу данных, тем быстрее кластер будет выполнять «тяжёлые» операции, а также работать со сжатыми таблицами.</p> <p>При выборе числа сегментов в кластере (которое в общем случае к числу серверов никак не привязано) необходимо помнить следующее:</p> <ul> <li><p>все ресурсы сервера делятся между всеми сегментами на сервере (нагрузкой зеркал, в случае если они располагаются на этих же серверах, можно условно пренебречь);</p></li> <li><p>каждый запрос на одном сегменте не может потреблять процессорных ресурсов больше, чем одно ядро CPU. Это означает, например, что, если кластер состоит из 32-ядерных серверов с 4-я сегментами GP на борту и используется в среднем для обработки 3-4 одновременных тяжёлых, хорошо утилизирующих CPU, запросов, «в среднем по больнице» CPU не будет утилизироваться оптимально. В данной ситуации лучше увеличить число сегментов на сервере до 6-8;</p></li> <li><p>штатный процесс бэкапа и рестора данных «из коробки» работает только на кластерах, имеющих одинаковое число сегментов. Восстановить данные, забэкапленные на кластере из 96 сегментов, в кластер из 100 сегментов без напильника будет невозможно;</p></li> </ul> <h3 id="subsec:greenplum_data_storage">Хранение данных</h3> <p>В Greenplum реализуется классическая схема шардирования данных. Каждая таблица представляет из себя N+1 таблиц на всех сегментах кластера, где N — число сегментов (+1 в этом случае — это таблица на мастере, данных в ней нет). На каждом сегменте хранится 1/N строк таблицы. Логика разбиения таблицы на сегменты задаётся ключом (полем) дистрибуции — таким полем, на основе данных которого любую строку можно отнести к одному из сегментов.</p> <p>Ключ (поле или набор полей) дистрибуции — очень важное понятие в GP. Как было сказано выше, Greenplum работает со скоростью самого медленного сегмента, это означает, что любой перекос в количестве данных (как в рамках одной таблицы, так и в рамках всей базы) между сегментами ведёт к деградации производительности кластера, а также к другим проблемам. Именно поэтому следует тщательно выбирать поле для дистрибуции — распределение количества вхождений значений в нём должно быть как можно более равномерным. Правильно ли вы выбрали ключ дистрибуции вам подскажет служебное поле <code>gp_segment_id</code>, существующее в каждой таблице — оно содержит номер сегмента, на котором хранится конкретная строка. Важный нюанс: GP не поддерживает <code>UPDATE</code> поля, по которому распределена таблица.</p> <p>Рассмотрим пример (здесь и далее в примерах кластер состоит из 96 сегментов):</p> <pre><code>db=# create table distrib_test_table as select generate_series(1,20) as num_field distributed by (num_field);
SELECT 20
db=# select count(1),gp_segment_id from distrib_test_table group by gp_segment_id order by gp_segment_id;
 count | gp_segment_id
-------+---------------
     1 |             4
     1 |             6
     1 |            15
     1 |            21
     1 |            23
     1 |            25
     1 |            31
     1 |            40
     1 |            42
     1 |            48
     1 |            50
     1 |            52
     1 |            65
     1 |            67
     1 |            73
     1 |            75
     1 |            77
     1 |            90
     1 |            92
     1 |            94

db=# truncate table distrib_test_table;
TRUNCATE TABLE
db=# insert into distrib_test_table values (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1);
INSERT 0 20
db=# select count(1),gp_segment_id from distrib_test_table group by gp_segment_id order by gp_segment_id;
 count | gp_segment_id
-------+---------------
    20 |            42</code></pre> <p>В обоих случаях распределена таблица по полю <code>num_field</code>. В первом случае вставили в это поле 20 уникальных значений, и, как видно, GP разложил все строки на разные сегменты. Во втором случае в поле было вставлено 20 одинаковых значений, и все строки были помещены на один сегмент.</p> <p>В случае, если в таблице нет подходящих полей для использования в качестве ключа дистрибуции, можно воспользоваться случайной дистрибуцией (<code>DISTRIBUTED RANDOMLY</code>). Поле для дистрибуции можно менять в уже созданной таблице, однако после этого её необходимо перераспределить. Именно по полю дистрибуции Greenplum совершает самые оптимальные <code>JOIN</code>: в случае, если в обоих таблицах поля, по которым совершается <code>JOIN</code>, являются ключами дистрибуции, <code>JOIN</code> выполняется локально на сегменте. Если же это условие не верно, GP придётся или перераспределить обе таблицы по искомому полю, или закинуть одну из таблиц целиком на каждый сегмент (операция <code>BROADCAST</code>) и уже затем джойнить таблицы локально на сегментах.</p> <pre><code>db=# create table distrib_test_table as select generate_series(1,192) as num_field, generate_series(1,192) as num_field_2 distributed by (num_field);
SELECT 192
db=# create table distrib_test_table_2 as select generate_series(1,1000) as num_field, generate_series(1,1000) as num_field_2 distributed by (num_field);
SELECT 1000
db=# explain select * from distrib_test_table sq
db-# left join distrib_test_table_2 sq2
db-# on sq.num_field = sq2.num_field;
QUERY PLAN
------------------------------------------------------------------------------------------
 Gather Motion 96:1  (slice1; segments: 96)  (cost=20.37..42.90 rows=861 width=16)
   -&gt;  Hash Left Join  (cost=20.37..42.90 rows=9 width=16)
         Hash Cond: sq.num_field = sq2.num_field
         -&gt;  Seq Scan on distrib_test_table sq  (cost=0.00..9.61 rows=9 width=8)
         -&gt;  Hash  (cost=9.61..9.61 rows=9 width=8)
               -&gt;  Seq Scan on distrib_test_table_2 sq2  (cost=0.00..9.61 rows=9 width=8)</code></pre> <pre><code>db_dev=# explain select * from distrib_test_table sq left join distrib_test_table_2 sq2
on sq.num_field_2 = sq2.num_field_2;
                                               QUERY PLAN
--------------------------------------------------------------------------------------------------------
 Gather Motion 96:1  (slice3; segments: 96)  (cost=37.59..77.34 rows=861 width=16)
   -&gt;  Hash Left Join  (cost=37.59..77.34 rows=9 width=16)
         Hash Cond: sq.num_field_2 = sq2.num_field_2
         -&gt;  Redistribute Motion 96:96  (slice1; segments: 96)  (cost=0.00..26.83 rows=9 width=8)
               Hash Key: sq.num_field_2
               -&gt;  Seq Scan on distrib_test_table sq  (cost=0.00..9.61 rows=9 width=8)
         -&gt;  Hash  (cost=26.83..26.83 rows=9 width=8)
               -&gt;  Redistribute Motion 96:96  (slice2; segments: 96)  (cost=0.00..26.83 rows=9 width=8)
                     Hash Key: sq2.num_field_2
                     -&gt;  Seq Scan on distrib_test_table_2 sq2  (cost=0.00..9.61 rows=9 width=8)</code></pre> <p>Как видно в примере «» в плане запроса появляются два дополнительных шага (по одному для каждой из участвующих в запросе таблиц): <code>Redistribute Motion</code>. По сути, перед выполнением запроса GP перераспределяет обе таблицы по сегментам, используя логику поля <code>num_field_2</code>, а не изначального ключа дистрибуции — поля <code>num_field</code>.</p> <h3 id="взаимодействие-с-клиентами">Взаимодействие с клиентами</h3> <p>В общем случае всё взаимодействие клиентов с кластером ведётся только через мастер — именно он отвечает клиентам, выдаёт им результат запроса и т.д. Обычные клиенты не имеют сетевого доступа к серверам-сегментам.</p> <p>Для ускорения загрузки данных в кластер используется bulk load — параллельная загрузка данных с/на клиент одновременно с нескольких сегментов. Bulk load возможен только с клиентов, имеющих доступ в интерконнекты. Обычно в роли таких клиентов выступают ETL-сервера и другие системы, которым необходима загрузка большого объёма данных (на рис [fig:greenplum_arch1] они обозначены как ETL/Pro client).</p> <p>Для параллельной загрузки данных на сегменты используется утилита <code>gpfdist</code>. По сути, утилита поднимает на удалённом сервере web-сервер, который предоставляет доступ по протоколам gpfdist и http к указанной папке. После запуска директория и все файлы в ней становятся доступны обычным <code>wget</code>. Создадим для примера файл в директории, обслуживаемой <code>gpfdist</code>, и обратимся к нему как к обычной таблице.</p> <pre><code># На ETL-сервере:
bash# for i in {1..1000}; do echo &quot;$i,$(cat /dev/urandom | tr -dc &#39;a-zA-Z0-9&#39; | fold -w 8 | head -n 1)&quot;; done &gt; /tmp/work/gpfdist_home/test_table.csv

# Теперь создаим внешнюю таблицу и прочитаем данные из файла
# В Greenplum DB:
db=# create external table ext_test_table
db-# (id integer, rand varchar(8))
db-# location (&#39;gpfdist://etl_hostname:8081/test_table.csv&#39;)
db-# format &#39;TEXT&#39; (delimiter &#39;,&#39; NULL &#39; &#39;);
CREATE EXTERNAL TABLE
db_dev=# select * from ext_test_table limit 100;
NOTICE:  External scan from gpfdist(s) server will utilize 64 out of 96 segment databases
 id  |   rand
-----+----------
   1 | UWlonJHO
   2 | HTyJNA41
   3 | CBP1QSn1
   4 | 0K9y51a3
...</code></pre> <p>Также, но с немного другим синтаксисом, создаются внешние web-таблицы. Их особенность заключается в том, что они ссылаются на http протокол, и могут работать с данными, предоставляемыми сторонними web-серверами (apache, nginx и другие).</p> <p>В Greenplum также существует возможность создавать внешние таблицы на данные, лежащие на распределённой файловой системе Hadoop (hdfs) — за это в GP ответственна отдельная компонента <code>gphdfs</code>. Для обеспечения её работы на каждый сервер, входящий в состав кластера GP, необходимо установить библиотеки Hadoop и прописать к ним путь в одной из системных переменных базы. Создание внешней таблицы, обращающейся к данным на hdfs, будет выглядеть примерно так:</p> <pre><code>db=# create external table hdfs_test_table
db=# (id int, rand text)
db=# location(&#39;gphdfs://hadoop_name_node:8020/tmp/test_file.csv&#39;)
db=# format &#39;TEXT&#39; (delimiter &#39;,&#39;);</code></pre> <p>где <code>hadoop_name_node</code> — адрес хоста неймноды, <code>/tmp/test_file.csv</code> — путь до искомого файла на hdfs.</p> <p>При обращении к такой таблице Greenplum выясняет у неймноды Hadoop расположение нужных блоков данных на датанодах, к которым затем обращается с серверов-сегментов параллельно. Естественно, все ноды кластера Hadoop должны быть в сетях интерконнекта кластера Greenplum. Такая схема работы позволяет достичь значительного прироста скорости даже по сравнению с <code>gpfdist</code>. Что интересно, логика выбора сегментов для чтения данных с датанод hdfs является весьма нетривиальной. Например, GP может начать тянуть данные со всех датанод только двумя сегмент-серверами, причём при повторном аналогичном запросе схема взаимодействия может поменяться.</p> <p>Также есть тип внешних таблиц, которые ссылаются на файлы на сегмент-серверах или файл на мастере, а также на результат выполнения команды на сегмент-серверах или на мастере. К слову сказать, старый добрый <code>COPY FROM</code> никуда не делся и также может использоваться, однако по сравнению с описанным выше работает он медленней.</p> <h3 id="надёжность-и-резервирование">Надёжность и резервирование</h3> <h4 id="резервирование-мастера">Резервирование мастера</h4> <p>Как было сказано ранее, в кластере GP используется полное резервирование мастера с помощью механизма репликации транзакционных логов, контролируемого специальным агентом (<code>gpsyncagent</code>). При этом автоматическое переключение роли мастера на резервный инстанс не поддерживается. Для переключения на резервный мастер необходимо:</p> <ul> <li><p>убедиться, что основной мастер остановлен (процесс убит и в рабочей директории инстанса мастера отсутствует файл postmaster.pid);</p></li> <li><p>на сервере резервного мастера выполнить команду <code>gpactivatestandby -d /master_instance_directory</code>;</p></li> <li><p>переключить виртуальный ip-адрес на сервер нового мастера (механизм виртуального ip в Greenplum отсутствует, необходимо использовать сторонние инструменты);</p></li> </ul> <p>Как видно, переключение выполняется совсем не сложно и при принятии определённых рисков может быть автоматизировано.</p> <h4 id="резервирование-сегментов">Резервирование сегментов</h4> <p>Схема резервирования сегментов похожа на таковую для мастера, отличия совсем небольшие. В случае падения одного из сегментов (инстанс PostgreSQL перестаёт отвечать мастеру в течении таймаута) сегмент помечается как сбойный, и вместо него автоматически запускается его зеркало (по сути, абсолютно аналогичный инстанс PostgreSQL). Репликация данных сегмента в его зеркало происходит на основе кастомной синхронной репликации на уровне файлов.</p> <p>Cтоит отметить, что довольно важное место в процессе планирования архитектуры кластера GP занимает вопрос расположения зеркал сегментов на серверах, благо GP даёт полную свободу в вопросе выбора мест расположения сегментов и их зеркал: с помощью специальной карты расположения сегментов их можно разместить на разных серверах, в разных директориях и заставить использовать разные порты. Рассмотрим два варианта:</p> <p>При использовании схемы [fig:greenplum_reserve_one] при отказе одного из серверов на сервере-соседе оказывается в два раза больше работающих сегментов. Как было сказано выше, производительность кластера равняется производительности самого медленного из сегментов, а значит, в случае отказа одного сервера производительность базы снижается минимум вдвое. Однако, такая схема имеет и положительные стороны: при работе с отказавшим сервером уязвимым местом кластера становится только один сервер — тот самый, куда переехали сегменты.</p> <p>При использовании схемы [fig:greenplum_reserve_two] в случае отказа сервера возросшая нагрузка равномерно распределяется между несколькими серверами, не сильно влияя на общую производительность кластера. Однако, существенно повышается риск выхода из строя всего кластера — достаточно выйти из строя одному из M серверов, соседствующих с вышедшим из строя изначально.</p> <p>Истина, как это часто бывает, где-то посередине — можно расположить по несколько зеркал сегментов одного сервера на нескольких других серверах, можно объединять сервера в группы отказоустойчивости, и так далее. Оптимальную конфигурацию зеркал следует подбирать исходя из конкретных аппаратных данных кластера, критичности простоя и так далее.</p> <p>Также в механизме резервирования сегментов есть ещё один нюанс, влияющий на производительность кластера. В случае выхода из строя зеркала одного из сегментов последний переходит в режим <code>change tracking</code> — сегмент логирует все изменения, чтобы затем при восстановлении упавшего зеркала применить их к нему, и получить свежую, консистентную копию данных. Другими словами, при падении зеркала нагрузка, создаваемая на дисковую подсистему сервера сегментом, оставшимся без зеркала, существенно возрастает.</p> <p>При устранении причины отказа сегмента (аппаратные проблемы, кончившееся место на устройстве хранения и прочее) его необходимо вернуть в работу вручную, с помощью специальной утилиты <code>gprecoverseg</code> (даунтайм СУБД не требуется). По факту эта утилита скопирует скопившиеся на сегменте WA-логи на зеркало и поднимет упавший сегмент/зеркало. В случае, если речь идёт о primary-сегменте, изначально он включится в работу как зеркало для своего зеркала, ставшего primary (зеркало и основной сегмент будут работать поменявшись ролями). Для того, чтобы вернуть всё на круги своя, потребуется процедура ребаланса — смены ролей. Такая процедура также не требует даунтайма СУБД, однако на время ребаланса все сессии в БД подвиснут.</p> <p>В случае, если повреждения упавшего сегмента настолько серьёзны, что простым копированием данных из WA-логов не обойтись, есть возможность использовать полное восстановление упавшего сегмента — в таком случае, по факту, инстанс PostgreSQL будет создан заново, однако за счёт того, что восстановление будет не инкрементальным, процесс восстановления может занять продолжительное время.</p> <h3 id="производительность">Производительность</h3> <p>Оценка производительности кластера Greenplum – понятие довольно растяжимое. Исходные данные: кластер из 24 сегмент-серверов, каждый сервер — 192 Гб памяти, 40 ядер. Число primary-сегментов в кластере: 96. В первом примере мы создаём таблицу с 4-я полями + первичный ключ по одному из полей. Затем мы наполняем таблицу данными (10 000 000 строк) и пробуем выполнить простой <code>SELECT</code> с несколькими условиями.</p> <pre><code>db=# CREATE TABLE test3
db-# (id bigint NOT NULL,
db(# profile bigint NOT NULL,
db(# status integer NOT NULL,
db(# switch_date timestamp without time zone NOT NULL,
db(# CONSTRAINT test3_id_pkey PRIMARY KEY (id) )
db-# distributed by (id);
NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index &quot;test3_pkey&quot; for table &quot;test3&quot;
CREATE TABLE

db=# insert into test3 (id , profile,status, switch_date) select a, round(random()*100000), round(random()*4), now() - &#39;1 year&#39;::interval * round(random() * 40) from generate_series(1,10000000) a;
INSERT 0 10000000

db=# explain analyze  select  profile, count(status) from test3
db=#                         where status&lt;&gt;2
db=#                         and switch_date between &#39;1970-01-01&#39; and &#39;2015-01-01&#39;  group by profile;

Gather Motion 96:1 (slice2; segments: 96) (cost=2092.80..2092.93 rows=10 width=16)
Rows out: 100001 rows at destination with 141 ms to first row, 169 ms to end, start offset by 0.778 ms.
-&gt; HashAggregate (cost=2092.80..2092.93 rows=1 width=16)
   Group By: test3.profile
   Rows out: Avg 1041.7 rows x 96 workers. Max 1061 rows (seg20) with 141 ms to end, start offset by 2.281 ms.
   Executor memory: 4233K bytes avg, 4233K bytes max (seg0).
   -&gt; Redistribute Motion 96:96 (slice1; segments: 96) (cost=2092.45..2092.65 rows=1 width=16)
      Hash Key: test3.profile
      Rows out: Avg 53770.2 rows x 96 workers at destination. Max 54896 rows (seg20) with 71 ms to first row, 117 ms to end, start offset by 5.205 ms.
      -&gt; HashAggregate (cost=2092.45..2092.45 rows=1 width=16)
      Group By: test3.profile
      Rows out: Avg 53770.2 rows x 96 workers. Max 54020 rows (seg69) with 71 ms to first row, 90 ms to end, start offset by 7.014 ms.
      Executor memory: 7882K bytes avg, 7882K bytes max (seg0).
      -&gt; Seq Scan on test3 (cost=0.00..2087.04 rows=12 width=12)
         Filter: status &lt;&gt; 2 AND switch_date &gt;= &#39;1970-01-01 00:00:00&#39;::timestamp without time zone AND switch_date &lt;= &#39;2015-01-01 00:00:00&#39;::timestamp without time zone
         Rows out: Avg 77155.1 rows x 96 workers. Max 77743 rows (seg26) with 0.092 ms to first row, 31 ms to end, start offset by 7.881 ms.
Slice statistics:
(slice0) Executor memory: 364K bytes.
(slice1) Executor memory: 9675K bytes avg x 96 workers, 9675K bytes max (seg0).
(slice2) Executor memory: 4526K bytes avg x 96 workers, 4526K bytes max (seg0).
Statement statistics:
Memory used: 128000K bytes
Total runtime: 175.859 ms</code></pre> <p>Как видно, время выполнения запроса составило 175 мс. Теперь попробуем пример с джойном по ключу дистрибуции одной таблицы и по обычному полю другой таблицы.</p> <pre><code>db=# create table test3_1 (id bigint NOT NULL, name text, CONSTRAINT test3_1_id_pkey PRIMARY KEY (id)) distributed by (id);
NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index &quot;test3_1_pkey&quot; for table &quot;test3_1&quot;
CREATE TABLE
db=# insert into test3_1 (id , name) select a, md5(random()::text) from generate_series(1,100000) a;
INSERT 0 100000
db=# explain analyze select test3.*,test3_1.name from test3 join test3_1 on test3.profile=test3_1.id;

-&gt; Hash Join (cost=34.52..5099.48 rows=1128 width=60)
   Hash Cond: test3.profile = test3_1.id
   Rows out: Avg 104166.2 rows x 96 workers. Max 106093 rows (seg20) with 7.644 ms to first row, 103 ms to end, start offset by 223 ms.
   Executor memory: 74K bytes avg, 75K bytes max (seg20).
   Work_mem used: 74K bytes avg, 75K bytes max (seg20). Workfile: (0 spilling, 0 reused)
   (seg20) Hash chain length 1.0 avg, 1 max, using 1061 of 262151 buckets.
   -&gt; Redistribute Motion 96:96 (slice1; segments: 96) (cost=0.00..3440.64 rows=1128 width=28)
      Hash Key: test3.profile
      Rows out: Avg 104166.7 rows x 96 workers at destination. Max 106093 rows (seg20) with 3.160 ms to first row, 44 ms to end, start offset by 228 ms.
      -&gt; Seq Scan on test3 (cost=0.00..1274.88 rows=1128 width=28)
      Rows out: Avg 104166.7 rows x 96 workers. Max 104209 rows (seg66) with 0.165 ms to first row, 16 ms to end, start offset by 228 ms.
   -&gt; Hash (cost=17.01..17.01 rows=15 width=40)
      Rows in: Avg 1041.7 rows x 96 workers. Max 1061 rows (seg20) with 1.059 ms to end, start offset by 227 ms.
      -&gt; Seq Scan on test3_1 (cost=0.00..17.01 rows=15 width=40)
         Rows out: Avg 1041.7 rows x 96 workers. Max 1061 rows (seg20) with 0.126 ms to first row, 0.498 ms to end, start offset by 227 ms.
Slice statistics:
(slice0) Executor memory: 364K bytes.
(slice1) Executor memory: 1805K bytes avg x 96 workers, 1805K bytes max (seg0).
(slice2) Executor memory: 4710K bytes avg x 96 workers, 4710K bytes max (seg0). Work_mem: 75K bytes max.
Statement statistics:
Memory used: 128000K bytes
Total runtime: 4526.065 ms</code></pre> <p>Время выполнения запроса составило 4.6 секунды. Много это или мало для такого объёма данных — вопрос спорный и лежащий вне этой книги.</p> <h3 id="расширение-кластера">Расширение кластера</h3> <p>В жизненном цикле распределённой аналитической БД рано или поздно возникает ситуация, когда объём доступного дискового пространства уже не может вместить всех необходимых данных, а добавление устройств хранения в имеющиеся сервера либо невозможна, либо слишком дорога и сложна (потребуется, как минимум, расширение существующих разделов). Кроме того, добавление одних лишь дисковых мощностей негативно скажется на соотношении «число процессоров/Тб данных», о котором мы говорили в «[subsec:greenplum_data_storage] ». Говоря простым языком, в кластер рано или поздно понадобится вводить новые сервера. Greenplum позволяет добавлять как новые сервера, так и новые сегменты практически без простоя СУБД. Последовательность этого действа примерно такая:</p> <ul> <li><p>разработать карту сегментов, согласно которой GP будет размещать новые сегменты и зеркала на новых серверах;</p></li> <li><p>сделать бэкап необходимых критичных данных (как минимум всех метаданных);</p></li> <li><p>установить ПО СУБД на новые сервера;</p></li> <li><p>остановить СУБД (следующий пункт выполняется в даунтайм);</p></li> <li><p>инициализировать новые сегменты утилитой <code>gpexpand</code> (занимает от 5 до 10 минут);</p></li> <li><p>поднять СУБД (даунтайм окончен);</p></li> <li><p>перераспределить (redistribute) все таблицы;</p></li> <li><p>собрать статистику (analyze) по всем таблицам;</p></li> </ul> <p>Как видно, хотя процедура расширения и продолжительна, полная недоступность БД при правильных действиях администратора не превысит 20-30 минут.</p> <h3 id="особенности-эксплуатации">Особенности эксплуатации</h3> <p>Как обычно, практика вносит в красивую теорию свои коррективы. Поделюсь некоторыми нюансами эксплуатации, выявленными нами за долгое время использования GP. Сразу оговорюсь, что стандартные нюансы PostgreSQL (необходимость <code>VACUUM</code>, особенности репликации) в этот перечень не попали:</p> <ul> <li><p>Автоматический failover не даёт 100% гарантии переключения на зеркало. Увы, но это так, особенно под нагрузкой, есть риск зависания процессов базы при попытке переключения на зеркало. Частично проблему решает уменьшение таймаута ответа от сегментов до нескольких минут, однако даже в таком случае риск остаётся. Как частное решение проблемы зависания при переключении можно использовать ручное убийство зависшего сегмента или перезагрузку базы;</p></li> <li><p>Greenplum и OLTP несовместимы. GP — аналитическая БД, предназначенная для небольшого числа одновременных запросов, выполняющих тяжёлые операции над большим объёмом данных. Большое число (более 600 queries per second) лёгких запросов/транзакций, выполняющих одну операцию, негативно сказывается на производительности базы из-за её распределённой архитектуры — каждая транзакция на мастере порождает N транзакций на сегментах. Хорошей практикой является агрегация большого числа <code>UPDATE/INSERT</code> в батчи;</p></li> <li><p>Отсутствие механизма инкрементального бэкапа;</p></li> <li><p>Свой синтаксис. Несмотря на то, что для клиента Greenplum по сути является PostgreSQL DB, небольшие различия в синтаксисе SQL заставляют использовать стандартный клиентский PostgreSQL-софт с большой осторожностью;</p></li> <li><p>Отсутствие возможности пометить сегменты как «архивные». Частично этот недостаток можно решить путём использования архивных партиций, находящихся на медленном дешевом tablespace, а также c помощью появившейся в последней на момент написания главы версии GP 4.3.6.0 возможности располагать партиции таблицы во внешних источниках (например, внешних таблицах <code>gphdfs</code>, лежащих в кластере Hadoop);</p></li> <li><p>Greenplum использует PostgreSQL версии 8.3.23, а значит о многих современных плюшках этой замечательной БД придётся забыть;</p></li> </ul> <h3 id="заключение-7">Заключение</h3> <p>Greenplum — мощный и гибкий инструмент для аналитической обработки больших объёмов данных. Он требует к себе немного другого подхода, чем остальные enterprise-level решения для Data Warehouse («напильник» — любимый инструмент администратора GP). Однако при достаточно низком пороге вхождения и большой унифицированности с PostgreSQL Greenplum является сильным игроком на поле Data Warehouse DB.</p> <h2 id="заключение-8">Заключение</h2> <p>В данной главе рассмотрены лишь базовые настройки кластеров БД. Про кластеры PostgreSQL потребуется написать отдельную книгу, чтобы рассмотреть все шаги с установкой, настройкой и работой кластеров. Надеюсь, что несмотря на это, информация будет полезна многим читателям.</p> <h1 id=pgpool-ii>PgPool-II</h1> <h2 id="введение-5">Введение</h2> <p>Pgpool-II — это прослойка, работающая между серверами PostgreSQL и клиентами СУБД PostgreSQL. Она предоставляет следующие функции:</p> <ul> <li><p><strong>Объединение соединений</strong></p> <p>Pgpool-II сохраняет соединения с серверами PostgreSQL и использует их повторно в случае если новое соединение устанавливается с теми же параметрами (т.е. имя пользователя, база данных, версия протокола). Это уменьшает накладные расходы на соединения и увеличивает производительность системы в целом;</p></li> <li><p><strong>Репликация</strong></p> <p>Pgpool-II может управлять множеством серверов PostgreSQL. Использование функции репликации данных позволяет создание резервной копии данных в реальном времени на 2 или более физических дисков, так что сервис может продолжать работать без остановки серверов в случае выхода из строя диска;</p></li> <li><p><strong>Балансировка нагрузки</strong></p> <p>Если база данных реплицируется, то выполнение запроса SELECT на любом из серверов вернет одинаковый результат. pgpool-II использует преимущество функции репликации для уменьшения нагрузки на каждый из серверов PostgreSQL распределяя запросы <code>SELECT</code> на несколько серверов, тем самым увеличивая производительность системы вцелом. В лучшем случае производительность возрастает пропорционально числу серверов PostgreSQL. Балансировка нагрузки лучше всего работает в случае когда много пользователей выполняют много запросов в одно и то же время.</p></li> <li><p><strong>Ограничение лишних соединений</strong></p> <p>Существует ограничение максимального числа одновременных соединений с PostgreSQL, при превышении которого новые соединения отклоняются. Установка максимального числа соединений, в то же время, увеличивает потребление ресурсов и снижает производительность системы. pgpool-II также имеет ограничение на максимальное число соединений, но «лишние» соединения будут поставлены в очередь вместо немедленного возврата ошибки.</p></li> <li><p><strong>Параллельные запросы</strong></p> <p>Используя функцию параллельных запросов можно разнести данные на множество серверов, благодаря чему запрос может быть выполнен на всех серверах одновременно для уменьшения общего времени выполнения. Параллельные запросы работают лучше всего при поиске в больших объемах данных.</p></li> </ul> <p>Pgpool-II общается по протоколу бэкенда и фронтенда PostgreSQL и располагается между ними. Таким образом, приложение базы данных считает что pgpool-II — настоящий сервер PostgreSQL, а сервер видит pgpool-II как одного из своих клиентов. Поскольку pgpool-II прозрачен как для сервера, так и для клиента, существующие приложения, работающие с базой данных, могут использоваться с pgpool-II практически без изменений в исходном коде.</p> <h2 id="установка-и-настройка-1">Установка и настройка</h2> <p>Во многих Linux системах pgpool-II может находиться в репозитории пакетов. Для Ubuntu Linux, например, достаточно будет выполнить:</p> <pre><code>$ sudo aptitude install pgpool2</code></pre> <h3 id="настройка-5">Настройка</h3> <p>Параметры конфигурации pgpool-II хранятся в файле <code>pgpool.conf</code>. Формат файла: одна пара <code>параметр = значение</code> в строке. При установке pgpool-II автоматически создается файл <code>pgpool.conf.sample</code>:</p> <pre><code>$ cp /usr/local/etc/pgpool.conf.sample /usr/local/etc/pgpool.conf</code></pre> <p>Pgpool-II принимает соединения только с <code>localhost</code> на порт 9999. Если требуется принимать соединения с других хостов, установите для параметра <code>listen_addresses</code> значение «*».</p> <pre><code>listen_addresses = &#39;localhost&#39;
port = 9999</code></pre> <h3 id="настройка-команд-pcp">Настройка команд PCP</h3> <p>У pgpool-II есть PCP интерфейс для административных целей (получить информацию об узлах базы данных, остановить pgpool-II, прочее). Чтобы использовать команды PCP, необходима идентификация пользователя. Эта идентификация отличается от идентификации пользователей в PostgreSQL. Имя пользователя и пароль нужно указывать в файле <code>pcp.conf</code>. В этом файле имя пользователя и пароль указываются как пара значений, разделенных двоеточием (:). Одна пара в строке, пароли зашифрованы в формате хэша md5:</p> <pre><code>postgres:e8a48653851e28c69d0506508fb27fc5</code></pre> <p>Для того чтобы зашифровать пароль в формате md5 хэша используется команда <code>pg_md5</code>, которая устанавливается как один из исполняемых файлов pgpool-II. <code>pg_md5</code> принимает текст в параметре командной строки и отображает md5 хэш как результат.</p> <pre><code>$ /usr/bin/pg_md5 postgres
e8a48653851e28c69d0506508fb27fc5</code></pre> <p>Команды PCP выполняются по сети, так что в файле <code>pgpool.conf</code> должен быть указан номер порта в параметре <code>pcp_port</code>:</p> <pre><code>pcp_port = 9898</code></pre> <h3 id="подготовка-узлов-баз-данных">Подготовка узлов баз данных</h3> <p>Далее требуется настроить серверы бэкендов PostgreSQL для pgpool-II. Эти серверы могут быть размещены на одном хосте с pgpool-II или на отдельных машинах. Если вы решите разместить серверы на том же хосте, для всех серверов должны быть установлены разные номера портов. Если серверы размещены на отдельных машинах, они должны быть настроены так чтобы могли принимать сетевые соединения от pgpool-II. В данном примере три сервера PostgreSQL размещено в рамках одного хоста вместе с pgpool-II (5432, 5433, 5434 порты соответственно):</p> <pre><code>backend_hostname0 = &#39;localhost&#39;
backend_port0 = 5432
backend_weight0 = 1
backend_hostname1 = &#39;localhost&#39;
backend_port1 = 5433
backend_weight1 = 1
backend_hostname2 = &#39;localhost&#39;
backend_port2 = 5434
backend_weight2 = 1</code></pre> <p>В параметрах <code>backend_hostname</code>, <code>backend_port</code>, <code>backend_weight</code> указывается имя хоста узла базы данных, номер порта и коэффициент для балансировки нагрузки. В конце имени каждого параметра должен быть указан идентификатор узла путем добавления положительного целого числа начиная с 0. Параметры <code>backend_weight</code> все равны 1, что означает что запросы <code>SELECT</code> равномерно распределены по трем серверам.</p> <h2 id="настройка-репликации-1">Настройка репликации</h2> <p>Pgpool-II репликация включает копирование одних и тех же данных на множество узлов базы данных (синхронная репликация). Но данная репликация имеет тот недостаток, что она создаёт дополнительную нагрузку при выполнении всех транзакций, в которых обновляются какие-либо реплики (кроме того, могут возникать проблемы, связанные с доступностью данных).</p> <h3 id="настройка-репликации-2">Настройка репликации</h3> <p>Чтобы включить функцию репликации базы данных установите значение <code>true</code> для параметра <code>replication_mode</code> в файле <code>pgpool.conf</code>.</p> <pre><code>replication_mode = true</code></pre> <p>Если параметр <code>replication_mode</code> равен <code>true</code>, pgpool-II будет отправлять копию принятого запроса на все узлы базы данных.</p> <p>Если параметр <code>load_balance_mode</code> равен <code>true</code>, pgpool-II будет распределять запросы <code>SELECT</code> между узлами базы данных.</p> <pre><code>load_balance_mode = true</code></pre> <h3 id="проверка-репликации">Проверка репликации</h3> <p>После настройки <code>pgpool.conf</code> и перезапуска pgpool-II, можно проверить репликацию в действии. Для этого создадим базу данных, которую будем реплицировать (базу данных нужно создать на всех узлах):</p> <pre><code>$ createdb -p 9999 bench_replication</code></pre> <p>Затем запустим <code>pgbench</code> с параметром <code>-i</code>. Параметр <code>-i</code> инициализирует базу данных предопределенными таблицами и данными в них.</p> <pre><code>$ pgbench -i -p 9999 bench_replication</code></pre> <p>Указанная ниже таблица содержит сводную информацию о таблицах и данных, которые будут созданы при помощи <code>pgbench -i</code>. Если на всех узлах базы данных перечисленные таблицы и данные были созданы, репликация работает корректно.</p> <table> <thead> <tr class=header> <th style="text-align: center;">Имя таблицы</th> <th style="text-align: center;">Число строк</th> </tr> </thead> <tbody> <tr class=odd> <td style="text-align: center;">branches</td> <td style="text-align: center;">1</td> </tr> <tr class=even> <td style="text-align: center;">tellers</td> <td style="text-align: center;">10</td> </tr> <tr class=odd> <td style="text-align: center;">accounts</td> <td style="text-align: center;">100000</td> </tr> <tr class=even> <td style="text-align: center;">history</td> <td style="text-align: center;">0</td> </tr> </tbody> </table> <p>Для проверки указанной выше информации на всех узлах используем простой скрипт на shell:</p> <pre><code>for port in 5432 5433 5434; do
&gt;     echo $port
&gt;     for table_name in branches tellers accounts history; do
&gt;         echo $table_name
&gt;         psql -c &quot;SELECT count(*) FROM $table_name&quot; -p \
&gt;         $port bench_replication
&gt;     done
&gt; done</code></pre> <h2 id="параллельное-выполнение-запросов">Параллельное выполнение запросов</h2> <p>Pgpool-II позволяет использовать распределение для таблиц. Данные из разных диапазонов сохраняются на двух или более узлах базы данных параллельным запросом. Более того, одни и те же данные на двух и более узлах базы данных могут быть воспроизведены с использованием распределения.</p> <p>Чтобы включить параллельные запросы в pgpool-II вы должны установить еще одну базу данных, называемую «системной базой данных» («System Database») (далее будет называться SystemDB). SystemDB хранит определяемые пользователем правила, определяющие какие данные будут сохраняться на каких узлах базы данных. Также SystemDB используется чтобы объединить результаты возвращенные узлами базы данных посредством dblink.</p> <h3 id="настройка-6">Настройка</h3> <p>Чтобы включить функцию выполнения параллельных запросов требуется установить для параметра <code>parallel_mode</code> значение <code>true</code> в файле <code>pgpool.conf</code>:</p> <pre><code>parallel_mode = true</code></pre> <p>Установка параметра <code>parallel_mode</code> равным <code>true</code> не запустит параллельные запросы автоматически. Для этого pgpool-II нужна SystemDB и правила определяющие как распределять данные по узлам базы данных. Также SystemDB использует dblink для создания соединений с pgpool-II. Таким образом, нужно установить значение параметра <code>listen_addresses</code> таким образом чтобы pgpool-II принимал эти соединения:</p> <pre><code>listen_addresses = &#39;*&#39;</code></pre> <p>Нужно обратить внимание, что репликация не реализована для таблиц, которые распределяются посредством параллельных запросов. Поэтому:</p> <pre><code>replication_mode = true
load_balance_mode = false</code></pre> <p>или</p> <pre><code>replication_mode = false
load_balance_mode = true</code></pre> <h3 id="настройка-systemdb">Настройка SystemDB</h3> <p>В основном, нет отличий между простой и системной базами данных. Однако, в системной базе данных определяется функция dblink и присутствует таблица, в которой хранятся правила распределения данных. Таблицу <code>dist_def</code> необходимо определять. Более того, один из узлов базы данных может хранить системную базу данных, а pgpool-II может использоваться для распределения нагрузки каскадным подключеним.</p> <p>Создадим SystemDB на узле с портом 5432. Далее приведен список параметров конфигурации для SystemDB:</p> <pre><code>system_db_hostname = &#39;localhost&#39;
system_db_port = 5432
system_db_dbname = &#39;pgpool&#39;
system_db_schema = &#39;pgpool_catalog&#39;
system_db_user = &#39;pgpool&#39;
system_db_password = &#39;&#39;</code></pre> <p>На самом деле, указанные выше параметры являются параметрами по умолчанию в файле <code>pgpool.conf</code>. Теперь требуется создать пользователя с именем «pgpool» и базу данных с именем «pgpool» и владельцем «pgpool»:</p> <pre><code>$ createuser -p 5432 pgpool
$ createdb -p 5432 -O pgpool pgpool</code></pre> <h4 id="установка-dblink">Установка dblink</h4> <p>Далее требуется установить dblink в базу данных «pgpool». Dblink — один из инструментов включенных в каталог contrib исходного кода PostgreSQL. После того как dblink был установлен в вашей системе мы добавим функции dblink в базу данных «pgpool».</p> <pre><code>$ psql -c &quot;CREATE EXTENSION dblink;&quot; -p 5432 pgpool</code></pre> <h4 id="создание-таблицы-dist_def">Создание таблицы dist_def</h4> <p>Следующим шагом мы создадим таблицу с именем <code>dist_def</code>, в которой будут храниться правила распределения данных. Поскольку pgpool-II уже был установлен, файл с именем <code>system_db.sql</code> должен быть установлен в <code>/usr/local/share/system_db.sql</code> (имейте в виду, что на вашей системе каталог установки может быть другой). Файл <code>system_db.sql</code> содержит директивы для создания специальных таблиц, включая и таблицу <code>dist_def</code>. Выполним следующую команду для создания таблицы <code>dist_def</code>:</p> <pre><code>$ psql -f /usr/local/share/system_db.sql -p 5432 -U pgpool pgpool</code></pre> <p>Все таблицы в файле <code>system_db.sql</code>, включая <code>dist_def</code>, создаются в схеме <code>pgpool_catalog</code>. Если вы установили параметр <code>system_db_schema</code> на использование другой схемы, вам нужно, соответственно, отредактировать файл <code>system_db.sql</code>. Описание таблицы <code>dist_def</code> выглядит так как показано ниже:</p> <pre><code>CREATE TABLE pgpool_catalog.dist_def (
    dbname text, -- имя базы данных
    schema_name text, -- имя схемы
    table_name text, -- имя таблицы
    col_name text NOT NULL CHECK (col_name = ANY (col_list)),
    -- столбец ключ для распределения данных
    col_list text[] NOT NULL, -- список имен столбцов
    type_list text[] NOT NULL, -- список типов столбцов
    dist_def_func text NOT NULL,
    -- имя функции распределения данных
    PRIMARY KEY (dbname, schema_name, table_name)
);</code></pre> <p>Записи, хранимые в таблице <code>dist_def</code>, могут быть двух типов:</p> <ul> <li><p>Правило распределения данных (<code>col_name</code>, <code>dist_def_func</code>);</p></li> <li><p>Мета-информация о таблицах (<code>dbname</code>, <code>schema_name</code>, <code>table_name</code>, <code>col_list</code>, <code>type_list</code>);</p></li> </ul> <p>Правило распределения данных определяет как будут распределены данные на конкретный узел базы данных. Данные будут распределены в зависимости от значения столбца <code>col_name</code>. <code>dist_def_func</code> — это функция, которая принимает значение <code>col_name</code> в качестве агрумента и возвращает целое число, которое соответствует идентификатору узла базы данных, на котором должны быть сохранены данные. Мета-информация используется для того чтобы переписывать запросы. Параллельный запрос должен переписывать исходные запросы так чтобы результаты, возвращаемые узлами-бэкендами, могли быть объединены в единый результат.</p> <h4 id="создание-таблицы-replicate_def">Создание таблицы replicate_def</h4> <p>В случае если указана таблица, для которой производится репликация в выражение SQL, использующее зарегистрированную в <code>dist_def</code> таблицу путем объединения таблиц, информация о таблице, для которой необходимо производить репликацию, предварительно регистрируется в таблице с именем <code>replicate_def</code>. Таблица <code>replicate_def</code> будет создана при обработке файла <code>system_db.sql</code>. Таблица <code>replicate_def</code> описана так как показано ниже:</p> <pre><code>CREATE TABLE pgpool_catalog.replicate_def (
    dbname text, -- имя базы данных
    schema_name text, -- имя схемы
    table_name text, -- имя таблицы
    col_list text[] NOT NULL, -- список имен столбцов
    type_list text[] NOT NULL, -- список типов столбцов
    PRIMARY KEY (dbname, schema_name, table_name)
);</code></pre> <h3 id="установка-правил-распределения-данных">Установка правил распределения данных</h3> <p>В данном примере будут определены правила распределения данных, созданных программой pgbench, на три узла базы данных. Тестовые данные будут созданы командой <code>pgbench -i -s 3</code> (т.е. масштабный коэффициент равен 3). Для этого раздела мы создадим новую базу данных с именем <code>bench_parallel</code>. В каталоге sample исходного кода pgpool-II вы можете найти файл <code>dist_def_pgbench.sql</code>. Будем использоваться этот файл с примером для создания правил распределения для pgbench. Выполним следующую команду в каталоге с распакованным исходным кодом pgpool-II:</p> <pre><code>$ psql -f sample/dist_def_pgbench.sql -p 5432 pgpool</code></pre> <p>В файле <code>dist_def_pgbench.sql</code> мы добавляем одну строку в таблицу <code>dist_def</code>. Это функция распределения данных для таблицы <code>accounts</code>. В качестве столбца-ключа указан столбец <code>aid</code>.</p> <pre><code>INSERT INTO pgpool_catalog.dist_def VALUES (
    &#39;bench_parallel&#39;,
    &#39;public&#39;,
    &#39;accounts&#39;,
    &#39;aid&#39;,
    ARRAY[&#39;aid&#39;, &#39;bid&#39;, &#39;abalance&#39;, &#39;filler&#39;],
    ARRAY[&#39;integer&#39;, &#39;integer&#39;, &#39;integer&#39;,
    &#39;character(84)&#39;],
    &#39;pgpool_catalog.dist_def_accounts&#39;
);</code></pre> <p>Теперь мы должны создать функцию распределения данных для таблицы <code>accounts</code>. Возможно использовать одну и ту же функцию для разных таблиц. Таблица <code>accounts</code> в момент инициализации данных хранит значение масштабного коэффициента равное 3, значения столбца <code>aid</code> от 1 до 300000. Функция создана таким образом что данные равномерно распределяются по трем узлам базы данных:</p> <pre><code>CREATE OR REPLACE FUNCTION
pgpool_catalog.dist_def_branches(anyelement)
RETURNS integer AS $$
    SELECT CASE WHEN $1 &gt; 0 AND $1 &lt;= 1 THEN 0
        WHEN $1 &gt; 1 AND $1 &lt;= 2 THEN 1
        ELSE 2
    END;
$$ LANGUAGE sql;</code></pre> <h3 id="установка-правил-репликации">Установка правил репликации</h3> <p>Правило репликации — это то что определяет какие таблицы должны быть использованы для выполнения репликации. Здесь это сделано при помощи pgbench с зарегистрированными таблицами <code>branches</code> и <code>tellers</code>. Как результат, стало возможно создание таблицы <code>accounts</code> и выполнение запросов, использующих таблицы <code>branches</code> и <code>tellers</code>:</p> <pre><code>INSERT INTO pgpool_catalog.replicate_def VALUES (
    &#39;bench_parallel&#39;,
    &#39;public&#39;,
    &#39;branches&#39;,
    ARRAY[&#39;bid&#39;, &#39;bbalance&#39;, &#39;filler&#39;],
    ARRAY[&#39;integer&#39;, &#39;integer&#39;, &#39;character(88)&#39;]
);

INSERT INTO pgpool_catalog.replicate_def VALUES (
    &#39;bench_parallel&#39;,
    &#39;public&#39;,
    &#39;tellers&#39;,
    ARRAY[&#39;tid&#39;, &#39;bid&#39;, &#39;tbalance&#39;, &#39;filler&#39;],
    ARRAY[&#39;integer&#39;, &#39;integer&#39;, &#39;integer&#39;, &#39;character(84)&#39;]
);</code></pre> <p>Подготовленный файл <code>replicate_def_pgbench.sql</code> находится в каталоге sample:</p> <pre><code>$ psql -f sample/replicate_def_pgbench.sql -p 5432 pgpool</code></pre> <h3 id="проверка-параллельного-запроса">Проверка параллельного запроса</h3> <p>После настройки <code>pgpool.conf</code> и перезапуска pgpool-II возможно провести проверку работоспособности параллельных запросов. Сначала требуется создать базу данных, которая будет распределена. Эту базу данных нужно создать на всех узлах:</p> <pre><code>$ createdb -p 9999 bench_parallel</code></pre> <p>Затем запустим pgbench с параметрами <code>-i -s 3</code>:</p> <pre><code>$ pgbench -i -s 3 -p 9999 bench_parallel</code></pre> <p>Один из способов проверить корректно ли были распределены данные — выполнить запрос <code>SELECT</code> посредством pgpool-II и напрямую на бэкендах и сравнить результаты. Если все настроено правильно база данных <code>bench_parallel</code> должна быть распределена как показано ниже:</p> <table> <thead> <tr class=header> <th style="text-align: center;">Имя таблицы</th> <th style="text-align: center;">Число строк</th> </tr> </thead> <tbody> <tr class=odd> <td style="text-align: center;">branches</td> <td style="text-align: center;">3</td> </tr> <tr class=even> <td style="text-align: center;">tellers</td> <td style="text-align: center;">30</td> </tr> <tr class=odd> <td style="text-align: center;">accounts</td> <td style="text-align: center;">300000</td> </tr> <tr class=even> <td style="text-align: center;">history</td> <td style="text-align: center;">0</td> </tr> </tbody> </table> <p>Для проверки указанной выше информации на всех узлах и посредством pgpool-II используем простой скрипт на shell. Приведенный ниже скрипт покажет минимальное и максимальное значение в таблице accounts используя для соединения порты 5432, 5433, 5434 и 9999.</p> <pre><code>for port in 5432 5433 5434i 9999; do
&gt;     echo $port
&gt;     psql -c &quot;SELECT min(aid), max(aid) FROM accounts&quot; \
&gt;     -p $port bench_parallel
&gt; done</code></pre> <h2 id="master-slave-режим">Master-slave режим</h2> <p>Этот режим предназначен для использования pgpool-II с другой репликацией (например streaming, londiste). Информация про БД указывается как для репликации. <code>master_slave_mode</code> и <code>load_balance_mode</code> устанавливается в <code>true</code>. pgpool-II будет посылать запросы <code>INSERT/UPDATE/DELETE</code> на master базу данных (1 в списке), а <code>SELECT</code> — использовать балансировку нагрузки, если это возможно. При этом, DDL и DML для временной таблицы может быть выполнен только на мастере. Если нужен <code>SELECT</code> только на мастере, то для этого нужно использовать комментарий <code>/*NO LOAD BALANCE*/</code> перед <code>SELECT</code>.</p> <p>В Master/Slave режиме <code>replication_mode</code> должен быть установлен <code>false</code>, а <code>master_slave_mode</code> — <code>true</code>.</p> <h3 id="streaming-replication-потоковая-репликация">Streaming Replication (Потоковая репликация)</h3> <p>В master-slave режиме с потоковой репликацией, если мастер или слейв упал, возможно использовать отказоустоичивый функционал внутри pgpool-II. Автоматически отключив упавший инстанс PostgreSQL, pgpool-II переключится на следующий слейв как на новый мастер (при падении мастера), или останется работать на мастере (при падении слейва). В потоковой репликации, когда слейв становится мастером, требуется создать триггер файл (который указан в <code>recovery.conf</code>, параметр <code>trigger_file</code>), чтобы PostgreSQL перешел из режима восстановления в нормальный. Для этого можно создать небольшой скрипт:</p> <pre><code>#! /bin/sh
# Failover command for streming replication.
# This script assumes that DB node 0 is primary, and 1 is standby.
#
# If standby goes down, does nothing. If primary goes down, create a
# trigger file so that standby take over primary node.
#
# Arguments: $1: failed node id. $2: new master hostname. $3: path to
# trigger file.

failed_node=$1
new_master=$2
trigger_file=$3

# Do nothing if standby goes down.
if [ $failed_node = 1 ]; then
	exit 0;
fi

# Create trigger file.
/usr/bin/ssh -T $new_master /bin/touch $trigger_file

exit 0;</code></pre> <p>Работает скрипт просто: если падает слейв — скрипт ничего не выполняет, при падении мастера — создает триггер файл на новом мастере. Сохраним этот файл под именем <code>failover\_stream.sh</code> и добавим в <code>pgpool.conf</code>:</p> <pre><code>failover_command = &#39;/path_to_script/failover_stream.sh %d %H /tmp/trigger_file&#39;</code></pre> <p>где <code>/tmp/trigger_file</code> — триггер файл, указаный в конфиге <code>recovery.conf</code>. Теперь, если мастер СУБД упадет, слейв будет переключен из режима восстановления в обычный и сможет принимать запросы на запись.</p> <h2 id="онлайн-восстановление">Онлайн восстановление</h2> <p>Pgpool-II в режиме репликации может синхронизировать базы данных и добавлять их как новые ноды. Называется это «онлайн восстановление». Этот метод также может быть использован когда нужно вернуть в репликацию упавший нод базы данных. Вся процедура выполняется в два задания. Несколько секунд или минут клиент может ждать подключения к pgpool, в то время как восстанавливается узел базы данных. Онлайн восстановление состоит из следующих шагов:</p> <ul> <li><p>CHECKPOINT;</p></li> <li><p>Первый этап восстановления;</p></li> <li><p>Ждем, пока все клиенты не отключатся;</p></li> <li><p>CHECKPOINT;</p></li> <li><p>Второй этап восстановления;</p></li> <li><p>Запуск postmaster (выполнить <code>pgpool_remote_start</code>);</p></li> <li><p>Восстанавливаем инстанс СУБД;</p></li> </ul> <p>Для работы онлайн восстановления потребуется указать следующие параметры:</p> <ul> <li><p><code>backend_data_directory</code> - каталог данных определенного PostgreSQL кластера;</p></li> <li><p><code>recovery_user</code> - имя пользователя PostgreSQL;</p></li> <li><p><code>recovery_password</code> - пароль пользователя PostgreSQL;</p></li> <li><p><code>recovery_1st_stage_command</code> - параметр указывает команду для первого этапа онлайн восстановления. Файл с командами должен быть помещен в каталог данных СУБД кластера из соображений безопасности. Например, если <code>recovery_1st_stage_command = 'some_script'</code>, то pgpool-II выполнит <code>$PGDATA/some_script</code>. Обратите внимание, что pgpool-II принимает подключения и запросы в то время как выполняется <code>recovery_1st_stage</code>;</p></li> <li><p><code>recovery_2nd_stage_command</code> - параметр указывает команду для второго этапа онлайн восстановления. Файл с командами должен быть помещен в каталог данных СУБД кластера из-за проблем безопасности. Например, если <code>recovery_2st_stage_command = 'some_script'</code>, то pgpool-II выполнит <code>$PGDATA/some_script</code>. Обратите внимание, что pgpool-II НЕ принимает подключения и запросы в то время как выполняется <code>recovery_2st_stage</code>. Таким образом, pgpool-II будет ждать пока все клиенты не закроют подключения;</p></li> </ul> <h3 id="streaming-replication-потоковая-репликация-1">Streaming Replication (Потоковая репликация)</h3> <p>В master-slave режиме с потоковой репликацией, онлайн восстановление — отличное средство вернуть назад упавший инстанс PostgreSQL. Вернуть возможно только слейв ноды, таким методом не восстановить упавший мастер. Для восстановления мастера потребуется остановить все PostgreSQL инстансы и pgpool-II (для восстановления из резервной копии мастера).</p> <p>Для настройки онлайн восстановления потребуется:</p> <ul> <li><p>Установить <code>recovery_user</code>. Обычно это «postgres»;</p></li> <li><p>Установить <code>recovery_password</code> для <code>recovery_user</code> для подключения к СУБД;</p></li> <li><p>Настроить <code>recovery_1st_stage_command</code>. Для этого можно создать скрипт <code>basebackup.sh</code> и положим его в папку с данными мастера (<code>$PGDATA</code>), установив ему права на выполнение:</p> <pre><code>#! /bin/sh
# Recovery script for streaming replication.
# This script assumes that DB node 0 is primary, and 1 is standby.
#
datadir=$1
desthost=$2
destdir=$3

psql -c &quot;SELECT pg_start_backup(&#39;Streaming Replication&#39;, true)&quot; postgres

rsync -C -a --delete -e ssh --exclude postgresql.conf --exclude postmaster.pid \
--exclude postmaster.opts --exclude pg_log --exclude pg_xlog \
--exclude recovery.conf $datadir/ $desthost:$destdir/

ssh -T localhost mv $destdir/recovery.done $destdir/recovery.conf

psql -c &quot;SELECT pg_stop_backup()&quot; postgres</code></pre> <p>При восстановления слейва, скрипт запускает бэкап мастера и через rsync утилиту передает данные с мастера на слейв. Для этого необходимо настроить ssh так, чтобы <code>recovery_user</code> мог заходить с мастера на слейв без пароля. Далее добавим скрипт на выполнение для первого этапа онлайн востановления:</p> <pre><code>recovery_1st_stage_command = &#39;basebackup.sh&#39;</code></pre></li> <li><p>Оставляем <code>recovery_2nd_stage_command</code> пустым. После успешного выполнения первого этапа онлайн восстановления, разницу в данных, что успели записатся во время работы скрипта <code>basebackup.sh</code>, слейв инстанс заберет через WAL файлы с мастера;</p></li> <li><p>Устанавливаем C и SQL функции для работы онлайн востановления на каждый инстанс СУБД:</p> <pre><code>$ cd pgpool-II-x.x.x/sql/pgpool-recovery
$ make
$ make install
$ psql -f pgpool-recovery.sql template1</code></pre></li> </ul> <p>После этого возможно использовать <code>pcp_recovery_node</code> для онлайн восстановления упавших слейвов.</p> <h2 id="заключение-9">Заключение</h2> <p>PgPool-II — прекрасное средство, функционал которого может помочь администраторам баз данных при масштабировании PostgreSQL.</p> <h1 id="мультиплексоры-соединений">Мультиплексоры соединений</h1> <h2 id="введение-6">Введение</h2> <p>Мультиплексоры соединений (программы для создания пула соединений) позволяют уменьшить накладные расходы на базу данных, в случае, когда огромное количество физических соединений ведет к падению производительности PostgreSQL. Это особенно важно на Windows, когда система ограничивает большое количество соединений. Это также важно для веб-приложений, где количество соединений может быть очень большим.</p> <p>Для PostgreSQL существует PgBouncer и Pgpool-II, которые работают как мультиплексоры соединений.</p> <h2 id=pgbouncer>PgBouncer</h2> <p>Это мультиплексор соединений для PostgreSQL от компании Skype. Существуют три режима управления:</p> <ul> <li><p>Session Pooling — наиболее «вежливый» режим. При начале сессии клиенту выделяется соединение с сервером; оно приписано ему в течение всей сессии и возвращается в пул только после отсоединения клиента;</p></li> <li><p>Transaction Pooling — клиент владеет соединением с бакендом только в течение транзакции. Когда PgBouncer замечает, что транзакция завершилась, он возвращает соединение назад в пул;</p></li> <li><p>Statement Pooling — наиболее агрессивный режим. Соединение с бакендом возвращается назад в пул сразу после завершения запроса. Транзакции с несколькими запросами в этом режиме не разрешены, так как они гарантировано будут отменены. Также не работают подготовленные выражения (prepared statements) в этом режиме;</p></li> </ul> <p>К достоинствам PgBouncer относится:</p> <ul> <li><p>малое потребление памяти (менее 2 КБ на соединение);</p></li> <li><p>отсутствие привязки к одному серверу баз данных;</p></li> <li><p>реконфигурация настроек без рестарта.</p></li> </ul> <p>Базовая утилита запускается просто:</p> <pre><code>$ pgbouncer [-d][-R][-v][-u user] &lt;pgbouncer.ini&gt;</code></pre> <p>Пример конфига:</p> <pre><code>template1 = host=127.0.0.1 port=5432 dbname=template1
[pgbouncer]
listen_port = 6543
listen_addr = 127.0.0.1
auth_type = md5
auth_file = userlist.txt
logfile = pgbouncer.log
pidfile = pgbouncer.pid
admin_users = someuser</code></pre> <p>Нужно создать файл пользователей <code>userlist.txt</code> примерно такого содержания: <code>&quot;someuser&quot; &quot;same_password_as_in_server&quot;</code>. Административный доступ из консоли к базе данных pgbouncer можно получить через команду ниже:</p> <pre><code>$ psql -h 127.0.0.1 -p 6543 pgbouncer</code></pre> <p>Здесь можно получить различную статистическую информацию с помощью команды <code>SHOW</code>.</p> <h2 id=pgpool-ii-vs-pgbouncer>PgPool-II vs PgBouncer</h2> <p>Если сравнивать PgPool-II и PgBouncer, то PgBouncer намного лучше работает с пулами соединений, чем PgPool-II. Если вам не нужны остальные возможности, которыми владеет PgPool-II (ведь пулы коннектов это мелочи к его функционалу), то конечно лучше использовать PgBouncer.</p> <ul> <li><p>PgBouncer потребляет меньше памяти, чем PgPool-II;</p></li> <li><p>у PgBouncer возможно настроить очередь соединений;</p></li> <li><p>в PgBouncer можно настраивать псевдо базы данных (на сервере они могут называться по-другому);</p></li> </ul> <p>Также возможен вариант использования PgBouncer и PgPool-II совместно.</p> <h1 id="кэширование-в-postgresql">Кэширование в PostgreSQL</h1> <h2 id="введение-7">Введение</h2> <p>Кэш или кеш — промежуточный буфер с быстрым доступом, содержащий информацию, которая может быть запрошена с наибольшей вероятностью. Кэширование <code>SELECT</code> запросов позволяет повысить производительность приложений и снизить нагрузку на PostgreSQL. Преимущества кэширования особенно заметны в случае с относительно маленькими таблицами, имеющими статические данные, например, справочными таблицами.</p> <p>Многие СУБД могут кэшировать SQL запросы, и данная возможность идет у них, в основном, «из коробки». PostgreSQL не обладает подобным функционалом. Почему? Во-первых, мы теряем транзакционную чистоту происходящего в базе. Что это значит? Управление конкурентным доступом с помощью многоверсионности (MVCC — MultiVersion Concurrency Control) — один из механизмов обеспечения одновременного конкурентного доступа к БД, заключающийся в предоставлении каждому пользователю «снимка» БД, обладающего тем свойством, что вносимые данным пользователем изменения в БД невидимы другим пользователям до момента фиксации транзакции. Этот способ управления позволяет добиться того, что пишущие транзакции не блокируют читающих, и читающие транзакции не блокируют пишущих. При использовании кэширования, которому нет дела к транзакциям СУБД, «снимки» БД могут быть с неверными данными. Во-вторых, кеширование результатов запросов, в основном, должно происходить на стороне приложения, а не СУБД. В таком случае управление кэшированием может работать более гибко (включатьcя и отключаться где потребуется для приложения), а СУБД будет заниматься своей непосредственной целью — хранением и обеспечение целостности данных.</p> <p>Для организации кэширования существует два инструмента для PostgreSQL:</p> <ul> <li><p>Pgmemcache (с memcached);</p></li> <li><p>Pgpool-II (query cache);</p></li> </ul> <h2 id="sec:pgmemcache">Pgmemcache</h2> <p><a href="http://memcached.org/">Memcached</a> — программное обеспечение, реализующее сервис кэширования данных в оперативной памяти на основе хеш-таблицы. С помощью клиентской библиотеки позволяет кэшировать данные в оперативной памяти множества доступных серверов. Распределение реализуется путём сегментирования данных по значению хэша ключа по аналогии с сокетами хэш-таблицы. Клиентская библиотека, используя ключ данных, вычисляет хэш и использует его для выбора соответствующего сервера. Ситуация сбоя сервера трактуется как промах кэша, что позволяет повышать отказоустойчивость комплекса за счет наращивания количества memcached серверов и возможности производить их горячую замену.</p> <p><a href="http://pgfoundry.org/projects/pgmemcache/">Pgmemcache</a> — это PostgreSQL API библиотека на основе libmemcached для взаимодействия с memcached. С помощью данной библиотеки PostgreSQL может записывать, считывать, искать и удалять данные из memcached.</p> <h3 id="установка-7">Установка</h3> <p>Поскольку Pgmemcache идет как модуль, то потребуется PostgreSQL с PGXS (если уже не установлен, поскольку в сборках для Linux присутствует PGXS). Также потребуется memcached и libmemcached библиотека версии не ниже 0.38. После скачивания и распаковки исходников достаточно выполнить в консоли:</p> <pre><code>$ make
$ sudo make install</code></pre> <h3 id="настройка-7">Настройка</h3> <p>После успешной установки Pgmemcache потребуется добавить во все базы данных (на которых вы хотите использовать Pgmemcache) функции для работы с этой библиотекой:</p> <pre><code>% psql [mydbname] [pguser]
[mydbname]=# CREATE EXTENSION pgmemcache;</code></pre> <p>Теперь можно добавлять сервера memcached через <code>memcache_server_add</code> и работать с кэшем. Но есть одно но. Все сервера memcached придется задавать при каждом новом подключении к PostgreSQL. Это ограничение можно обойти, если настроить параметры в <code>postgresql.conf</code> файле:</p> <ul> <li><p>Добавить <code>pgmemcache</code> в <code>shared_preload_libraries</code> (автозагрузка библиотеки pgmemcache во время старта PostgreSQL);</p></li> <li><p>Добавить <code>pgmemcache</code> в <code>custom_variable_classes</code> (устанавливаем переменную для pgmemcache);</p></li> <li><p>Создаем <code>pgmemcache.default_servers</code>, указав в формате «host:port» (port - опционально) через запятую. Например:</p> <pre><code>pgmemcache.default_servers = &#39;127.0.0.1, 192.168.0.20:11211&#39; # подключили два сервера memcached</code></pre></li> <li><p>Также можем настроить работу самой библиотеки pgmemcache через <code>pgmemcache.default_behavior</code>. Настройки соответствуют настройкам libmemcached. Например:</p> <pre><code>pgmemcache.default_behavior=&#39;BINARY_PROTOCOL:1&#39;</code></pre></li> </ul> <p>Теперь не требуется при подключении к PostgreSQL указывать сервера memcached.</p> <h3 id="проверка-1">Проверка</h3> <p>После успешной установки и настройки pgmemcache становится доступен список команд для работы с memcached серверами.</p> <p><span>| &gt;p<span>7cm</span>| &gt;p<span>7cm</span> |</span> Команда &amp; Описание<br/> memcache_server_add(’hostname:port’::TEXT)</p> <p>memcache_server_add(’hostname’::TEXT) &amp; Добавляет memcached сервер в список доступных серверов. Если порт не указан, по умолчанию используется 11211.<br/> </p> <p>memcache_add(key::TEXT, value::TEXT, expire::TIMESTAMPTZ)</p> <p>memcache_add(key::TEXT, value::TEXT, expire::INTERVAL)</p> <p>memcache_add(key::TEXT, value::TEXT) &amp; Добавляет ключ в кэш, если ключ не существует.<br/> </p> <p>newval = memcache_decr(key::TEXT, decrement::INT4)</p> <p>newval = memcache_decr(key::TEXT) &amp; Если ключ существует и является целым числом, происходит уменьшение его значения на указаное число (по умолчанию на единицу). Возвращает целое число после уменьшения.<br/> </p> <p>memcache_delete(key::TEXT, hold_timer::INTERVAL)</p> <p>memcache_delete(key::TEXT)</p> <p>&amp; Удаляет указанный ключ. Если указать таймер, то ключ с таким же названием может быть добавлен только после окончания таймера.<br/> </p> <p>memcache_flush_all()</p> <p>&amp; Очищает все данные на всех memcached серверах.<br/> </p> <p>value = memcache_get(key::TEXT)</p> <p>&amp; Выбирает ключ из кэша. Возвращает NULL, если ключ не существует, иначе — текстовую строку.<br/> </p> <p>memcache_get_multi(keys::TEXT[])</p> <p>memcache_get_multi(keys::BYTEA[])</p> <p>&amp; Получает массив ключей из кэша. Возвращает список найденных записей в виде «ключ=значение».<br/> </p> <p>newval = memcache_incr(key::TEXT, increment::INT4)</p> <p>newval = memcache_incr(key::TEXT)</p> <p>&amp; Если ключ существует и является целым числом, происходит увеличение его значения на указаное число (по умолчанию на единицу). Возвращает целое число после увеличения.<br/> </p> <p>memcache_replace(key::TEXT, value::TEXT, expire::TIMESTAMPTZ)</p> <p>memcache_replace(key::TEXT, value::TEXT, expire::INTERVAL)</p> <p>memcache_replace(key::TEXT, value::TEXT)</p> <p>&amp; Заменяет значение для существующего ключа.<br/> </p> <p>memcache_set(key::TEXT, value::TEXT, expire::TIMESTAMPTZ)</p> <p>memcache_set(key::TEXT, value::TEXT, expire::INTERVAL)</p> <p>memcache_set(key::TEXT, value::TEXT)</p> <p>&amp; Создает ключ со значением. Если такой ключ существует — заменяет в нем значение на указаное.<br/> </p> <p>stats = memcache_stats()</p> <p>&amp; Возвращает статистику по всем серверам memcached.<br/> </p> <p>Посмотрим работу в СУБД данных функций. Для начала получим информацию о memcached серверах:</p> <pre><code>pgmemcache=# SELECT memcache_stats();
      memcache_stats
---------------------------

 Server: 127.0.0.1 (11211)
 pid: 1116
 uptime: 70
 time: 1289598098
 version: 1.4.5
 pointer_size: 32
 rusage_user: 0.0
 rusage_system: 0.24001
 curr_items: 0
 total_items: 0
 bytes: 0
 curr_connections: 5
 total_connections: 7
 connection_structures: 6
 cmd_get: 0
 cmd_set: 0
 get_hits: 0
 get_misses: 0
 evictions: 0
 bytes_read: 20
 bytes_written: 782
 limit_maxbytes: 67108864
 threads: 4

(1 row)</code></pre> <p>Теперь сохраним данные в memcached и попробуем их забрать:</p> <pre><code>pgmemcache=# SELECT memcache_add(&#39;some_key&#39;, &#39;test_value&#39;);
 memcache_add
--------------
 t
(1 row)

pgmemcache=# SELECT memcache_get(&#39;some_key&#39;);
 memcache_get
--------------
 test_value
(1 row)</code></pre> <p>Можно также проверить работу счетчиков в memcached (данный функционал может пригодиться для создания последовательностей):</p> <pre><code>pgmemcache=# SELECT memcache_add(&#39;some_seq&#39;, &#39;10&#39;);
 memcache_add
--------------
 t
(1 row)

pgmemcache=# SELECT memcache_incr(&#39;some_seq&#39;);
 memcache_incr
---------------
            11
(1 row)

pgmemcache=# SELECT memcache_incr(&#39;some_seq&#39;);
 memcache_incr
---------------
            12
(1 row)

pgmemcache=# SELECT memcache_incr(&#39;some_seq&#39;, 10);
 memcache_incr
---------------
            22
(1 row)

pgmemcache=# SELECT memcache_decr(&#39;some_seq&#39;);
 memcache_decr
---------------
            21
(1 row)

pgmemcache=# SELECT memcache_decr(&#39;some_seq&#39;);
 memcache_decr
---------------
            20
(1 row)

pgmemcache=# SELECT memcache_decr(&#39;some_seq&#39;, 6);
 memcache_decr
---------------
            14
(1 row)</code></pre> <p>Для работы с pgmemcache лучше создать функции и, если требуется, активировать эти функции через триггеры.</p> <p>Например, приложение кэширует зашифрованые пароли пользователей в memcached (для более быстрого доступа), и нам требуется обновлять информацию в кэше, если она изменяется в СУБД. Создаем функцию:</p> <pre><code>CREATE OR REPLACE FUNCTION auth_passwd_upd() RETURNS TRIGGER AS $$
	BEGIN
	IF OLD.passwd != NEW.passwd THEN
		PERFORM memcache_set(&#39;user_id_&#39; || NEW.user_id || &#39;_password&#39;, NEW.passwd);
	END IF;
	RETURN NEW;
END;
$$ LANGUAGE &#39;plpgsql&#39;;</code></pre> <p>Активируем триггер для обновления таблицы пользователей:</p> <pre><code>CREATE TRIGGER auth_passwd_upd_trg AFTER UPDATE ON passwd FOR EACH ROW EXECUTE PROCEDURE auth_passwd_upd();</code></pre> <p>Но данный пример транзакционно не безопасен — при отмене транзации кэш не вернется на старое значение. Поэтому лучше очищать старые данные:</p> <pre><code>CREATE OR REPLACE FUNCTION auth_passwd_upd() RETURNS TRIGGER AS $$
BEGIN
	IF OLD.passwd != NEW.passwd THEN
		PERFORM memcache_delete(&#39;user_id_&#39; || NEW.user_id || &#39;_password&#39;);
	END IF;
	RETURN NEW;
END;$$ LANGUAGE &#39;plpgsql&#39;;</code></pre> <p>Также нужен триггер на чистку кэша при удалении записи из СУБД:</p> <pre><code>CREATE TRIGGER auth_passwd_del_trg AFTER DELETE ON passwd FOR EACH ROW EXECUTE PROCEDURE auth_passwd_upd();</code></pre> <p>Данный пример сделан для наглядности, а создавать кэш в memcached на кешированый пароль нового пользователя (или обновленного) лучше через приложение.</p> <h3 id="заключение-10">Заключение</h3> <p>PostgreSQL с помощью Pgmemcache библиотеки позволяет работать с memcached серверами, что может потребоваться в определенных случаях для кэширования данных напрямую с СУБД. Удобство данной библиотеки заключается в полном доступе к функциям memcached, но вот готовой реализации кэширование SQL запросов тут нет, и её придется дорабатывать вручную через функции и триггеры PostgreSQL.</p> <h2 id="заключение-11">Заключение</h2> <p>TODO</p> <h1 id="расширения">Расширения</h1> <h2 id="введение-8">Введение</h2> <p>Один из главных плюсов PostgreSQL это возможность расширения его функционала с помощью расширений. В данной статье я затрону только самые интересные и популярные из существующих расширений.</p> <h2 id=postgis>PostGIS</h2> <p><a href="http://www.postgis.org/">PostGIS</a> добавляет поддержку для географических объектов в PostgreSQL. По сути PostGIS позволяет использовать PostgreSQL в качестве бэкэнда пространственной базы данных для геоинформационных систем (ГИС), так же, как ESRI SDE или пространственного расширения Oracle. PostGIS соответствует OpenGIS «Простые особенности. Спецификация для SQL» и был сертифицирован.</p> <h3 id="установка-и-использование">Установка и использование</h3> <p>Для начала инициализируем расширение в базе данных:</p> <pre><code># CREATE EXTENSION postgis;</code></pre> <p>При создании пространственной базы данных автоматически создаются таблица метаданных <code>spatial_ref_sys</code> и представления <code>geometry_columns</code>, <code>geography_columns</code>, <code>raster_columns</code> и <code>raster_overviews</code>. Они создаются в соответствии со спецификацией «Open Geospatial Consortium Simple Features for SQL specification», выпущенной <a href="http://www.opengeospatial.org/">OGC</a> и описывающей стандартные типы объектов ГИС, функции для манипуляции ими и набор таблиц метаданных. Таблица <code>spatial_ref_sys</code> содержит числовые идентификаторы и текстовые описания систем координат, используемых в пространственной базе данных. Одним из полей этой таблицы является поле <code>SRID</code> — уникальный идентификатор, однозначно определяющий систему координат. <code>SRID</code> представляет из себя числовой код, которому соответствует некоторая система координат. Например, распространенный код EPSG 4326 соответствует географической системе координат WGS84. Более подробную информацию по таблицами метаданных можно найти в руководстве по <a href="http://postgis.net/docs/manual-2.3/using_postgis_dbmanagement.html#spatial_ref_sys">PostGIS</a>.</p> <p>Теперь, имея пространственную базу данных, можно создать несколько пространственных таблиц. Для начала создадим обычную таблицу базы данных, чтобы хранить данные о городе. Эта таблица будет содержвать три поля: числовой идентификатор, название города и колонка геометрии, содержащую данные о местоположении городов:</p> <pre><code># CREATE TABLE cities ( id int4 primary key, name varchar(50), the_geom geometry(POINT,4326) );</code></pre> <p><code>the_geom</code> поле указывает PostGIS, какой тип геометрии имеет каждый из объектов (точки, линии, полигоны и т.п.), какая размерность (т.к. возможны и 3-4 измерения — <code>POINTZ</code>, <code>POINTM</code>, <code>POINTZM</code>) и какая система координат. Для данных по городам мы будем использовать систему координат EPSG:4326. Чтобы добавить данные геометрии в соответствующую колонку, используется функция PostGIS <code>ST_GeomFromText</code>, чтобы сконвертировать координаты и идентификатор референсной системы из текстового формата:</p> <pre><code># INSERT INTO cities (id, the_geom, name) VALUES (1,ST_GeomFromText(&#39;POINT(-0.1257 51.508)&#39;,4326),&#39;London, England&#39;);
# INSERT INTO cities (id, the_geom, name) VALUES (2,ST_GeomFromText(&#39;POINT(-81.233 42.983)&#39;,4326),&#39;London, Ontario&#39;);
# INSERT INTO cities (id, the_geom, name) VALUES (3,ST_GeomFromText(&#39;POINT(27.91162491 -33.01529)&#39;,4326),&#39;East London,SA&#39;);</code></pre> <p>Все самые обычные операторы SQL могут быть использованы для выбора данных из таблицы PostGIS:</p> <pre><code># SELECT * FROM cities;
 id |      name       |                      the_geom
----+-----------------+----------------------------------------------------
  1 | London, England | 0101000020E6100000BBB88D06F016C0BF1B2FDD2406C14940
  2 | London, Ontario | 0101000020E6100000F4FDD478E94E54C0E7FBA9F1D27D4540
  3 | East London,SA  | 0101000020E610000040AB064060E93B4059FAD005F58140C0
(3 rows)</code></pre> <p>Это возвращает нам бессмысленные значения координат в шестнадцатеричной системе. Если вы хотите увидеть вашу геометрию в текстовом формате WKT, используйте функцию <code>ST_AsText(the_geom)</code> или <code>ST_AsEwkt(the_geom)</code>. Вы также можете использовать функции <code>ST_X(the_geom)</code>, <code>ST_Y(the_geom)</code>, чтобы получить числовые значения координат:</p> <pre><code># SELECT id, ST_AsText(the_geom), ST_AsEwkt(the_geom), ST_X(the_geom), ST_Y(the_geom) FROM cities;
 id |          st_astext           |               st_asewkt                |    st_x     |   st_y
----+------------------------------+----------------------------------------+-------------+-----------
  1 | POINT(-0.1257 51.508)        | SRID=4326;POINT(-0.1257 51.508)        |     -0.1257 |    51.508
  2 | POINT(-81.233 42.983)        | SRID=4326;POINT(-81.233 42.983)        |     -81.233 |    42.983
  3 | POINT(27.91162491 -33.01529) | SRID=4326;POINT(27.91162491 -33.01529) | 27.91162491 | -33.01529
(3 rows)</code></pre> <p>Большинство таких функций начинаются с ST (пространственный тип) и описаны в документации PostGIS. Теперь ответим на практический вопрос: на каком расстоянии в метрах друг от другах находятся три города с названием Лондон, учитывая сферичность земли?</p> <pre><code># SELECT p1.name,p2.name,ST_Distance_Sphere(p1.the_geom,p2.the_geom) FROM cities AS p1, cities AS p2 WHERE p1.id &gt; p2.id;
      name       |      name       | st_distance_sphere
-----------------+-----------------+--------------------
 London, Ontario | London, England |   5875787.03777356
 East London,SA  | London, England |   9789680.59961472
 East London,SA  | London, Ontario |   13892208.6782928
(3 rows)</code></pre> <p>Этот запрос возвращает расстояние в метрах между каждой парой городов. Обратите внимание как часть <code>WHERE</code> предотвращает нас от получения расстояния от города до самого себя (расстояние всегда будет равно нулю) и расстояния в обратном порядке (расстояние от Лондона, Англия до Лондона, Онтарио будет таким же как от Лондона, Онтарио до Лондона, Англия). Также можем рассчитать расстояния на сфере, используя различные функции и указывая называния сфероида, параметры главных полуосей и коэффициента обратного сжатия:</p> <pre><code># SELECT p1.name,p2.name,ST_Distance_Spheroid(
#         p1.the_geom,p2.the_geom, &#39;SPHEROID[&quot;GRS_1980&quot;,6378137,298.257222]&#39;
#         )
#        FROM cities AS p1, cities AS p2 WHERE p1.id &gt; p2.id;
      name       |      name       | st_distance_spheroid
-----------------+-----------------+----------------------
 London, Ontario | London, England |     5892413.63999153
 East London,SA  | London, England |     9756842.65715046
 East London,SA  | London, Ontario |     13884149.4143795
(3 rows)</code></pre> <h3 id="заключение-12">Заключение</h3> <p>В данной главе мы рассмотрели как начать работать с PostGIS. Более подробно о использовании расширения можно ознакомиться через <a href="http://postgis.net/documentation/">официальную документацию</a>.</p> <h2 id=pgsphere>pgSphere</h2> <p><a href="http://pgsphere.github.io/">pgSphere</a> обеспечивает PostgreSQL сферическими типами данных, а также функциями и операторами для работы с ними. Используется для работы с географическими (может использоваться вместо PostGIS) или астрономическими типами данных.</p> <h3 id="установка-и-использование-1">Установка и использование</h3> <p>Для начала инициализируем расширение в базе данных:</p> <pre><code># CREATE EXTENSION pg_sphere;</code></pre> <p>После этого можем проверить, что расширение функционирует:</p> <pre><code># SELECT spoly &#39;{ (270d,-10d), (270d,30d), (290d,10d) } &#39;;
                                                          spoly
-------------------------------------------------------------------------------------------------------------------------
 {(4.71238898038469 , -0.174532925199433),(4.71238898038469 , 0.523598775598299),(5.06145483078356 , 0.174532925199433)}
(1 row)</code></pre> <p>И работу индексов:</p> <pre><code># CREATE TABLE test (
#   pos spoint NOT NULL
# );
CREATE TABLE
# CREATE INDEX test_pos_idx ON test USING GIST (pos);
CREATE INDEX
# INSERT INTO test(pos) VALUES (&#39;( 10.1d, -90d)&#39;), (&#39;( 10d 12m 11.3s, -13d 14m)&#39;);
INSERT 0 2
# VACUUM ANALYZE test;
VACUUM
# SELECT set_sphere_output(&#39;DEG&#39;);
 set_sphere_output
-------------------
 SET DEG
(1 row)

# SELECT * FROM test;
                   pos
------------------------------------------
 (10.1d , -90d)
 (10.2031388888889d , -13.2333333333333d)
(2 rows)
# SET enable_seqscan = OFF;
SET
# EXPLAIN SELECT * FROM test WHERE pos = spoint &#39;(10.1d,-90d)&#39;;
                                QUERY PLAN
---------------------------------------------------------------------------
 Bitmap Heap Scan on test  (cost=4.16..9.50 rows=2 width=16)
   Recheck Cond: (pos = &#39;(10.1d , -90d)&#39;::spoint)
   -&gt;  Bitmap Index Scan on test_pos_idx  (cost=0.00..4.16 rows=2 width=0)
         Index Cond: (pos = &#39;(10.1d , -90d)&#39;::spoint)
(4 rows)</code></pre> <h3 id="заключение-13">Заключение</h3> <p>Более подробно о использовании расширения можно ознакомиться через <a href="http://pgsphere.projects.pgfoundry.org/">официальную документацию</a>.</p> <h2 id="sec:hstore-extension">HStore</h2> <p><a href="https://www.postgresql.org/docs/current/static/hstore.html">HStore</a> – расширение, которое реализует тип данных для хранения ключ/значение в пределах одного значения в PostgreSQL (например, в одном текстовом поле). Это может быть полезно в различных ситуациях, таких как строки с многими атрибутами, которые редко вибираются, или полу-структурированные данные. Ключи и значения являются простыми текстовыми строками.</p> <p>Начиная с версии 9.4 PostgreSQL был добавлен JSONB тип (бинарный JSON). Данный тип является объединением JSON структуры с возможностью использовать индексы, как у Hstore. JSONB лучше Hstore тем, что есть возможность сохранять вложеную структуру данных (nested) и хранить не только текстовые строки в значениях. Поэтому лучше использовать JSONB, если есть такая возможность.</p> <h3 id="установка-и-использование-2">Установка и использование</h3> <p>Для начала активируем расширение:</p> <pre><code># CREATE EXTENSION hstore;</code></pre> <p>Проверим его работу:</p> <pre><code># SELECT &#39;a=&gt;1,a=&gt;2&#39;::hstore;
  hstore
----------
 &quot;a&quot;=&gt;&quot;1&quot;
(1 row)</code></pre> <p>Как видно в примере [lst:hstore2] ключи в hstore уникальны. Создадим таблицу и заполним её данными:</p> <pre><code>CREATE TABLE products (
   id serial PRIMARY KEY,
   name varchar,
   attributes hstore
);
INSERT INTO products (name, attributes)
VALUES (
  &#39;Geek Love: A Novel&#39;,
  &#39;author    =&gt; &quot;Katherine Dunn&quot;,
  pages     =&gt; 368,
  category  =&gt; fiction&#39;
),
(
 &#39;Leica M9&#39;,
 &#39;manufacturer  =&gt; Leica,
  type          =&gt; camera,
  megapixels    =&gt; 18,
  sensor        =&gt; &quot;full-frame 35mm&quot;&#39;
),
( &#39;MacBook Air 11&#39;,
 &#39;manufacturer  =&gt; Apple,
  type          =&gt; computer,
  ram           =&gt; 4GB,
  storage       =&gt; 256GB,
  processor     =&gt; &quot;1.8 ghz Intel i7 duel core&quot;,
  weight        =&gt; 2.38lbs&#39;
);</code></pre> <p>Теперь можно производить поиск по ключу:</p> <pre><code># SELECT name, attributes-&gt;&#39;pages&#39; as page FROM products WHERE attributes ? &#39;pages&#39;;
        name        | page
--------------------+------
 Geek Love: A Novel | 368
(1 row)</code></pre> <p>Или по значению ключа:</p> <pre><code># SELECT name, attributes-&gt;&#39;manufacturer&#39; as manufacturer FROM products WHERE attributes-&gt;&#39;type&#39; = &#39;computer&#39;;
       name      | manufacturer
 ----------------+--------------
  MacBook Air 11 | Apple
 (1 row)</code></pre> <p>Создание индексов:</p> <pre><code># CREATE INDEX products_hstore_index ON products USING GIST (attributes);
# CREATE INDEX products_hstore_index ON products USING GIN (attributes);</code></pre> <p>Можно также cоздавать индекс на ключ:</p> <pre><code># CREATE INDEX product_manufacturer ON products ((products.attributes-&gt;&#39;manufacturer&#39;));</code></pre> <h3 id="заключение-14">Заключение</h3> <p>HStore — расширение для удобного и индексируемого хранения слабоструктурированых данных в PostgreSQL, если нет возможности использовать версию базы 9.4 или выше, где для данной задачи есть встроеный <a href="https://www.postgresql.org/docs/current/static/datatype-json.html">JSONB</a> тип данных.</p> <h2 id=plv8>PLV8</h2> <p><a href="https://github.com/plv8/plv8">PLV8</a> является расширением, которое предоставляет PostgreSQL процедурный язык с движком V8 JavaScript. С помощью этого расширения можно писать в PostgreSQL JavaScript функции, которые можно вызывать из SQL.</p> <h3 id="установка-и-использование-3">Установка и использование</h3> <p>Для начала инициализируем расширение в базе данных:</p> <pre><code># CREATE extension plv8;</code></pre> <p><a href="http://en.wikipedia.org/wiki/V8_(JavaScript_engine)">V8</a> компилирует JavaScript код непосредственно в машинный код и с помощью этого достигается высокая скорость работы. Для примера расмотрим расчет числа Фибоначчи. Вот функция написана на plpgsql:</p> <pre><code>CREATE OR REPLACE FUNCTION
psqlfib(n int) RETURNS int AS $$
 BEGIN
     IF n &lt; 2 THEN
         RETURN n;
     END IF;
     RETURN psqlfib(n-1) + psqlfib(n-2);
 END;
$$ LANGUAGE plpgsql IMMUTABLE STRICT;</code></pre> <p>Замерим скорость её работы:</p> <pre><code># SELECT n, psqlfib(n) FROM generate_series(0,25,5) as n;
 n  | psqlfib
----+---------
  0 |       0
  5 |       5
 10 |      55
 15 |     610
 20 |    6765
 25 |   75025
(6 rows)

Time: 2520.386 ms</code></pre> <p>Теперь сделаем то же самое, но с использованием PLV8:</p> <pre><code>CREATE OR REPLACE FUNCTION
fib(n int) RETURNS int as $$

  function fib(n) {
    return n&lt;2 ? n : fib(n-1) + fib(n-2)
  }
  return fib(n)

$$ LANGUAGE plv8 IMMUTABLE STRICT;</code></pre> <p>Замерим скорость работы:</p> <pre><code># SELECT n, fib(n) FROM generate_series(0,25,5) as n;
 n  |  fib
----+-------
  0 |     0
  5 |     5
 10 |    55
 15 |   610
 20 |  6765
 25 | 75025
(6 rows)

Time: 5.200 ms</code></pre> <p>Как видим PLV8 приблизительно в 484 (2520.386/5.200) раз быстрее plpgsql. Можно ускорить работу расчета чисел Фибоначи на PLV8 за счет кеширования:</p> <pre><code>CREATE OR REPLACE FUNCTION
fibcache(n int) RETURNS int as $$
  var memo = {0: 0, 1: 1};
  function fib(n) {
    if(!(n in memo))
      memo[n] = fib(n-1) + fib(n-2)
    return memo[n]
  }
  return fib(n);
$$ LANGUAGE plv8 IMMUTABLE STRICT;</code></pre> <p>Замерим скорость работы:</p> <pre><code># SELECT n, fibcache(n) FROM generate_series(0,25,5) as n;
 n  | fibcache
----+----------
  0 |        0
  5 |        5
 10 |       55
 15 |      610
 20 |     6765
 25 |    75025
(6 rows)

Time: 1.202 ms</code></pre> <p>Естественно эти измерения не имеют ничего общего с реальным миром (не нужно каждый день считать числа фибоначи в базе данных), но позволяет понять, как V8 может помочь ускорить функции, которые занимаются вычислением чего-либо в базе.</p> <h3 id=nosql>NoSQL</h3> <p>Одним из полезных применений PLV8 может быть создание на базе PostgreSQL документоориентированного хранилища. Для хранения неструктурированных данных можно использовать hstore («[sec:hstore-extension] »), но у него есть свои недостатки:</p> <ul> <li><p>нет вложенности;</p></li> <li><p>все данные (ключ и значение по ключу) это строка;</p></li> </ul> <p>Для хранения данных многие документоориентированные базы данных используют JSON (MongoDB, CouchDB, Couchbase и т.д.). Для этого, начиная с PostgreSQL 9.2, добавлен тип данных JSON, а с версии 9.4 — JSONB. JSON тип можно добавить для PostgreSQL 9.1 и ниже используя PLV8 и <code>DOMAIN</code>:</p> <pre><code>CREATE OR REPLACE FUNCTION
valid_json(json text)
RETURNS BOOLEAN AS $$
  try {
    JSON.parse(json); return true;
  } catch(e) {
    return false;
  }
$$ LANGUAGE plv8 IMMUTABLE STRICT;

CREATE DOMAIN json AS TEXT
CHECK(valid_json(VALUE));</code></pre> <p>Функция <code>valid_json</code> используется для проверки JSON данных. Пример использования:</p> <pre><code>$ CREATE TABLE members ( id SERIAL, profile json );
$ INSERT INTO members
VALUES(&#39;not good json&#39;);
ERROR:  value for domain json
violates check constraint &quot;json_check&quot;
$ INSERT INTO members
VALUES(&#39;{&quot;good&quot;: &quot;json&quot;, &quot;is&quot;: true}&#39;);
INSERT 0 1
$ SELECT * FROM members;
	    profile
------------------------------
  {&quot;good&quot;: &quot;json&quot;, &quot;is&quot;: true}
(1 row)</code></pre> <p>Рассмотрим пример использования JSON для хранения данных и PLV8 для их поиска. Для начала создадим таблицу и заполним её данными:</p> <pre><code>$ CREATE TABLE members ( id SERIAL, profile json );
$ SELECT count(*) FROM members;
  count
---------
 1000000
(1 row)

Time: 201.109 ms</code></pre> <p>В <code>profile</code> поле мы записали приблизительно такую структуру JSON:</p> <pre><code>{                                  +
  &quot;name&quot;: &quot;Litzy Satterfield&quot;,     +
  &quot;age&quot;: 24,                       +
  &quot;siblings&quot;: 2,                   +
  &quot;faculty&quot;: false,                +
  &quot;numbers&quot;: [                     +
    {                              +
      &quot;type&quot;:   &quot;work&quot;,            +
      &quot;number&quot;: &quot;684.573.3783 x368&quot;+
    },                             +
    {                              +
      &quot;type&quot;:   &quot;home&quot;,            +
      &quot;number&quot;: &quot;625.112.6081&quot;     +
    }                              +
  ]                                +
}</code></pre> <p>Теперь создадим функцию для вывода значения по ключу из JSON (в данном случае ожидаем цифру):</p> <pre><code>CREATE OR REPLACE FUNCTION get_numeric(json_raw json, key text)
RETURNS numeric AS $$
  var o = JSON.parse(json_raw);
  return o[key];
$$ LANGUAGE plv8 IMMUTABLE STRICT;</code></pre> <p>Теперь мы можем произвести поиск по таблице, фильтруя по значениям ключей <code>age</code>, <code>siblings</code> или другим числовым полям:</p> <pre><code>$ SELECT * FROM members WHERE get_numeric(profile, &#39;age&#39;) = 36;
Time: 9340.142 ms
$ SELECT * FROM members WHERE get_numeric(profile, &#39;siblings&#39;) = 1;
Time: 14320.032 ms</code></pre> <p>Поиск работает, но скорость очень маленькая. Чтобы увеличить скорость, нужно создать функциональные индексы:</p> <pre><code>$ CREATE INDEX member_age ON members (get_numeric(profile, &#39;age&#39;));
$ CREATE INDEX member_siblings ON members (get_numeric(profile, &#39;siblings&#39;));</code></pre> <p>С индексами скорость поиска по JSON станет достаточно высокая:</p> <pre><code>$ SELECT * FROM members WHERE get_numeric(profile, &#39;age&#39;) = 36;
Time: 57.429 ms
$ SELECT * FROM members WHERE get_numeric(profile, &#39;siblings&#39;) = 1;
Time: 65.136 ms
$ SELECT count(*) from members where  get_numeric(profile, &#39;age&#39;) = 26 and get_numeric(profile, &#39;siblings&#39;) = 1;
Time: 106.492 ms</code></pre> <p>Получилось отличное документоориентированное хранилище из PostgreSQL. Если используется PostgreSQL 9.4 или новее, то можно использовать JSONB тип, у которого уже есть возможность создавать индексы на требуемые ключи в JSON структуре.</p> <p>PLV8 позволяет использовать некоторые JavaScript библиотеки внутри PostgreSQL. Вот пример рендера <a href="http://mustache.github.com/">Mustache</a> темплейтов:</p> <pre><code>CREATE OR REPLACE FUNCTION mustache(template text, view json)
RETURNS text as $$
  // …400 lines of mustache.js…
  return Mustache.render(template, JSON.parse(view))
$$ LANGUAGE plv8 IMMUTABLE STRICT;</code></pre> <pre><code>$ SELECT mustache(
  &#39;hello {{#things}}{{.}} {{/things}}:) {{#data}}{{key}}{{/data}}&#39;,
  &#39;{&quot;things&quot;: [&quot;world&quot;, &quot;from&quot;, &quot;postgresql&quot;], &quot;data&quot;: {&quot;key&quot;: &quot;and me&quot;}}&#39;
);
		mustache
---------------------------------------
  hello world from postgresql :) and me
(1 row)

Time: 0.837 ms</code></pre> <p>Этот пример показывает какие возможности предоставляет PLV8 в PostgreSQL. В действительности рендерить Mustache темплейты в PostgreSQL не лучшая идея.</p> <h3 id="заключение-15">Заключение</h3> <p>PLV8 расширение предоставляет PostgreSQL процедурный язык с движком V8 JavaScript, с помощью которого можно работать с JavaScript билиотеками, индексировать JSON данные и использовать его как более быстрый язык для вычислений внутри базы.</p> <h2 id=pg_repack>Pg_repack</h2> <p>Таблицы в PostgreSQL представлены в виде страниц, размером 8 КБ, в которых размещены записи. Когда одна страница полностью заполняется записями, к таблице добавляется новая страница. При удалалени записей с помощью <code>DELETE</code> или изменении с помощью <code>UPDATE</code>, место где были старые записи не может быть повторно использовано сразу же. Для этого процесс очистки autovacuum, или команда <code>VACUUM</code>, пробегает по изменённым страницам и помечает такое место как свободное, после чего новые записи могут спокойно записываться в это место. Если autovacuum не справляется, например в результате активного изменения большего количества данных или просто из-за плохих настроек, то к таблице будут излишне добавляться новые страницы по мере поступления новых записей. И даже после того как очистка дойдёт до наших удалённых записей, новые страницы останутся. Получается что таблица становится более разряженной в плане плотности записей. Это и называется эффектом раздувания таблиц (table bloat).</p> <p>Процедура очистки, autovacuum или <code>VACUUM</code>, может уменьшить размер таблицы убрав полностью пустые страницы, но только при условии что они находятся в самом конце таблицы. Чтобы максимально уменьшить таблицу в PostgreSQL есть <code>VACUUM FULL</code> или <code>CLUSTER</code>, но оба эти способа требуют «exclusively locks» на таблицу (то есть в это время с таблицы нельзя ни читать, ни писать), что далеко не всегда является подходящим решением.</p> <p>Для решения подобных проблем существует утилита <a href="http://reorg.github.io/pg_repack/">pg_repack</a>. Эта утилита позволяет сделать <code>VACUUM FULL</code> или <code>CLUSTER</code> команды без блокировки таблицы. Для чистки таблицы pg_repack создает точную её копию в <code>repack</code> схеме базы данных (ваша база по умолчанию работает в <code>public</code> схеме) и сортирует строки в этой таблице. После переноса данных и чиски мусора, утилита меняет схему у таблиц. Для чистки индексов утилита создает новые индексы с другими именами, а по выполнению работы меняет их на первоначальные. Для выполнения всех этих работ потребуется дополнительное место на диске (например, если у вас 100 ГБ данных, и из них 40 ГБ - распухание таблиц или индексов, то вам потребуется 100 ГБ + (100 ГБ - 40 ГБ) = 160 ГБ на диске минимум). Для проверки «распухания» таблиц и индексов в вашей базе можно воспользоватся SQL запросом из раздела «[sec:snippets-bloating] ».</p> <p>Существует ряд ограничений в работе pg_repack:</p> <ul> <li><p>Не может очистить временные таблицы;</p></li> <li><p>Не может очистить таблицы с использованием GIST индексов;</p></li> <li><p>Нельзя выполнять DDL (Data Definition Language) на таблице во время работы;</p></li> </ul> <h3 id="пример-использования-1">Пример использования</h3> <p>Выполнить команду <code>CLUSTER</code> всех кластеризированных таблиц и <code>VACUUM FULL</code> для всех не кластеризированных таблиц в <code>test</code> базе данных:</p> <pre><code>$ pg_repack test</code></pre> <p>Выполните команду <code>VACUUM FULL</code> на <code>foo</code> и <code>bar</code> таблицах в <code>test</code> базе данных (кластеризация таблиц игнорируется):</p> <pre><code>$ pg_repack --no-order --table foo --table bar test</code></pre> <p>Переместить все индексы таблицы <code>foo</code> в неймспейс <code>tbs</code>:</p> <pre><code>$ pg_repack -d test --table foo --only-indexes --tablespace tbs</code></pre> <h3 id=pgcompact>Pgcompact</h3> <p>Существует еще одно решение для борьбы с раздуванием таблиц. При обновлении записи с помощью <code>UPDATE</code>, если в таблице есть свободное место, то новая версия пойдет именно в свободное место, без выделения новых страниц. Предпочтение отдается свободному месту ближе к началу таблицы. Если обновлять таблицу с помощью «fake updates»(<code>some_column = some_column</code>) с последней страницы, в какой-то момент, все записи с последней страницы перейдут в свободное место в предшествующих страницах таблицы. Таким образом, после нескольких таких операций, последние страницы окажутся пустыми и обычный неблокирующий <code>VACUUM</code> сможет отрезать их от таблицы, тем самым уменьшив размер. В итоге, с помощью такой техники можно максимально сжать таблицу, при этом не вызывая критичных блокировок, а значит без помех для других сессий и нормальной работы базы. Для автоматизации этой процедуры существует утилита <a href="https://github.com/grayhemp/pgtoolkit">pgcompactor</a>.</p> <p>Основные характеристики утилиты:</p> <ul> <li><p>не требует никаких зависимостей кроме Perl &gt;=5.8.8, можно просто скопировать pgcompactor на сервер и работать с ним;</p></li> <li><p>работает через адаптеры <code>DBD::Pg</code>, <code>DBD::PgPP</code> или даже через стандартную утилиту psql, если первых двух на сервере нет;</p></li> <li><p>обработка как отдельных таблиц, так и всех таблиц внутри схемы, базы или всего кластера;</p></li> <li><p>возможность исключения баз, схем или таблиц из обработки;</p></li> <li><p>анализ эффекта раздувания и обработка только тех таблиц, у которых он присутствует, для более точных расчетов рекомендуется установить расширение <a href="https://www.postgresql.org/docs/current/static/pgstattuple.html">pgstattuple</a>;</p></li> <li><p>анализ и перестроение индексов с эффектом раздувания;</p></li> <li><p>анализ и перестроение уникальных ограничений (unique constraints) и первичных ключей (primary keys) с эффектом раздувания;</p></li> <li><p>инкрементальное использование, т.е. можно остановить процесс сжатия без ущерба чему-либо;</p></li> <li><p>динамическая подстройка под текущую нагрузку базы данных, чтобы не влиять на производительность пользовательских запросов (с возможностью регулировки при запуске);</p></li> <li><p>рекомендации администраторам, сопровождаемые готовым DDL, для перестроения объектов базы, которые не могут быть перестроены в автоматическом режиме;</p></li> </ul> <p>Рассмотрим пример использования данной утилиты. Для начала создадим таблицу:</p> <pre><code># create table test (
    id int4,
    int_1 int4,
    int_2 int4,
    int_3 int4,
    ts_1 timestamptz,
    ts_2 timestamptz,
    ts_3 timestamptz,
    text_1 text,
    text_2 text,
    text_3 text
);</code></pre> <p>После этого заполним её данными:</p> <pre><code># insert into test
select
    i,
    cast(random() * 10000000 as int4),
    cast(random()*10000000 as int4),
    cast(random()*10000000 as int4),
    now() - &#39;2 years&#39;::interval * random(),
    now() - &#39;2 years&#39;::interval * random(),
    now() - &#39;2 years&#39;::interval * random(),
    repeat(&#39;text_1 &#39;, cast(10 + random() * 100 as int4)),
    repeat(&#39;text_2 &#39;, cast(10 + random() * 100 as int4)),
    repeat(&#39;text_2 &#39;, cast(10 + random() * 100 as int4))
from generate_series(1, 1000000) i;</code></pre> <p>И добавим индексы:</p> <pre><code># alter table test add primary key (id);
ALTER TABLE

# create unique index i1 on test (int_1, int_2);
CREATE INDEX

# create index i2 on test (int_2);
CREATE INDEX

# create index i3 on test (int_3, ts_1);
CREATE INDEX

# create index i4 on test (ts_2);
CREATE INDEX

# create index i5 on test (ts_3);
CREATE INDEX

# create index i6 on test (text_1);
CREATE INDEX</code></pre> <p>В результате получам test таблицу как показано на [lst:pgcompactor4]:</p> <pre><code># \d test
              Table &quot;public.test&quot;
 Column |           Type           | Modifiers
--------+--------------------------+-----------
 id     | integer                  | not null
 int_1  | integer                  |
 int_2  | integer                  |
 int_3  | integer                  |
 ts_1   | timestamp with time zone |
 ts_2   | timestamp with time zone |
 ts_3   | timestamp with time zone |
 text_1 | text                     |
 text_2 | text                     |
 text_3 | text                     |
Indexes:
    &quot;test_pkey&quot; PRIMARY KEY, btree (id)
    &quot;i1&quot; UNIQUE, btree (int_1, int_2)
    &quot;i2&quot; btree (int_2)
    &quot;i3&quot; btree (int_3, ts_1)
    &quot;i4&quot; btree (ts_2)
    &quot;i5&quot; btree (ts_3)
    &quot;i6&quot; btree (text_1)</code></pre> <p>Размер таблицы и индексов:</p> <pre><code># SELECT nspname || &#39;.&#39; || relname AS &quot;relation&quot;,
    pg_size_pretty(pg_total_relation_size(C.oid)) AS &quot;total_size&quot;
  FROM pg_class C
  LEFT JOIN pg_namespace N ON (N.oid = C.relnamespace)
  WHERE nspname NOT IN (&#39;pg_catalog&#39;, &#39;information_schema&#39;)
    AND nspname !~ &#39;^pg_toast&#39;
  ORDER BY pg_total_relation_size(C.oid) DESC
  LIMIT 20;
     relation     | total_size
------------------+------------
 public.test      | 1705 MB
 public.i6        | 242 MB
 public.i3        | 30 MB
 public.i1        | 21 MB
 public.i2        | 21 MB
 public.i4        | 21 MB
 public.i5        | 21 MB
 public.test_pkey | 21 MB
(8 rows)</code></pre> <p>Теперь давайте создадим распухание таблицы. Для этого удалим 95% записей в ней:</p> <pre><code># DELETE FROM test WHERE random() &lt; 0.95;
DELETE 950150</code></pre> <p>Далее сделаем <code>VACUUM</code> и проверим размер заново:</p> <pre><code># VACUUM;
VACUUM
# SELECT nspname || &#39;.&#39; || relname AS &quot;relation&quot;,
    pg_size_pretty(pg_total_relation_size(C.oid)) AS &quot;total_size&quot;
  FROM pg_class C
  LEFT JOIN pg_namespace N ON (N.oid = C.relnamespace)
  WHERE nspname NOT IN (&#39;pg_catalog&#39;, &#39;information_schema&#39;)
    AND nspname !~ &#39;^pg_toast&#39;
  ORDER BY pg_total_relation_size(C.oid) DESC
  LIMIT 20;
     relation     | total_size
------------------+------------
 public.test      | 1705 MB
 public.i6        | 242 MB
 public.i3        | 30 MB
 public.i1        | 21 MB
 public.i2        | 21 MB
 public.i4        | 21 MB
 public.i5        | 21 MB
 public.test_pkey | 21 MB
(8 rows)</code></pre> <p>Как видно из результата, размер не изменился. Теперь попробуем убрать через pgcompact распухание таблицы и индексов (для этого дополнительно добавим в базу данных расширение pgstattuple):</p> <pre><code># CREATE EXTENSION pgstattuple;
# \q

$ git clone https://github.com/grayhemp/pgtoolkit.git
$ cd pgtoolkit
$ time ./bin/pgcompact -v info -d analytics_prod --reindex 2&gt;&amp;1 | tee pgcompact.output
Sun Oct 30 11:01:18 2016 INFO Database connection method: psql.
Sun Oct 30 11:01:18 2016 dev INFO Created environment.
Sun Oct 30 11:01:18 2016 dev INFO Statictics calculation method: pgstattuple.
Sun Oct 30 11:02:03 2016 dev, public.test INFO Vacuum initial: 169689 pages left, duration 45.129 seconds.
Sun Oct 30 11:02:13 2016 dev, public.test INFO Bloat statistics with pgstattuple: duration 9.764 seconds.
Sun Oct 30 11:02:13 2016 dev, public.test NOTICE Statistics: 169689 pages (218233 pages including toasts and indexes), approximately 94.62% (160557 pages) can be compacted reducing the size by 1254 MB.
Sun Oct 30 11:02:13 2016 dev, public.test INFO Update by column: id.
Sun Oct 30 11:02:13 2016 dev, public.test INFO Set pages/round: 10.
Sun Oct 30 11:02:13 2016 dev, public.test INFO Set pages/vacuum: 3394.
Sun Oct 30 11:04:56 2016 dev, public.test INFO Progress: 0%, 260 pages completed.
Sun Oct 30 11:05:45 2016 dev, public.test INFO Cleaning in average: 85.8 pages/second (0.117 seconds per 10 pages).
Sun Oct 30 11:05:48 2016 dev, public.test INFO Vacuum routine: 166285 pages left, duration 2.705 seconds.
Sun Oct 30 11:05:48 2016 dev, public.test INFO Set pages/vacuum: 3326.
Sun Oct 30 11:05:57 2016 dev, public.test INFO Progress: 2%, 4304 pages completed.
Sun Oct 30 11:06:19 2016 dev, public.test INFO Cleaning in average: 841.6 pages/second (0.012 seconds per 10 pages).
Sun Oct 30 11:06:23 2016 dev, public.test INFO Vacuum routine: 162942 pages left, duration 4.264 seconds.
Sun Oct 30 11:06:23 2016 dev, public.test INFO Set pages/vacuum: 3260.
Sun Oct 30 11:06:49 2016 dev, public.test INFO Cleaning in average: 818.1 pages/second (0.012 seconds per 10 pages).
Sun Oct 30 11:06:49 2016 dev, public.test INFO Vacuum routine: 159681 pages left, duration 0.325 seconds.
Sun Oct 30 11:06:49 2016 dev, public.test INFO Set pages/vacuum: 3194.
Sun Oct 30 11:06:57 2016 dev, public.test INFO Progress: 6%, 10958 pages completed.
Sun Oct 30 11:07:23 2016 dev, public.test INFO Cleaning in average: 694.8 pages/second (0.014 seconds per 10 pages).
Sun Oct 30 11:07:24 2016 dev, public.test INFO Vacuum routine: 156478 pages left, duration 1.362 seconds.
Sun Oct 30 11:07:24 2016 dev, public.test INFO Set pages/vacuum: 3130.
...
Sun Oct 30 11:49:02 2016 dev NOTICE Processing complete.
Sun Oct 30 11:49:02 2016 dev NOTICE Processing results: size reduced by 1256 MB (1256 MB including toasts and indexes) in total.
Sun Oct 30 11:49:02 2016 NOTICE Processing complete: 0 retries from 10.
Sun Oct 30 11:49:02 2016 NOTICE Processing results: size reduced by 1256 MB (1256 MB including toasts and indexes) in total, 1256 MB (1256 MB) dev.
Sun Oct 30 11:49:02 2016 dev INFO Dropped environment.

real	47m44.831s
user	0m37.692s
sys	0m16.336s</code></pre> <p>После данной процедуры проверим размер таблицы и индексов в базе:</p> <pre><code># VACUUM;
VACUUM
# SELECT nspname || &#39;.&#39; || relname AS &quot;relation&quot;,
    pg_size_pretty(pg_total_relation_size(C.oid)) AS &quot;total_size&quot;
  FROM pg_class C
  LEFT JOIN pg_namespace N ON (N.oid = C.relnamespace)
  WHERE nspname NOT IN (&#39;pg_catalog&#39;, &#39;information_schema&#39;)
    AND nspname !~ &#39;^pg_toast&#39;
  ORDER BY pg_total_relation_size(C.oid) DESC
  LIMIT 20;
     relation     | total_size
------------------+------------
 public.test      | 449 MB
 public.i6        | 12 MB
 public.i3        | 1544 kB
 public.i1        | 1112 kB
 public.i2        | 1112 kB
 public.test_pkey | 1112 kB
 public.i4        | 1112 kB
 public.i5        | 1112 kB
(8 rows)</code></pre> <p>Как видно в результате размер таблицы сократился до 449 МБ (было 1705 МБ). Индексы тоже стали меньше (например, <code>i6</code> был 242 МБ, а стал 12 МБ). Операция занял 47 минут и обрабатывала в среднем 600 страниц с секунду (4.69 МБ в секунду). Можно ускорить выполнение этой операции через <code>--delay-ratio</code> параметр (задержка между раундами выполнения, по умолчанию 2 секунды) и <code>--max-pages-per-round</code> параметр (количество страниц, что будет обработано за один раунд, по умолчанию 10). Более подробно по параметрам pgcompact можно ознакомится через команду <code>pgcompact --man</code>.</p> <h3 id="заключение-16">Заключение</h3> <p>Pg_repack и pgcompact — утилиты, которые могут помочь в борьбе с распуханием таблиц в PostgreSQL «на лету».</p> <h2 id=pg_prewarm>Pg_prewarm</h2> <p>Модуль <a href="https://www.postgresql.org/docs/current/static/pgprewarm.html">pg_prewarm</a> обеспечивает удобный способ загрузки данных обьектов (таблиц, индексов, прочего) в буферный кэш PostgreSQL или операционной системы. Данный модуль добавлен в contrib начиная с PostgreSQL 9.4.</p> <h3 id="установка-и-использование-4">Установка и использование</h3> <p>Для начала нужно установить модуль:</p> <pre><code>$ CREATE EXTENSION pg_prewarm;</code></pre> <p>После уставновки доступна функция <code>pg_prewarm</code>:</p> <pre><code>$ SELECT pg_prewarm(&#39;pgbench_accounts&#39;);
 pg_prewarm
------------
       4082
(1 row)</code></pre> <p>Первый аргумент — объект, который требуется предварительно загружать в память. Второй аргумент — «режим» загрузки в память, который может содержать такие варианты:</p> <ul> <li><p><code>prefetch</code> — чтение файла ядром системы в асинхронном режиме (в фоновом режиме). Это позволяет положить содержимое файла в кэше ядра системы. Но этот режим не работает на всех платформах;</p></li> <li><p><code>read</code> — результат похож на <code>prefetch</code>, но делается синхронно (а значит медленнее). Работает на всех платформах;</p></li> <li><p><code>buffer</code> — в этом режиме данные будут грузится в PostgreSQL <code>shared_buffers</code>. Этот режим используется по умолчанию;</p></li> </ul> <p>Третий аргумент называется «fork». Про него не нужно беспокоиться. Возможные значения: «main» (используется по умолчанию), «fsm», «vm».</p> <p>Четвертый и пятый аргументы указывают диапазон страниц для загрузки данных. По умолчанию загружается весь обьект в память, но можно решить, например, загрузить только последние 1000 страниц:</p> <pre><code>$ SELECT pg_prewarm(
    &#39;pgbench_accounts&#39;,
    first_block := (
        SELECT pg_relation_size(&#39;pgbench_accounts&#39;) / current_setting(&#39;block_size&#39;)::int4 - 1000
    )
);</code></pre> <h3 id="заключение-17">Заключение</h3> <p>Pg_prewarm — расширение, которое позволяет предварительно загрузить («подогреть») данные в буферной кэш PostgreSQL или операционной системы.</p> <h2 id=smlar>Smlar</h2> <p>Поиск похожести в больших базах данных является важным вопросом в настоящее время для таких систем как блоги (похожие статьи), интернет-магазины (похожие продукты), хостинг изображений (похожие изображения, поиск дубликатов изображений) и т.д. PostgreSQL позволяет сделать такой поиск более легким. Прежде всего, необходимо понять, как мы будем вычислять сходство двух объектов.</p> <h3 id="похожесть">Похожесть</h3> <p>Любой объект может быть описан как список характеристик. Например, статья в блоге может быть описана тегами, продукт в интернет-магазине может быть описан размером, весом, цветом и т.д. Это означает, что для каждого объекта можно создать цифровую подпись — массив чисел, описывающих объект (<a href="http://en.wikipedia.org/wiki/Fingerprint">отпечатки пальцев</a>, <a href="http://en.wikipedia.org/wiki/N-gram">n-grams</a>). То есть нужно создать массив из цифр для описания каждого объекта.</p> <h3 id="расчет-похожести">Расчет похожести</h3> <p>Есть несколько методов вычисления похожести сигнатур объектов. Прежде всего, легенда для расчетов:</p> <ul> <li><p><span class=LaTeX>$N_a$</span>, <span class=LaTeX>$N_b$</span> — количество уникальных элементов в массивах;</p></li> <li><p><span class=LaTeX>$N_u$</span> — количество уникальных элементов при объединении массивов;</p></li> <li><p><span class=LaTeX>$N_i$</span> — количество уникальных элементов при пересечение массивов.</p></li> </ul> <p>Один из простейших расчетов похожести двух объектов - количество уникальных элементов при пересечение массивов делить на количество уникальных элементов в двух массивах:</p> <p><span class=LaTeX>$$\label{eq:smlar1} S(A,B) = \frac{N_{i}}{(N_{a}+N_{b})}$$</span></p> <p>или проще</p> <p><span class=LaTeX>$$\label{eq:smlar2} S(A,B) = \frac{N_{i}}{N_{u}}$$</span></p> <p>Преимущества:</p> <ul> <li><p>Легко понять;</p></li> <li><p>Скорость расчета: <span class=LaTeX>$N * \log{N}$</span>;</p></li> <li><p>Хорошо работает на похожих и больших <span class=LaTeX>$N_a$</span> и <span class=LaTeX>$N_b$</span>;</p></li> </ul> <p>Также похожесть можно рассчитана по <a href="http://en.wikipedia.org/wiki/Law_of_cosines">формуле косинусов</a>:</p> <p><span class=LaTeX>$$\label{eq:smlar3} S(A,B) = \frac{N_{i}}{\sqrt{N_{a}*N_{b}}}$$</span></p> <p>Преимущества:</p> <ul> <li><p>Скорость расчета: <span class=LaTeX>$N * \log{N}$</span>;</p></li> <li><p>Отлично работает на больших <span class=LaTeX>$N$</span>;</p></li> </ul> <p>Но у обоих этих методов есть общие проблемы:</p> <ul> <li><p>Если элементов мало, то разброс похожести не велик;</p></li> <li><p>Глобальная статистика: частые элементы ведут к тому, что вес ниже;</p></li> <li><p>Спамеры и недобросовестные пользователи могут разрушить работу алгоритма и он перестанет работать на Вас;</p></li> </ul> <p>Для избежания этих проблем можно воспользоваться <a href="http://en.wikipedia.org/wiki/Tf*idf">TF/IDF метрикой</a>:</p> <p><span class=LaTeX>$$\label{eq:smlar4} S(A,B) = \frac{\sum_{i &lt; N_{a}, j &lt; N_{b}, A_{i} = B_{j}}TF_{i} * TF_{j}}{\sqrt{\sum_{i &lt; N_{a}}TF_{i}^{2} * \sum_{j &lt; N_{b}}TF_{j}^{2}}}$$</span></p> <p>где инвертированный вес элемента в коллекции:</p> <p><span class=LaTeX>$$\label{eq:smlar5} IDF_{element} = \log{(\frac{N_{objects}}{N_{objects\ with\ element}} + 1)}$$</span></p> <p>и вес элемента в массиве:</p> <p><span class=LaTeX>$$\label{eq:smlar6} TF_{element} = IDF_{element} * N_{occurrences}$$</span></p> <p>Все эти алгоритмы встроены в smlar расширение. Главное понимать, что для TF/IDF метрики требуются вспомогательная таблица для хранения данных, по сравнению с другими простыми метриками.</p> <h3 id=smlar-1>Smlar</h3> <p>Олег Бартунов и Теодор Сигаев разработали PostgreSQL расширение <a href="http://sigaev.ru/git/gitweb.cgi?p=smlar.git;a=blob;hb=HEAD;f=README">smlar</a>, которое предоставляет несколько методов для расчета похожести массивов (все встроенные типы данных поддерживаются) и оператор для расчета похожести с поддержкой индекса на базе GIST и GIN. Для начала установим это расширение:</p> <pre><code>$ git clone git://sigaev.ru/smlar
$ cd smlar
$ USE_PGXS=1 make &amp;&amp; make install</code></pre> <p>Теперь проверим расширение:</p> <pre><code>$ psql
psql (9.5.1)
Type &quot;help&quot; for help.

test=# CREATE EXTENSION smlar;
CREATE EXTENSION

test=# SELECT smlar(&#39;{1,4,6}&#39;::int[], &#39;{5,4,6}&#39;::int[]);
  smlar
----------
 0.666667
(1 row)

test=# SELECT smlar(&#39;{1,4,6}&#39;::int[], &#39;{5,4,6}&#39;::int[], &#39;N.i / sqrt(N.a * N.b)&#39; );
  smlar
----------
 0.666667
(1 row)</code></pre> <p>Методы, которые предоставляет это расширение:</p> <ul> <li><p><code>float4 smlar(anyarray, anyarray)</code> — вычисляет похожесть двух массивов. Массивы должны быть одного типа;</p></li> <li><p><code>float4 smlar(anyarray, anyarray, bool useIntersect)</code> — вычисляет похожесть двух массивы составных типов. Составной тип выглядит следующим образом:</p> <pre><code>CREATE TYPE type_name AS (element_name anytype, weight_name float4);</code></pre> <p><code>useIntersect</code> параметр для использования пересекающихся элементов в знаменателе;</p></li> <li><p><code>float4 smlar( anyarray a, anyarray b, text formula )</code> — вычисляет похожесть двух массивов по данной формуле, массивы должны быть того же типа. Доступные переменные в формуле:</p> <ul> <li><p>N.i — количество общих элементов в обоих массивов (пересечение);</p></li> <li><p>N.a — количество уникальных элементов первого массива;</p></li> <li><p>N.b — количество уникальных элементов второго массива;</p></li> </ul></li> <li><p><code>anyarray % anyarray</code> — возвращает истину, если похожесть массивов больше, чем указанный предел. Предел указывается в конфиге PostgreSQL:</p> <pre><code>custom_variable_classes = &#39;smlar&#39;
smlar.threshold = 0.8 # предел от 0 до 1</code></pre> <p>Также в конфиге можно указать дополнительные настройки для smlar:</p> <pre><code>custom_variable_classes = &#39;smlar&#39;
smlar.threshold = 0.8 # предел от 0 до 1
smlar.type = &#39;cosine&#39; # по какой формуле производить расчет похожести: cosine, tfidf, overlap
smlar.stattable = &#39;stat&#39; # Имя таблицы для хранения статистики при работе по формуле tfidf</code></pre> <p>Более подробно можно прочитать в README этого расширения.</p></li> </ul> <p>GiST и GIN индексы поддерживаются для оператора <code>%</code>.</p> <h3 id="пример-поиск-дубликатов-картинок">Пример: поиск дубликатов картинок</h3> <p>Рассмотрим простой пример поиска дубликатов картинок. Алгоритм помогает найти похожие изображения, которые, например, незначительно отличаются (изображение обесцветили, добавили водяные знаки, пропустили через фильтры). Но, поскольку точность мала, то у алгоритма есть и позитивная сторона — скорость работы. Как можно определить, что картинки похожи? Самый простой метод — сравнивать попиксельно два изображения. Но скорость такой работы будет не велика на больших разрешениях. Тем более, такой метод не учитывает, что могли изменять уровень света, насыщенность и прочие характеристики изображения. Нам нужно создать сигнатуру для картинок в виде массива цифр:</p> <ul> <li><p>Создаем пиксельную матрицу к изображению (изменения размера изображения к требуемоему размеру пиксельной матрице), например 15X15 пикселей(Рис. [fig:smlar1]);</p></li> <li><p>Рассчитаем интенсивность каждого пикселя (интенсивность вычисляется по формуле <span class=LaTeX>$0.299 * \textup{красный} + 0.587 * \textup{зеленый} + 0.114 * \textup{синий}$</span>). Интенсивность поможет нам находить похожие изображения, не обращая внимание на используемые цвета в них;</p></li> <li><p>Узнаем отношение интенсивности каждого пикселя к среднему значению интенсивности по всей матрице(Рис. [fig:smlar2]);</p></li> <li><p>Генерируем уникальное число для каждой ячейки (отношение интенсивности + координаты ячейки);</p></li> <li><p>Сигнатура для картинки готова;</p></li> </ul> <p>Создаем таблицу, где будем хранить имя картинки, путь к ней и её сигнатуру:</p> <pre><code>CREATE TABLE images (
 id serial PRIMARY KEY,
 name varchar(50),
 img_path varchar(250),
 image_array integer[]
);</code></pre> <p>Создадим GIN или GIST индекс:</p> <pre><code>CREATE INDEX image_array_gin ON images USING GIN(image_array _int4_sml_ops);
CREATE INDEX image_array_gist ON images USING GIST(image_array _int4_sml_ops);</code></pre> <p>Теперь можно произвести поиск дубликатов:</p> <pre><code>test=# SELECT count(*) from images;
  count
---------
 1000000
(1 row)

test=# EXPLAIN ANALYZE SELECT count(*) FROM images WHERE images.image_array % &#39;{1010259,1011253,...,2423253,2424252}&#39;::int[];

 Bitmap Heap Scan on images  (cost=286.64..3969.45 rows=986 width=4) (actual time=504.312..2047.533 rows=200000 loops=1)
   Recheck Cond: (image_array % &#39;{1010259,1011253,...,2423253,2424252}&#39;::integer[])
   -&gt;  Bitmap Index Scan on image_array_gist  (cost=0.00..286.39 rows=986 width=0) (actual time=446.109..446.109 rows=200000 loops=1)
         Index Cond: (image_array % &#39;{1010259,1011253,...,2423253,2424252}&#39;::integer[])
 Total runtime: 2152.411 ms
(5 rows)</code></pre> <p>где <code>'{1010259,...,2424252}'::int[]</code> — сигнатура изображения, для которой пытаемся найти похожие изображения. С помощью <code>smlar.threshold</code> управляем <code>%</code> похожести картинок (при каком проценте они будут попадать в выборку).</p> <p>Дополнительно можем добавить сортировку по самым похожим изображениям:</p> <pre><code>test=# EXPLAIN ANALYZE SELECT smlar(images.image_array, &#39;{1010259,...,2424252}&#39;::int[]) as similarity FROM images WHERE images.image_array % &#39;{1010259,1011253, ...,2423253,2424252}&#39;::int[] ORDER BY similarity DESC;


 Sort  (cost=4020.94..4023.41 rows=986 width=924) (actual time=2888.472..2901.977 rows=200000 loops=1)
   Sort Key: (smlar(image_array, &#39;{...,2424252}&#39;::integer[]))
   Sort Method: quicksort  Memory: 15520kB
   -&gt;  Bitmap Heap Scan on images  (cost=286.64..3971.91 rows=986 width=924) (actual time=474.436..2729.638 rows=200000 loops=1)
         Recheck Cond: (image_array % &#39;{...,2424252}&#39;::integer[])
         -&gt;  Bitmap Index Scan on image_array_gist  (cost=0.00..286.39 rows=986 width=0) (actual time=421.140..421.140 rows=200000 loops=1)
               Index Cond: (image_array % &#39;{...,2424252}&#39;::integer[])
 Total runtime: 2912.207 ms
(8 rows)</code></pre> <h3 id="заключение-18">Заключение</h3> <p>Smlar расширение может быть использовано в системах, где нам нужно искать похожие объекты, такие как: тексты, темы, блоги, товары, изображения, видео, отпечатки пальцев и прочее.</p> <h2 id=multicorn>Multicorn</h2> <p><a href="http://multicorn.org/">Multicorn</a> — расширение для PostgreSQL версии 9.1 или выше, которое позволяет создавать собственные FDW (Foreign Data Wrapper) используя язык программирования <a href="https://www.python.org/">Python</a>. Foreign Data Wrapper позволяют подключится к другим источникам данных (другая база, файловая система, REST API, прочее) в PostgreSQL и были представленны с версии 9.1.</p> <h3 id="пример">Пример</h3> <p>Установка будет проводится на Ubuntu Linux. Для начала нужно установить требуемые зависимости:</p> <pre><code>$ sudo aptitude install build-essential postgresql-server-dev-9.3 python-dev python-setuptools</code></pre> <p>Следующим шагом установим расширение:</p> <pre><code>$ git clone git@github.com:Kozea/Multicorn.git
$ cd Multicorn
$ make &amp;&amp; sudo make install</code></pre> <p>Для завершения установки активируем расширение для базы данных:</p> <pre><code># CREATE EXTENSION multicorn;
CREATE EXTENSION</code></pre> <p>Рассмотрим какие FDW может предоставить Multicorn.</p> <h4 id="реляционная-субд">Реляционная СУБД</h4> <p>Для подключения к другой реляционной СУБД Multicorn использует <a href="http://www.sqlalchemy.org/">SQLAlchemy</a> библиотеку. Данная библиотека поддерживает SQLite, PostgreSQL, MySQL, Oracle, MS-SQL, Firebird, Sybase, и другие базы данных. Для примера настроим подключение к MySQL. Для начала нам потребуется установить зависимости:</p> <pre><code>$ sudo aptitude install python-sqlalchemy python-mysqldb</code></pre> <p>В MySQL базе данных «testing» у нас есть таблица «companies»:</p> <pre><code>$ mysql -u root -p testing

mysql&gt; SELECT * FROM companies;
+----+---------------------+---------------------+
| id | created_at          | updated_at          |
+----+---------------------+---------------------+
|  1 | 2013-07-16 14:06:09 | 2013-07-16 14:06:09 |
|  2 | 2013-07-16 14:30:00 | 2013-07-16 14:30:00 |
|  3 | 2013-07-16 14:33:41 | 2013-07-16 14:33:41 |
|  4 | 2013-07-16 14:38:42 | 2013-07-16 14:38:42 |
|  5 | 2013-07-19 14:38:29 | 2013-07-19 14:38:29 |
+----+---------------------+---------------------+
5 rows in set (0.00 sec)</code></pre> <p>В PostgreSQL мы должны создать сервер для Multicorn:</p> <pre><code># CREATE SERVER alchemy_srv foreign data wrapper multicorn options (
    wrapper &#39;multicorn.sqlalchemyfdw.SqlAlchemyFdw&#39;
);
CREATE SERVER</code></pre> <p>Теперь мы можем создать таблицу, которая будет содержать данные из MySQL таблицы «companies»:</p> <pre><code># CREATE FOREIGN TABLE mysql_companies (
  id integer,
  created_at timestamp without time zone,
  updated_at timestamp without time zone
) server alchemy_srv options (
  tablename &#39;companies&#39;,
  db_url &#39;mysql://root:password@127.0.0.1/testing&#39;
);
CREATE FOREIGN TABLE</code></pre> <p>Основные опции:</p> <ul> <li><p><code>db_url (string)</code> — SQLAlchemy настройки подключения к базе данных (примеры: <code>mysql://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;/&lt;dbname&gt;</code>, <code>mssql: mssql://&lt;user&gt;:&lt;password&gt;@&lt;dsname&gt;</code>). Подробнее можно узнать из <a href="http://docs.sqlalchemy.org/en/latest/dialects/">SQLAlchemy документации</a>;</p></li> <li><p><code>tablename (string)</code> — имя таблицы в подключенной базе данных.</p></li> </ul> <p>Теперь можем проверить, что все работает:</p> <pre><code># SELECT * FROM mysql_companies;
 id |     created_at      |     updated_at
----+---------------------+---------------------
  1 | 2013-07-16 14:06:09 | 2013-07-16 14:06:09
  2 | 2013-07-16 14:30:00 | 2013-07-16 14:30:00
  3 | 2013-07-16 14:33:41 | 2013-07-16 14:33:41
  4 | 2013-07-16 14:38:42 | 2013-07-16 14:38:42
  5 | 2013-07-19 14:38:29 | 2013-07-19 14:38:29
(5 rows)</code></pre> <h4 id="imap-сервер">IMAP сервер</h4> <p>Multicorn может использоватся для получение писем по <a href="https://ru.wikipedia.org/wiki/IMAP">IMAP</a> протоколу. Для начала установим зависимости:</p> <pre><code>$ sudo aptitude install python-pip
$ sudo pip install imapclient</code></pre> <p>Следующим шагом мы должны создать сервер и таблицу, которая будет подключена к IMAP серверу:</p> <pre><code># CREATE SERVER multicorn_imap FOREIGN DATA WRAPPER multicorn options ( wrapper &#39;multicorn.imapfdw.ImapFdw&#39; );
CREATE SERVER
# CREATE FOREIGN TABLE my_inbox (
    &quot;Message-ID&quot; character varying,
    &quot;From&quot; character varying,
    &quot;Subject&quot; character varying,
    &quot;payload&quot; character varying,
    &quot;flags&quot; character varying[],
    &quot;To&quot; character varying) server multicorn_imap options (
        host &#39;imap.gmail.com&#39;,
        port &#39;993&#39;,
        payload_column &#39;payload&#39;,
        flags_column &#39;flags&#39;,
        ssl &#39;True&#39;,
        login &#39;example@gmail.com&#39;,
        password &#39;supersecretpassword&#39;
);
CREATE FOREIGN TABLE</code></pre> <p>Основные опции:</p> <ul> <li><p><code>host (string)</code> — IMAP хост;</p></li> <li><p><code>port (string)</code> — IMAP порт;</p></li> <li><p><code>login (string)</code> — IMAP логин;</p></li> <li><p><code>password (string)</code> — IMAP пароль;</p></li> <li><p><code>payload_column (string)</code> — имя поля, которое будет содержать текст письма;</p></li> <li><p><code>flags_column (string)</code> — имя поля, которое будет содержать IMAP флаги письма (массив);</p></li> <li><p><code>ssl (boolean)</code> — использовать SSL для подключения;</p></li> <li><p><code>imap_server_charset (string)</code> — кодировка для IMAP команд. По умолчанию UTF8.</p></li> </ul> <p>Теперь можно получить письма через таблицу «my_inbox»:</p> <pre><code># SELECT flags, &quot;Subject&quot;, payload FROM my_inbox LIMIT 10;
                flags                 |      Subject      |       payload
--------------------------------------+-------------------+---------------------
 {$MailFlagBit1,&quot;\\Flagged&quot;,&quot;\\Seen&quot;} | Test email        | Test email\r       +
                                      |                   |
 {&quot;\\Seen&quot;}                           | Test second email | Test second email\r+
                                      |                   |
(2 rows)</code></pre> <h4 id=rss>RSS</h4> <p>Multicorn может использовать <a href="http://ru.wikipedia.org/wiki/RSS">RSS</a> как источник данных. Для начала установим зависимости:</p> <pre><code>$ sudo aptitude install python-lxml</code></pre> <p>Как и в прошлые разы, создаем сервер и таблицу для RSS ресурса:</p> <pre><code># CREATE SERVER rss_srv foreign data wrapper multicorn options (
    wrapper &#39;multicorn.rssfdw.RssFdw&#39;
);
CREATE SERVER
# CREATE FOREIGN TABLE my_rss (
    &quot;pubDate&quot; timestamp,
    description character varying,
    title character varying,
    link character varying
) server rss_srv options (
    url     &#39;http://news.yahoo.com/rss/entertainment&#39;
);
CREATE FOREIGN TABLE</code></pre> <p>Основные опции:</p> <ul> <li><p><code>url (string)</code> — URL RSS ленты.</p></li> </ul> <p>Кроме того, вы должны быть уверены, что PostgreSQL база данных использовать UTF-8 кодировку (в другой кодировке вы можете получить ошибки). Результат таблицы «my_rss»:</p> <pre><code># SELECT &quot;pubDate&quot;, title, link from my_rss ORDER BY &quot;pubDate&quot; DESC LIMIT 10;
       pubDate       |                       title                        |                                         link
---------------------+----------------------------------------------------+--------------------------------------------------------------------------------------
 2013-09-28 14:11:58 | Royal Mint coins to mark Prince George christening | http://news.yahoo.com/royal-mint-coins-mark-prince-george-christening-115906242.html
 2013-09-28 11:47:03 | Miss Philippines wins Miss World in Indonesia      | http://news.yahoo.com/miss-philippines-wins-miss-world-indonesia-144544381.html
 2013-09-28 10:59:15 | Billionaire&#39;s daughter in NJ court in will dispute | http://news.yahoo.com/billionaires-daughter-nj-court-dispute-144432331.html
 2013-09-28 08:40:42 | Security tight at Miss World final in Indonesia    | http://news.yahoo.com/security-tight-miss-world-final-indonesia-123714041.html
 2013-09-28 08:17:52 | Guest lineups for the Sunday news shows            | http://news.yahoo.com/guest-lineups-sunday-news-shows-183815643.html
 2013-09-28 07:37:02 | Security tight at Miss World crowning in Indonesia | http://news.yahoo.com/security-tight-miss-world-crowning-indonesia-113634310.html
 2013-09-27 20:49:32 | Simons stamps his natural mark on Dior             | http://news.yahoo.com/simons-stamps-natural-mark-dior-223848528.html
 2013-09-27 19:50:30 | Jackson jury ends deliberations until Tuesday      | http://news.yahoo.com/jackson-jury-ends-deliberations-until-tuesday-235030969.html
 2013-09-27 19:23:40 | Eric Clapton-owned Richter painting to sell in NYC | http://news.yahoo.com/eric-clapton-owned-richter-painting-sell-nyc-201447252.html
 2013-09-27 19:14:15 | Report: Hollywood is less gay-friendly off-screen  | http://news.yahoo.com/report-hollywood-less-gay-friendly-off-screen-231415235.html
(10 rows)</code></pre> <h4 id=csv>CSV</h4> <p>Multicorn может использовать <a href="http://ru.wikipedia.org/wiki/CSV">CSV</a> файл как источник данных. Как и в прошлые разы, создаем сервер и таблицу для CSV ресурса:</p> <pre><code># CREATE SERVER csv_srv foreign data wrapper multicorn options (
    wrapper &#39;multicorn.csvfdw.CsvFdw&#39;
);
CREATE SERVER
# CREATE FOREIGN TABLE csvtest (
       sort_order numeric,
       common_name character varying,
       formal_name character varying,
       main_type character varying,
       sub_type character varying,
       sovereignty character varying,
       capital character varying
) server csv_srv options (
       filename &#39;/var/data/countrylist.csv&#39;,
       skip_header &#39;1&#39;,
       delimiter &#39;,&#39;);
CREATE FOREIGN TABLE</code></pre> <p>Основные опции:</p> <ul> <li><p><code>filename (string)</code> — полный путь к CSV файлу;</p></li> <li><p><code>delimiter (character)</code> — разделитель в CSV файле (по умолчанию «,»);</p></li> <li><p><code>quotechar (character)</code> — кавычки в CSV файле;</p></li> <li><p><code>skip_header (integer)</code> — число строк, которые необходимо пропустить (по умолчанию 0).</p></li> </ul> <p>Результат таблицы «csvtest»:</p> <pre><code># SELECT * FROM csvtest LIMIT 10;
sort_order |     common_name     |               formal_name               |     main_type     | sub_type | sovereignty |     capital
------------+---------------------+-----------------------------------------+-------------------+----------+-------------+------------------
         1 | Afghanistan         | Islamic State of Afghanistan            | Independent State |          |             | Kabul
         2 | Albania             | Republic of Albania                     | Independent State |          |             | Tirana
         3 | Algeria             | People&#39;s Democratic Republic of Algeria | Independent State |          |             | Algiers
         4 | Andorra             | Principality of Andorra                 | Independent State |          |             | Andorra la Vella
         5 | Angola              | Republic of Angola                      | Independent State |          |             | Luanda
         6 | Antigua and Barbuda |                                         | Independent State |          |             | Saint John&#39;s
         7 | Argentina           | Argentine Republic                      | Independent State |          |             | Buenos Aires
         8 | Armenia             | Republic of Armenia                     | Independent State |          |             | Yerevan
         9 | Australia           | Commonwealth of Australia               | Independent State |          |             | Canberra
        10 | Austria             | Republic of Austria                     | Independent State |          |             | Vienna
(10 rows)</code></pre> <h4 id="другие-fdw">Другие FDW</h4> <p>Multicorn также содержать FDW для LDAP и файловой системы. LDAP FDW может использоваться для доступа к серверам по <a href="http://ru.wikipedia.org/wiki/LDAP">LDAP протоколу</a>. FDW для файловой системы может быть использован для доступа к данным, хранящимся в различных файлах в файловой системе.</p> <h4 id="собственный-fdw">Собственный FDW</h4> <p>Multicorn предоставляет простой интерфейс для написания собственных FDW. Более подробную информацию вы можете найти по <a href="http://multicorn.org/implementing-an-fdw/">этой ссылке</a>.</p> <h3 id="postgresql-9.3">PostgreSQL 9.3+</h3> <p>В PostgreSQL 9.1 и 9.2 была представленна реализация FDW только на чтение, и начиная с версии 9.3 FDW может писать в внешнии источники данных. Сейчас Multicorn поддерживает запись данных в другие источники начиная с версии 1.0.0.</p> <h3 id="заключение-19">Заключение</h3> <p>Multicorn — расширение для PostgreSQL, которое позволяет использовать встроенные FDW или создавать собственные на Python.</p> <h2 id=pgaudit>Pgaudit</h2> <p><a href="https://github.com/2ndQuadrant/pgaudit">Pgaudit</a> — расширение для PostgreSQL, которое позволяет собирать события из различных источников внутри PostgreSQL и записывает их в формате CSV c временной меткой, информацией о пользователе, информацию про обьект, который был затронут командой (если такое произошло) и полный текст команды. Поддерживает все DDL, DML (включая <code>SELECT</code>) и прочие команды. Данное расширение работает в PostgreSQL 9.3 и выше.</p> <p>После установки расширения нужно добавит в конфиг PostgreSQL настройки расширения:</p> <pre><code>shared_preload_libraries = &#39;pgaudit&#39;

pgaudit.log = &#39;read, write, user&#39;</code></pre> <p>Далее перегрузить базу данных и установить расширение для базы:</p> <pre><code># CREATE EXTENSION pgaudit;</code></pre> <p>После этого в логах можно увидеть подобный результат от pgaudit:</p> <pre><code>LOG:  [AUDIT],2014-04-30 17:13:55.202854+09,auditdb,ianb,ianb,DEFINITION,CREATE TABLE,TABLE,public.x,CREATE  TABLE  public.x (a pg_catalog.int4   , b pg_catalog.int4   )   WITH (oids=OFF)
LOG:  [AUDIT],2014-04-30 17:14:06.548923+09,auditdb,ianb,ianb,WRITE,INSERT,TABLE,public.x,INSERT INTO x VALUES(1,1);
LOG:  [AUDIT],2014-04-30 17:14:21.221879+09,auditdb,ianb,ianb,READ,SELECT,TABLE,public.x,SELECT * FROM x;
LOG:  [AUDIT],2014-04-30 17:15:25.620213+09,auditdb,ianb,ianb,READ,SELECT,VIEW,public.v_x,SELECT * from v_x;
LOG:  [AUDIT],2014-04-30 17:15:25.620262+09,auditdb,ianb,ianb,READ,SELECT,TABLE,public.x,SELECT * from v_x;
LOG:  [AUDIT],2014-04-30 17:16:00.849868+09,auditdb,ianb,ianb,WRITE,UPDATE,TABLE,public.x,UPDATE x SET a=a+1;
LOG:  [AUDIT],2014-04-30 17:16:18.291452+09,auditdb,ianb,ianb,ADMIN,VACUUM,,,VACUUM x;
LOG:  [AUDIT],2014-04-30 17:18:01.08291+09,auditdb,ianb,ianb,DEFINITION,CREATE FUNCTION,FUNCTION,public.func_x(),CREATE  FUNCTION public.func_x() RETURNS  pg_catalog.int4 LANGUAGE sql  VOLATILE  CALLED ON NULL INPUT SECURITY INVOKER COST 100.000000   AS $dprs_$SELECT a FROM x LIMIT 1;$dprs_$</code></pre> <p>Более подробную информацию про настройку расширения можно найти в официальном <a href="https://github.com/2ndQuadrant/pgaudit#configuration">README</a>.</p> <h2 id=ltree>Ltree</h2> <p><a href="https://www.postgresql.org/docs/current/static/ltree.html">Ltree</a> — расширение, которое позволяет хранить древовидные структуры в виде меток, а также предоставляет широкие возможности поиска по ним.</p> <h3 id="почему-ltree">Почему Ltree?</h3> <ul> <li><p>Реализация алгоритма Materialized Path (достаточно быстрый как на запись, так и на чтение);</p></li> <li><p>Как правило данное решение будет быстрее, чем использовании CTE (Common Table Expressions) или рекурсивный функции (постоянно будут пересчитываться ветвления);</p></li> <li><p>Встроены механизмы поиска по дереву;</p></li> <li><p>Индексы;</p></li> </ul> <h3 id="установка-и-использование-5">Установка и использование</h3> <p>Для начала активируем расширение для базы данных:</p> <pre><code># CREATE EXTENSION ltree;</code></pre> <p>Далее создадим таблицу коментариев, которые будут хранится как дерево:</p> <pre><code>CREATE TABLE comments (user_id integer, description text, path ltree);
INSERT INTO comments (user_id, description, path) VALUES ( 1, md5(random()::text), &#39;0001&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 2, md5(random()::text), &#39;0001.0001.0001&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 2, md5(random()::text), &#39;0001.0001.0001.0001&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 1, md5(random()::text), &#39;0001.0001.0001.0002&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 5, md5(random()::text), &#39;0001.0001.0001.0003&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 6, md5(random()::text), &#39;0001.0002&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 6, md5(random()::text), &#39;0001.0002.0001&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 6, md5(random()::text), &#39;0001.0003&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 8, md5(random()::text), &#39;0001.0003.0001&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 9, md5(random()::text), &#39;0001.0003.0002&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 11, md5(random()::text), &#39;0001.0003.0002.0001&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 2, md5(random()::text), &#39;0001.0003.0002.0002&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 5, md5(random()::text), &#39;0001.0003.0002.0003&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 7, md5(random()::text), &#39;0001.0003.0002.0002.0001&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 20, md5(random()::text), &#39;0001.0003.0002.0002.0002&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 31, md5(random()::text), &#39;0001.0003.0002.0002.0003&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 22, md5(random()::text), &#39;0001.0003.0002.0002.0004&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 34, md5(random()::text), &#39;0001.0003.0002.0002.0005&#39;);
INSERT INTO comments (user_id, description, path) VALUES ( 22, md5(random()::text), &#39;0001.0003.0002.0002.0006&#39;);</code></pre> <p>Не забываем добавить индексы:</p> <pre><code># CREATE INDEX path_gist_comments_idx ON comments USING GIST(path);
# CREATE INDEX path_comments_idx ON comments USING btree(path);</code></pre> <p>В данном примере я создаю таблицу <code>comments</code> с полем <code>path</code>, которые и будет содержать полный путь к этому коментарию в дереве (я использую 4 цифры и точку для делителя узлов дерева). Для начала найдем все коментарии, у который путь начинается с <code>0001.0003</code>:</p> <pre><code># SELECT user_id, path FROM comments WHERE path &lt;@ &#39;0001.0003&#39;;
 user_id |           path
---------+--------------------------
       6 | 0001.0003
       8 | 0001.0003.0001
       9 | 0001.0003.0002
      11 | 0001.0003.0002.0001
       2 | 0001.0003.0002.0002
       5 | 0001.0003.0002.0003
       7 | 0001.0003.0002.0002.0001
      20 | 0001.0003.0002.0002.0002
      31 | 0001.0003.0002.0002.0003
      22 | 0001.0003.0002.0002.0004
      34 | 0001.0003.0002.0002.0005
      22 | 0001.0003.0002.0002.0006
(12 rows)</code></pre> <p>И проверим как работают индексы:</p> <pre><code># SET enable_seqscan=false;
SET
# EXPLAIN ANALYZE SELECT user_id, path FROM comments WHERE path &lt;@ &#39;0001.0003&#39;;
                                                            QUERY PLAN
-----------------------------------------------------------------------------------------------------------------------------------
 Index Scan using path_gist_comments_idx on comments  (cost=0.00..8.29 rows=2 width=38) (actual time=0.023..0.034 rows=12 loops=1)
   Index Cond: (path &lt;@ &#39;0001.0003&#39;::ltree)
 Total runtime: 0.076 ms
(3 rows)</code></pre> <p>Данную выборку можно сделать другим запросом:</p> <pre><code># SELECT user_id, path FROM comments WHERE path ~ &#39;0001.0003.*&#39;;
user_id |           path
---------+--------------------------
       6 | 0001.0003
       8 | 0001.0003.0001
       9 | 0001.0003.0002
      11 | 0001.0003.0002.0001
       2 | 0001.0003.0002.0002
       5 | 0001.0003.0002.0003
       7 | 0001.0003.0002.0002.0001
      20 | 0001.0003.0002.0002.0002
      31 | 0001.0003.0002.0002.0003
      22 | 0001.0003.0002.0002.0004
      34 | 0001.0003.0002.0002.0005
      22 | 0001.0003.0002.0002.0006
(12 rows)</code></pre> <p>Не забываем про сортировку дерева:</p> <pre><code># INSERT INTO comments (user_id, description, path) VALUES ( 9, md5(random()::text), &#39;0001.0003.0001.0001&#39;);
# INSERT INTO comments (user_id, description, path) VALUES ( 9, md5(random()::text), &#39;0001.0003.0001.0002&#39;);
# INSERT INTO comments (user_id, description, path) VALUES ( 9, md5(random()::text), &#39;0001.0003.0001.0003&#39;);
# SELECT user_id, path FROM comments WHERE path ~ &#39;0001.0003.*&#39;;
user_id |           path
---------+--------------------------
       6 | 0001.0003
       8 | 0001.0003.0001
       9 | 0001.0003.0002
      11 | 0001.0003.0002.0001
       2 | 0001.0003.0002.0002
       5 | 0001.0003.0002.0003
       7 | 0001.0003.0002.0002.0001
      20 | 0001.0003.0002.0002.0002
      31 | 0001.0003.0002.0002.0003
      22 | 0001.0003.0002.0002.0004
      34 | 0001.0003.0002.0002.0005
      22 | 0001.0003.0002.0002.0006
       9 | 0001.0003.0001.0001
       9 | 0001.0003.0001.0002
       9 | 0001.0003.0001.0003
(15 rows)
# SELECT user_id, path FROM comments WHERE path ~ &#39;0001.0003.*&#39; ORDER by path;
 user_id |           path
---------+--------------------------
       6 | 0001.0003
       8 | 0001.0003.0001
       9 | 0001.0003.0001.0001
       9 | 0001.0003.0001.0002
       9 | 0001.0003.0001.0003
       9 | 0001.0003.0002
      11 | 0001.0003.0002.0001
       2 | 0001.0003.0002.0002
       7 | 0001.0003.0002.0002.0001
      20 | 0001.0003.0002.0002.0002
      31 | 0001.0003.0002.0002.0003
      22 | 0001.0003.0002.0002.0004
      34 | 0001.0003.0002.0002.0005
      22 | 0001.0003.0002.0002.0006
       5 | 0001.0003.0002.0003
(15 rows)</code></pre> <p>Для поиска можно использовать разные модификаторы. Пример использования «или» (<code>|</code>):</p> <pre><code># SELECT user_id, path FROM comments WHERE path ~ &#39;0001.*{1,2}.0001|0002.*&#39; ORDER by path;
 user_id |           path
---------+--------------------------
       2 | 0001.0001.0001
       2 | 0001.0001.0001.0001
       1 | 0001.0001.0001.0002
       5 | 0001.0001.0001.0003
       6 | 0001.0002.0001
       8 | 0001.0003.0001
       9 | 0001.0003.0001.0001
       9 | 0001.0003.0001.0002
       9 | 0001.0003.0001.0003
       9 | 0001.0003.0002
      11 | 0001.0003.0002.0001
       2 | 0001.0003.0002.0002
       7 | 0001.0003.0002.0002.0001
      20 | 0001.0003.0002.0002.0002
      31 | 0001.0003.0002.0002.0003
      22 | 0001.0003.0002.0002.0004
      34 | 0001.0003.0002.0002.0005
      22 | 0001.0003.0002.0002.0006
       5 | 0001.0003.0002.0003
(19 rows)</code></pre> <p>Например, найдем прямых потомков от <code>0001.0003</code>:</p> <pre><code># SELECT user_id, path FROM comments WHERE path ~ &#39;0001.0003.*{1}&#39; ORDER by path;
 user_id |      path
---------+----------------
       8 | 0001.0003.0001
       9 | 0001.0003.0002
(2 rows)</code></pre> <p>Можно также найти родителя для потомка «0001.0003.0002.0002.0005»:</p> <pre><code># SELECT user_id, path FROM comments WHERE path = subpath(&#39;0001.0003.0002.0002.0005&#39;, 0, -1) ORDER by path;
 user_id |        path
---------+---------------------
       2 | 0001.0003.0002.0002
(1 row)</code></pre> <h3 id="заключение-20">Заключение</h3> <p>Ltree — расширение, которое позволяет хранить и удобно управлять Materialized Path в PostgreSQL.</p> <h2 id=postpic>PostPic</h2> <p><a href="http://drotiro.github.io/postpic/">PostPic</a> расширение для PostgreSQL, которое позволяет обрабатывать изображения в базе данных, как PostGIS делает это с пространственными данными. Он добавляет новый типа поля <code>image</code>, а также несколько функций для обработки изображений (обрезка краев, создание миниатюр, поворот и т.д.) и извлечений его атрибутов (размер, тип, разрешение). Более подробно о возможностях расширения можно ознакомится на <a href="https://github.com/drotiro/postpic/wiki/SQL-Functions-Guide">официальной странице</a>.</p> <h2 id=fuzzystrmatch>Fuzzystrmatch</h2> <p><a href="https://www.postgresql.org/docs/current/static/fuzzystrmatch.html">Fuzzystrmatch</a> расширение предоставляет несколько функций для определения сходства и расстояния между строками. Функция <code>soundex</code> используется для согласования сходно звучащих имен путем преобразования их в одинаковый код. Функция <code>difference</code> преобразует две строки в soundex код, а затем сообщает количество совпадающих позиций кода. В <code>soundex</code> код состоит из четырех символов, поэтому результат будет от нуля до четырех: 0 — не совпадают, 4 — точное совпадение (таким образом, функция названа неверно — как название лучше подходит similarity):</p> <pre><code># CREATE EXTENSION fuzzystrmatch;
CREATE EXTENSION
# SELECT soundex(&#39;hello world!&#39;);
 soundex
---------
 H464
(1 row)

# SELECT soundex(&#39;Anne&#39;), soundex(&#39;Ann&#39;), difference(&#39;Anne&#39;, &#39;Ann&#39;);
 soundex | soundex | difference
---------+---------+------------
 A500    | A500    |          4
(1 row)

# SELECT soundex(&#39;Anne&#39;), soundex(&#39;Andrew&#39;), difference(&#39;Anne&#39;, &#39;Andrew&#39;);
 soundex | soundex | difference
---------+---------+------------
 A500    | A536    |          2
(1 row)

# SELECT soundex(&#39;Anne&#39;), soundex(&#39;Margaret&#39;), difference(&#39;Anne&#39;, &#39;Margaret&#39;);
 soundex | soundex | difference
---------+---------+------------
 A500    | M626    |          0
(1 row)

# CREATE TABLE s (nm text);
CREATE TABLE
# INSERT INTO s VALUES (&#39;john&#39;), (&#39;joan&#39;), (&#39;wobbly&#39;), (&#39;jack&#39;);
INSERT 0 4
# SELECT * FROM s WHERE soundex(nm) = soundex(&#39;john&#39;);
  nm
------
 john
 joan
(2 rows)

# SELECT * FROM s WHERE difference(s.nm, &#39;john&#39;) &gt; 2;
  nm
------
 john
 joan
 jack
(3 rows)</code></pre> <p>Функция <code>levenshtein</code> вычисляет <a href="http://en.wikipedia.org/wiki/Levenshtein_distance">расстояние Левенштейна</a> между двумя строками. <code>levenshtein_less_equal</code> ускоряется функцию levenshtein для маленьких значений расстояния:</p> <pre><code># SELECT levenshtein(&#39;GUMBO&#39;, &#39;GAMBOL&#39;);
 levenshtein
-------------
           2
(1 row)

# SELECT levenshtein(&#39;GUMBO&#39;, &#39;GAMBOL&#39;, 2, 1, 1);
 levenshtein
-------------
           3
(1 row)

# SELECT levenshtein_less_equal(&#39;extensive&#39;, &#39;exhaustive&#39;, 2);
 levenshtein_less_equal
------------------------
                      3
(1 row)

test=# SELECT levenshtein_less_equal(&#39;extensive&#39;, &#39;exhaustive&#39;, 4);
 levenshtein_less_equal
------------------------
                      4
(1 row)</code></pre> <p>Функция <code>metaphone</code>, как и soundex, построена на идее создания кода для строки: две строки, которые будут считатся похожими, будут иметь одинаковые коды. Последним параметром указывается максимальная длина <code>metaphone</code> кода. Функция <code>dmetaphone</code> вычисляет два «как звучит» кода для строки — «первичный» и «альтернативный»:</p> <pre><code># SELECT metaphone(&#39;GUMBO&#39;, 4);
 metaphone
-----------
 KM
(1 row)
# SELECT dmetaphone(&#39;postgresql&#39;);
 dmetaphone
------------
 PSTK
(1 row)

# SELECT dmetaphone_alt(&#39;postgresql&#39;);
 dmetaphone_alt
----------------
 PSTK
(1 row)</code></pre> <h2 id=pg_trgm>Pg_trgm</h2> <p><a href="https://ru.wikipedia.org/wiki/%D0%90%D0%B2%D1%82%D0%BE%D0%B4%D0%BE%D0%BF%D0%BE%D0%BB%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5">Автодополнение</a> — функция в программах, предусматривающих интерактивный ввод текста по дополнению текста по введённой его части. Реализуется это простым <code>LIKE 'some%'</code> запросом в базу, где «some» — то, что пользователь успел ввести в поле ввода. Проблема в том, что в огромной таблице такой запрос будет работать очень медленно. Для ускорения запроса типа <code>LIKE 'bla%'</code> можно использовать <code>text_pattern_ops</code> для <code>text</code> поля или <code>varchar_pattern_ops</code> для <code>varchar</code> поля класс операторов в определении индекса (данные типы индексов не будут работать для стандартных операторов <code>&lt;</code>, <code>&lt;=</code>, <code>=&gt;</code>, <code>&gt;</code> и для работы с ними придется создать обычный btree индекс).</p> <pre><code># create table tags (
#  tag    text primary key,
#  name      text not null,
#  shortname text,
#  status    char default &#39;S&#39;,
#
#  check( status in (&#39;S&#39;, &#39;R&#39;) )
# );
NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index &quot;tags_pkey&quot; for table &quot;tags&quot;
CREATE TABLE

# CREATE INDEX i_tag ON tags USING btree(lower(tag) text_pattern_ops);
CREATE INDEX

# EXPLAIN ANALYZE select * from tags where lower(tag) LIKE lower(&#39;0146%&#39;);
                                                      QUERY PLAN
-----------------------------------------------------------------------------------------------------------------------
 Bitmap Heap Scan on tags  (cost=5.49..97.75 rows=121 width=26) (actual time=0.025..0.025 rows=1 loops=1)
   Filter: (lower(tag) ~~ &#39;0146%&#39;::text)
   -&gt;  Bitmap Index Scan on i_tag (cost=0.00..5.46 rows=120 width=0) (actual time=0.016..0.016 rows=1 loops=1)
         Index Cond: ((lower(tag) ~&gt;=~ &#39;0146&#39;::text) AND (lower(tag) ~&lt;~ &#39;0147&#39;::text))
 Total runtime: 0.050 ms
(5 rows)</code></pre> <p>Для более сложных вариантов поиска, таких как <code>LIKE '%some%'</code> или <code>LIKE 'so%me%'</code> такой индекс не будет работать, но эту проблему можно решить через расширение.</p> <p><a href="https://www.postgresql.org/docs/current/static/pgtrgm.html">Pg_trgm</a> — PostgreSQL расширение, которое предоставляет функции и операторы для определения схожести алфавитно-цифровых строк на основе триграмм, а также классы операторов индексов, поддерживающие быстрый поиск схожих строк. Триграмма — это группа трёх последовательных символов, взятых из строки. Можно измерить схожесть двух строк, подсчитав число триграмм, которые есть в обеих. Эта простая идея оказывается очень эффективной для измерения схожести слов на многих естественных языках. Модуль <code>pg_trgm</code> предоставляет классы операторов индексов GiST и GIN, позволяющие создавать индекс по текстовым колонкам для очень быстрого поиска по критерию схожести. Эти типы индексов поддерживают <code>%</code> и <code>&lt;-&gt;</code> операторы схожести и дополнительно поддерживают поиск на основе триграмм для запросов с <code>LIKE</code>, <code>ILIKE</code>, <code>~</code> и <code>~*</code> (эти индексы не поддерживают простые операторы сравнения и равенства, так что может понадобиться и обычный btree индекс).</p> <pre><code># CREATE TABLE test_trgm (t text);
# CREATE INDEX trgm_idx ON test_trgm USING gist (t gist_trgm_ops);
-- or
# CREATE INDEX trgm_idx ON test_trgm USING gin (t gin_trgm_ops);</code></pre> <p>После создания GIST или GIN индекса по колонке <code>t</code> можно осуществлять поиск по схожести. Пример запроса:</p> <pre><code>SELECT t, similarity(t, &#39;word&#39;) AS sml
  FROM test_trgm
  WHERE t % &#39;word&#39;
  ORDER BY sml DESC, t;</code></pre> <p>Он выдаст все значения в текстовой колонке, которые достаточно схожи со словом <code>word</code>, в порядке сортировки от наиболее к наименее схожим. Другой вариант предыдущего запроса (может быть довольно эффективно выполнен с применением индексов GiST, а не GIN):</p> <pre><code>SELECT t, t &lt;-&gt; &#39;word&#39; AS dist
  FROM test_trgm
  ORDER BY dist LIMIT 10;</code></pre> <p>Начиная с PostgreSQL 9.1, эти типы индексов также поддерживают поиск с операторами <code>LIKE</code> и <code>ILIKE</code>, например:</p> <pre><code>SELECT * FROM test_trgm WHERE t LIKE &#39;%foo%bar&#39;;</code></pre> <p>Начиная с PostgreSQL 9.3, индексы этих типов также поддерживают поиск по регулярным выражениям (операторы <code>~</code> и <code>~*</code>), например:</p> <pre><code>SELECT * FROM test_trgm WHERE t ~ &#39;(foo|bar)&#39;;</code></pre> <p>Относительно поиска по регулярному выражению или с <code>LIKE</code>, нужно принимать в расчет, что при отсутствии триграмм в искомом шаблоне поиск сводится к полному сканирования индекса. Выбор между индексами GiST и GIN зависит от относительных характеристик производительности GiST и GIN, которые здесь не рассматриваются. Как правило, индекс GIN быстрее индекса GiST при поиске, но строится или обновляется он медленнее; поэтому GIN лучше подходит для статических, а GiST для часто изменяемых данных.</p> <h2 id=cstore_fdw>Cstore_fdw</h2> <p><a href="https://citusdata.github.io/cstore_fdw/">Cstore_fdw</a> расширение реализует модель хранения данных на базе <a href="https://en.wikipedia.org/wiki/Column-oriented_DBMS">семейства столбцов</a> (column-oriented systems) для PostgreSQL (колоночное хранение данных). Такое хранение данных обеспечивает заметные преимущества для аналитических задач (<a href="https://ru.wikipedia.org/wiki/OLAP">OLAP</a>, <a href="https://en.wikipedia.org/wiki/Data_warehouse">data warehouse</a>), поскольку требуется считывать меньше данных с диска (благодаря формату хранения и компресии). Расширение использует <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC#LanguageManualORC-ORCFileFormat">Optimized Row Columnar (ORC)</a> формат для размещения данных на диске, который имеет следующие преимущества:</p> <ul> <li><p>Уменьшение (сжатие) размера данных в памяти и на диске в 2-4 раза. Можно добавить в расширение другой кодек для сжатия (алгоритм Лемпеля-Зива, LZ присутствует в расширении);</p></li> <li><p>Считывание с диска только тех данных, которые требуются. Повышается производительность по I/O диска для других запросов;</p></li> <li><p>Хранение минимального/максимального значений для групп полей (skip index, индекс с пропусками), что помогает пропустить не требуемые данные на диске при выборке;</p></li> </ul> <h3 id="установка-и-использование-6">Установка и использование</h3> <p>Для работы <code>cstore_fdw</code> требуется <a href="https://github.com/protobuf-c/protobuf-c">protobuf-c</a> для сериализации и десериализации данных. Далее требуется добавить в <code>postgresql.conf</code> расширение:</p> <pre><code>shared_preload_libraries = &#39;cstore_fdw&#39;</code></pre> <p>И активировать его для базы:</p> <pre><code># CREATE EXTENSION cstore_fdw;</code></pre> <p>Для загрузки данных в cstore таблицы существует два варианта:</p> <ul> <li><p>Использование команды <code>COPY</code> для загрузки или добаления данных из файлов или STDIN;</p></li> <li><p>Использование конструкции <code>INSERT INTO cstore_table SELECT ...</code> для загрузки или добаления данных из другой таблицы;</p></li> </ul> <p>Cstore таблицы не поддерживают <code>INSERT</code> (кроме выше упомянутого <code>INSERT INTO ... SELECT</code>), <code>UPDATE</code> или <code>DELETE</code> команды.</p> <p>Для примера загрузим тестовые данные:</p> <pre><code>$ wget http://examples.citusdata.com/customer_reviews_1998.csv.gz
$ wget http://examples.citusdata.com/customer_reviews_1999.csv.gz

$ gzip -d customer_reviews_1998.csv.gz
$ gzip -d customer_reviews_1999.csv.gz</code></pre> <p>Далее загрузим эти данные в cstore таблицу (расширение уже активировано для PostgreSQL):</p> <pre><code>-- create server object
CREATE SERVER cstore_server FOREIGN DATA WRAPPER cstore_fdw;

-- create foreign table
CREATE FOREIGN TABLE customer_reviews
(
    customer_id TEXT,
    review_date DATE,
    review_rating INTEGER,
    review_votes INTEGER,
    review_helpful_votes INTEGER,
    product_id CHAR(10),
    product_title TEXT,
    product_sales_rank BIGINT,
    product_group TEXT,
    product_category TEXT,
    product_subcategory TEXT,
    similar_product_ids CHAR(10)[]
)
SERVER cstore_server
OPTIONS(compression &#39;pglz&#39;);

COPY customer_reviews FROM &#39;/tmp/customer_reviews_1998.csv&#39; WITH CSV;
COPY customer_reviews FROM &#39;/tmp/customer_reviews_1999.csv&#39; WITH CSV;

ANALYZE customer_reviews;</code></pre> <p>После этого можно проверить как работает расширение:</p> <pre><code>-- Find all reviews a particular customer made on the Dune series in 1998.
# SELECT
    customer_id, review_date, review_rating, product_id, product_title
FROM
    customer_reviews
WHERE
    customer_id =&#39;A27T7HVDXA3K2A&#39; AND
    product_title LIKE &#39;%Dune%&#39; AND
    review_date &gt;= &#39;1998-01-01&#39; AND
    review_date &lt;= &#39;1998-12-31&#39;;
  customer_id   | review_date | review_rating | product_id |                 product_title
----------------+-------------+---------------+------------+-----------------------------------------------
 A27T7HVDXA3K2A | 1998-04-10  |             5 | 0399128964 | Dune (Dune Chronicles (Econo-Clad Hardcover))
 A27T7HVDXA3K2A | 1998-04-10  |             5 | 044100590X | Dune
 A27T7HVDXA3K2A | 1998-04-10  |             5 | 0441172717 | Dune (Dune Chronicles, Book 1)
 A27T7HVDXA3K2A | 1998-04-10  |             5 | 0881036366 | Dune (Dune Chronicles (Econo-Clad Hardcover))
 A27T7HVDXA3K2A | 1998-04-10  |             5 | 1559949570 | Dune Audio Collection
(5 rows)

Time: 238.626 ms

-- Do we have a correlation between a book&#39;s title&#39;s length and its review ratings?
# SELECT
    width_bucket(length(product_title), 1, 50, 5) title_length_bucket,
    round(avg(review_rating), 2) AS review_average,
    count(*)
FROM
   customer_reviews
WHERE
    product_group = &#39;Book&#39;
GROUP BY
    title_length_bucket
ORDER BY
    title_length_bucket;
 title_length_bucket | review_average | count
---------------------+----------------+--------
                   1 |           4.26 | 139034
                   2 |           4.24 | 411318
                   3 |           4.34 | 245671
                   4 |           4.32 | 167361
                   5 |           4.30 | 118422
                   6 |           4.40 | 116412
(6 rows)

Time: 1285.059 ms</code></pre> <h3 id="заключение-21">Заключение</h3> <p>Более подробно о использовании расширения можно ознакомиться через <a href="https://citusdata.github.io/cstore_fdw/">официальную документацию</a>.</p> <h2 id=postgresql-hll>Postgresql-hll</h2> <p>На сегодняшний день широко распространена задача подсчета количества уникальных элементов (count-distinct problem) в потоке данных, которые могут содержать повторяющиеся элементы. Например, сколько уникальных IP-адресов подключалось к серверу за последний час? Сколько различных слов в большом куске текстов? Какое количество уникальных посетителей побывало на популярном сайте за день? Сколько уникальных URL было запрошено через прокси-сервер? Данную задачу можно решить «в лоб»: пройтись по всем элементам и убрать дубликаты, после этого посчитать их количество (например использовать множество, set). Трудности в таком подходе возникают при увеличении масштаба. С минимальными затратами можно подсчитать тысячу или даже миллион уникальных посетителей, IP-адресов, URL или слов. А что если речь идет о 100 миллионах уникальных элементов на один сервер при наличии тысяч серверов? Теперь это уже становится интересным.</p> <p>Текущее решение проблемы будет выглядеть так: необходимо сформировать множества (set) уникальных элементов для каждого из 1000 серверов, каждое из которых может содержать около 100 миллионов уникальных элементов, а затем подсчитать количество уникальных элементов в объединении этих множеств. Другими словами, мы имеем дело с распределенным вариантом задачи подсчета уникальных элементов. Хоть это решение является вполне логичным, на практике этот подход обойдется высокой ценой. Для примера возьмем URL, средняя длина которого составляет 76 символов. В нашем случае один сервер обслуживает около 100 миллионов уникальных URL, следовательно, размер файла с их перечнем составит около 7.6 ГБ. Даже если каждый URL преобразовать в 64-битный хеш, размер файла составит 800 МБ. Это намного лучше, но не забывайте, что речь идет о 1000 серверов. Каждый сервер отправляет файл с перечнем уникальных URL на центральный сервер, следовательно, при наличии 1000 серверов функция объединения множеств должна обработать 800 ГБ данных. Если такая операция должна выполняться часто, тогда необходимо будет либо установить систему для обработки больших данных (и нанять команду для ее обслуживания), либо найти другое решение.</p> <p>И вот на сцену выходит <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a> алгоритм. Этот алгоритм реализует вероятностный подход к задаче подсчета уникальных элементов и базируется на двух следующих положениях:</p> <ul> <li><p>вероятность того, что любой данный бит двоичного представления случайного числа равен единице, составляет 50%;</p></li> <li><p>вероятность того, что совместно произойдут два независимых случайных события <span class=LaTeX>$A$</span> и <span class=LaTeX>$B$</span>, вычисляется по формуле <span class=LaTeX>$P(A)*P(B)$</span>. Таким образом, если вероятность равенства единице одного любого бита случайного числа составляет 50%, тогда вероятность равенства единице двух любых битов составляет 25%, трех — 12,5% и т.д;</p></li> </ul> <p>Вспомним еще одно базовое положение теории вероятностей, согласно которому ожидаемое количество испытаний, необходимое для наступления события, вычисляется по формуле <span class=LaTeX>$1/P(event)$</span>. Следовательно, если <span class=LaTeX>$P(one\ specific\ bit\ set) = 50\%$</span>, то ожидаемое количество испытаний равно 2. Для двух битов — 4, для трех битов — 8 и т.д.</p> <p>В общем случае входные значения не являются равномерно распределенными случайными числами, поэтому необходим способ преобразования входных значений к равномерному распределению, т.е. необходима хеш-функция. Обратите внимание, в некоторых случаях распределение, получаемое на выходе хеш-функции, не оказывает существенное влияние на точность системы. Однако HyperLogLog очень чувствителен в этом отношении. Если выход хеш-функции не соответствует равномерному распределению, алгоритм теряет точность, поскольку не выполняются базовые допущения, лежащие в его основе.</p> <p>Рассмотрим алгоритм подробно. Вначале необходимо хешировать все элементы исследуемого набора. Затем нужно подсчитать количество последовательных начальных битов, равных единице, в двоичном представлении каждого хеша и определить максимальное значение этого количества среди всех хешей. Если максимальное количество единиц обозначить <span class=LaTeX>$n$</span>, тогда количество уникальных элементов в наборе можно оценить, как <span class=LaTeX>$2^n$</span>. То есть, если максимум один начальный бит равен единице, тогда количество уникальных элементов, в среднем, равно 2; если максимум три начальных бита равны единице, в среднем, мы можем ожидать 8 уникальных элементов и т.д.</p> <p>Подход, направленный на повышение точности оценки и являющийся одной из ключевых идей HyperLogLog, заключается в следующем: разделяем хеши на подгруппы на основании их конечных битов, определяем максимальное количество начальных единиц в каждой подгруппе, а затем находим среднее. Этот подход позволяет получить намного более точную оценку общего количества уникальных элементов. Если мы имеем <span class=LaTeX>$m$</span> подгрупп и <span class=LaTeX>$n$</span> уникальных элементов, тогда, в среднем, в каждой подгруппе будет <span class=LaTeX>$n/m$</span> уникальных элементов. Таким образом, нахождение среднего по всем подгруппам дает достаточно точную оценку величины <span class=LaTeX>$log_2{(n/m)}$</span>, а отсюда легко можно получить необходимое нам значение. Более того, HyperLogLog позволяет обрабатывать по отдельности различные варианты группировок, а затем на основе этих данных находить итоговую оценку. Следует отметить, что для нахождения среднего HyperLogLog использует среднее гармоническое, которое обеспечивает лучшие результаты по сравнению со средним арифметическим (более подробную информацию можно найти в оригинальных публикациях, посвященных <a href="http://www.ic.unicamp.br/~celio/peer2peer/math/bitmap-algorithms/durand03loglog.pdf">LogLog</a> и <a href="http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf">HyperLogLog</a>).</p> <p>Вернемся к задаче. По условию существует 1000 серверов и 100 миллионов уникальных URL на каждый сервер, следовательно, центральный сервер должен обрабатывать 800 ГБ данных при каждом выполнении простого варианта алгоритма. Это также означает, что 800 ГБ данных каждый раз необходимо передавать по сети. HyperLogLog меняет ситуацию кардинально. Согласно анализу, проведенному авторами оригинальной публикации, HyperLogLog обеспечивает точность около 98% при использовании всего 1.5 КБ памяти. Каждый сервер формирует соответствующий файл размером 1.5 КБ, а затем отправляет его на центральный сервер. При наличии 1000 серверов, центральный сервер обрабатывает всего 1.5 МБ данных при каждом выполнении алгоритма. Другими словами, обрабатывается лишь 0.0002% данных по сравнению с предыдущим решением. Это полностью меняет экономический аспект задачи. Благодаря HyperLogLog, возможно выполнять такие операции чаще и в большем количестве. И все это ценой всего лишь 2% погрешности.</p> <p>Для работы с этим алгоритмом внутри PostgreSQL было создано расширение <a href="https://github.com/aggregateknowledge/postgresql-hll">postgresql-hll</a>. Оно добавляет новый тип поля <code>hll</code>, который представляет собой HyperLogLog структуру данных. Рассмотрим пример его использования.</p> <h3 id="установка-и-использование-7">Установка и использование</h3> <p>Для начала инициализируем расширение в базе данных:</p> <pre><code># CREATE EXTENSION hll;</code></pre> <p>Давайте предположим, что есть таблица <code>users_visits</code>, которая записывает визиты пользователей на сайт, что они сделали и откуда они пришли. В таблице сотни миллионов строк.</p> <pre><code>CREATE TABLE users_visits (
  date            date,
  user_id         integer,
  activity_type   smallint,
  referrer        varchar(255)
);</code></pre> <p>Требуется получать очень быстро представление о том, сколько уникальных пользователей посещают сайт в день на админ панеле. Для этого создадим агрегатную таблицу:</p> <pre><code>CREATE TABLE daily_uniques (
  date            date UNIQUE,
  users           hll
);

-- Fill it with the aggregated unique statistics
INSERT INTO daily_uniques(date, users)
    SELECT date, hll_add_agg(hll_hash_integer(user_id))
    FROM users_visits
    GROUP BY 1;</code></pre> <p>Далее хэшируется <code>user_id</code> и собираются эти хэш-значения в один <code>hll</code> за день. Теперь можно запросить информацию по уникальным пользователям за каждый день:</p> <pre><code># SELECT date, hll_cardinality(users) FROM daily_uniques;
    date    | hll_cardinality
------------+-----------------
 2017-02-21 |            23123
 2017-02-22 |            59433
 2017-02-23 |          2134890
 2017-02-24 |          3276247
(4 rows)</code></pre> <p>Можно возразить, что такую задачу можно решить и через <code>COUNT DISTINCT</code> и это будет верно. Но в примере только ответили на вопрос: «Сколько уникальных пользователей посещает сайт каждый день?». А что, если требуется знать сколько уникальных пользователей посетили сайт за неделю?</p> <pre><code>SELECT hll_cardinality(hll_union_agg(users)) FROM daily_uniques WHERE date &gt;= &#39;2017-02-20&#39;::date AND date &lt;= &#39;2017-02-26&#39;::date;</code></pre> <p>Или выбрать уникальных пользователей за каждый месяц в течении года?</p> <pre><code>SELECT EXTRACT(MONTH FROM date) AS month, hll_cardinality(hll_union_agg(users))
FROM daily_uniques
WHERE date &gt;= &#39;2016-01-01&#39; AND
      date &lt;  &#39;2017-01-01&#39;
GROUP BY 1;</code></pre> <p>Или узнать количество пользователей, что посетили сайт вчера, но не сегодня?</p> <pre><code>SELECT date, (#hll_union_agg(users) OVER two_days) - #users AS lost_uniques
FROM daily_uniques
WINDOW two_days AS (ORDER BY date ASC ROWS 1 PRECEDING);</code></pre> <p>Это всего пара примеров типов запросов, которые будут возвращать результат в течении миллисекунд благодаря <code>hll</code>, но потребует либо полностью отдельные предварительно созданные агрегирующие таблицы или <code>self join/generate_series</code> фокусы в <code>COUNT DISTINCT</code> мире.</p> <h3 id="заключение-22">Заключение</h3> <p>Более подробно о использовании расширения можно ознакомиться через <a href="https://github.com/aggregateknowledge/postgresql-hll/blob/master/README.markdown">официальную документацию</a>.</p> <h2 id=tsearch2>Tsearch2</h2> <p>Как и многие современные СУБД, PostgreSQL имеет встроенный механизм полнотекстового поиска. Отметим, что операторы поиска по текстовым данных существовали очень давно, это операторы <code>LIKE</code>, <code>ILIKE</code>, <code>~</code>, <code>~*</code>. Однако, они не годились для эффективного полнотекстового поиска, так как:</p> <ul> <li><p>У них не было лингвистической поддержки, например, при поиске слова <code>satisfies</code> будут не найдены документы со словом <code>satisfy</code> и никакими регулярными выражениями этому не помочь. В принципе, используя <code>OR</code> и все формы слова, можно найти все необходимые документы, но это очень неэффективно, так как в некоторых языках могут быть слова со многими тысячами форм!;</p></li> <li><p>Они не предоставляют никакой информации для ранжирования (сортировки) документов, что делает такой поиск практически бесполезным, если только не существует другой сортировки или в случае малого количества найденных документов;</p></li> <li><p>Они, в целом, очень медленные из-за того, что они каждый раз просматривают весь документ и не имеют индексной поддержки;</p></li> </ul> <p>Для улучшения ситуации Олег Бартунов и Федор Сигаев предложили и реализовали новый полнотекстовый поиск, существовавший как модуль расширения и интегрированный в PostgreSQL, начиная с версии 8.3 — <a href="https://www.postgresql.org/docs/current/static/tsearch2.html">Tsearch2</a>.</p> <p>Идея нового поиска состояла в том, чтобы затратить время на обработку документа один раз и сохранить время при поиске, использовать специальные программы-словари для нормализации слов, чтобы не заботиться, например, о формах слов, учитывать информацию о важности различных атрибутов документа и положения слова из запроса в документе для ранжирования найденных документов. Для этого, требовалось создать новые типы данных, соответствующие документу и запросу, и полнотекстовый оператор для сравнения документа и запроса, который возвращает <code>TRUE</code>, если запрос удовлетворяет запросу, и в противном случае - <code>FALSE</code>.</p> <p>PostgreSQL предоставляет возможность как для создания новых типов данных, операторов, так и создания индексной поддержки для доступа к ним, причем с поддержкой конкурентности и восстановления после сбоев. Однако, надо понимать, что индексы нужны только ускорения поиска, сам поиск обязан работать и без них. Таким образом, были созданы новые типы данных - <code>tsvector</code>, который является хранилищем для лексем из документа, оптимизированного для поиска, и <code>tsquery</code> - для запроса с поддержкой логических операций, полнотекстовый оператор «две собаки» <code>@@</code> и индексная поддержка для него с использованием GiST и GIN. <code>tsvector</code> помимо самих лексем может хранить информацию о положении лексемы в документе и ее весе (важности), которая потом может использоваться для вычисления ранжирующей информации.</p> <h3 id="установка-и-использование-8">Установка и использование</h3> <p>Для начала активируем расширение:</p> <pre><code># CREATE EXTENSION tsearch2;</code></pre> <p>Проверим его работу:</p> <pre><code># SELECT &#39;This is test string&#39;::tsvector;
          tsvector
-----------------------------
 &#39;This&#39; &#39;is&#39; &#39;string&#39; &#39;test&#39;
(1 row)

# SELECT strip(to_tsvector(&#39;The air smells of sea water.&#39;));
            strip
-----------------------------
 &#39;air&#39; &#39;sea&#39; &#39;smell&#39; &#39;water&#39;
(1 row)</code></pre> <h3 id="заключение-23">Заключение</h3> <p>Данное расширение заслуживает отдельной книги, поэтому лучше ознакомится о нем подробнее в «<a href="http://www.sai.msu.su/~megera/postgres/talks/fts_pgsql_intro.html">Введение в полнотекстовый поиск в PostgreSQL</a>» документе.</p> <h2 id=plproxy>PL/Proxy</h2> <p><a href="http://pgfoundry.org/projects/plproxy/">PL/Proxy</a> представляет собой прокси-язык для удаленного вызова процедур и партицирования данных между разными базами (шардинг). Подробнее можно почитать в «[sec:plproxy] » главе.</p> <h2 id=texcaller>Texcaller</h2> <p><a href="http://www.profv.de/texcaller/">Texcaller</a> — это удобный интерфейс для командной строки <a href="https://ru.wikipedia.org/wiki/TeX">TeX</a>, который обрабатывает все виды ошибок. Он написан в простом C, довольно портативный, и не имеет внешних зависимостей, кроме TeX. Неверный TeX документ обрабатывается путем простого возвращения NULL, а не прерывается с ошибкой. В случае неудачи, а также в случае успеха, дополнительная обработка информации осуществляется через NOTICEs.</p> <h2 id=pgmemcache>Pgmemcache</h2> <p><a href="http://pgfoundry.org/projects/pgmemcache/">Pgmemcache</a> — это PostgreSQL API библиотека на основе libmemcached для взаимодействия с memcached. С помощью данной библиотеки PostgreSQL может записывать, считывать, искать и удалять данные из memcached. Подробнее можно почитать в «[sec:pgmemcache] » главе.</p> <h2 id=prefix>Prefix</h2> <p><a href="http://pgfoundry.org/projects/prefix">Prefix</a> реализует поиск текста по префиксу (<code>prefix @&gt; text</code>). Prefix используется в приложениях телефонии, где маршрутизация вызовов и расходы зависят от вызывающего/вызываемого префикса телефонного номера оператора.</p> <h3 id="установка-и-использование-9">Установка и использование</h3> <p>Для начала инициализируем расширение в базе данных:</p> <pre><code># CREATE EXTENSION prefix;</code></pre> <p>После этого можем проверить, что расширение функционирует:</p> <pre><code># select &#39;123&#39;::prefix_range @&gt; &#39;123456&#39;;
 ?column?
----------
 t
(1 row)

# select a, b, a | b as union, a &amp; b as intersect
  from  (select a::prefix_range, b::prefix_range
           from (values(&#39;123&#39;, &#39;123&#39;),
                       (&#39;123&#39;, &#39;124&#39;),
                       (&#39;123&#39;, &#39;123[4-5]&#39;),
                       (&#39;123[4-5]&#39;, &#39;123[2-7]&#39;),
                       (&#39;123&#39;, &#39;[2-3]&#39;)) as t(a, b)
        ) as x;

    a     |    b     |  union   | intersect
----------+----------+----------+-----------
 123      | 123      | 123      | 123
 123      | 124      | 12[3-4]  |
 123      | 123[4-5] | 123      | 123[4-5]
 123[4-5] | 123[2-7] | 123[2-7] | 123[4-5]
 123      | [2-3]    | [1-3]    |
(5 rows)</code></pre> <p>В примере [lst:pgprefixexample2] производится поиск мобильного оператора по номеру телефона:</p> <pre><code>$ wget https://github.com/dimitri/prefix/raw/master/prefixes.fr.csv
$ psql

# create table prefixes (
       prefix    prefix_range primary key,
       name      text not null,
       shortname text,
       status    char default &#39;S&#39;,

       check( status in (&#39;S&#39;, &#39;R&#39;) )
);
CREATE TABLE
# comment on column prefixes.status is &#39;S:   - R: reserved&#39;;
COMMENT
# \copy prefixes from &#39;prefixes.fr.csv&#39; with delimiter &#39;;&#39; csv quote &#39;&quot;&#39;
COPY 11966
# create index idx_prefix on prefixes using gist(prefix);
CREATE INDEX
# select * from prefixes limit 10;
 prefix |                            name                            | shortname | status
--------+------------------------------------------------------------+-----------+--------
 010001 | COLT TELECOMMUNICATIONS FRANCE                             | COLT      | S
 010002 | EQUANT France                                              | EQFR      | S
 010003 | NUMERICABLE                                                | NURC      | S
 010004 | PROSODIE                                                   | PROS      | S
 010005 | INTERNATIONAL TELECOMMUNICATION NETWORK France (Vivaction) | ITNF      | S
 010006 | SOCIETE FRANCAISE DU RADIOTELEPHONE                        | SFR       | S
 010007 | SOCIETE FRANCAISE DU RADIOTELEPHONE                        | SFR       | S
 010008 | BJT PARTNERS                                               | BJTP      | S
 010009 | LONG PHONE                                                 | LGPH      | S
 010010 | IPNOTIC TELECOM                                            | TLNW      | S
(10 rows)

# select * from prefixes where prefix @&gt; &#39;0146640123&#39;;
 prefix |      name      | shortname | status
--------+----------------+-----------+--------
 0146   | FRANCE TELECOM | FRTE      | S
(1 row)

# select * from prefixes where prefix @&gt; &#39;0100091234&#39;;
 prefix |    name    | shortname | status
--------+------------+-----------+--------
 010009 | LONG PHONE | LGPH      | S
(1 row)</code></pre> <h3 id="заключение-24">Заключение</h3> <p>Более подробно о использовании расширения можно ознакомиться через <a href="https://github.com/dimitri/prefix/blob/master/README.md">официальную документацию</a>.</p> <h2 id=dblink>Dblink</h2> <p><a href="https://www.postgresql.org/docs/current/static/dblink.html">Dblink</a> – расширение, которое позволяет выполнять запросы к удаленным базам данных непосредственно из SQL, не прибегая к помощи внешних скриптов.</p> <h3 id="установка-и-использование-10">Установка и использование</h3> <p>Для начала инициализируем расширение в базе данных:</p> <pre><code># CREATE EXTENSION dblink;</code></pre> <p>Для создания подключения к другой базе данных нужно использовать <code>dblink_connect</code> функцию, где первым параметром указывается имя подключения, а вторым - опции подключения к базе:</p> <pre><code># SELECT dblink_connect(&#39;slave_db&#39;, &#39;host=slave.example.com port=5432 dbname=exampledb user=admin password=password&#39;);
 dblink_connect
----------------
 OK
(1 row)</code></pre> <p>При успешном выполнении команды будет выведен ответ «OK». Теперь можно попробовать считать данные из таблиц через <code>dblink</code> функцию:</p> <pre><code># SELECT *
FROM dblink(&#39;slave_db&#39;, &#39;SELECT id, username FROM users LIMIT 3&#39;)
AS dblink_users(id integer, username text);

 id |             username
----+----------------------------------
  1 | 64ec7083d7facb7c5d97684e7f415b65
  2 | 404c3b639a920b5ba814fc01353368f2
  3 | 153041f992e3eab6891f0e8da9d11f23
(3 rows)</code></pre> <p>По завершению работы с сервером, подключение требуется закрыть через функцию <code>dblink_disconnect</code>:</p> <pre><code># SELECT dblink_disconnect(&#39;slave_db&#39;);
 dblink_disconnect
-------------------
 OK
(1 row)</code></pre> <h3 id="курсоры">Курсоры</h3> <p>Dblink поддерживает <a href="https://www.postgresql.org/docs/current/static/plpgsql-cursors.html">курсоры</a> — инкапсулирующие запросы, которые позволяют получать результат запроса по нескольку строк за раз. Одна из причин использования курсоров заключается в том, чтобы избежать переполнения памяти, когда результат содержит большое количество строк.</p> <p>Для открытия курсора используется функция <code>dblink_open</code>, где первый параметр - название подключения, второй - название для курсора, а третий - сам запрос:</p> <pre><code># SELECT dblink_open(&#39;slave_db&#39;, &#39;users&#39;, &#39;SELECT id, username FROM users&#39;);
 dblink_open
-------------
 OK
(1 row)</code></pre> <p>Для получения данных из курсора требуется использовать <code>dblink_fetch</code>, где первый параметр - название подключения, второй - название для курсора, а третий - требуемое количество записей из курсора:</p> <pre><code># SELECT id, username FROM dblink_fetch(&#39;slave_db&#39;, &#39;users&#39;, 2)
AS (id integer, username text);
 id |             username
----+----------------------------------
  1 | 64ec7083d7facb7c5d97684e7f415b65
  2 | 404c3b639a920b5ba814fc01353368f2
(2 rows)

# SELECT id, username FROM dblink_fetch(&#39;slave_db&#39;, &#39;users&#39;, 2)
AS (id integer, username text);
 id |             username
----+----------------------------------
  3 | 153041f992e3eab6891f0e8da9d11f23
  4 | 318c33458b4840f90d87ee4ea8737515
(2 rows)

# SELECT id, username FROM dblink_fetch(&#39;slave_db&#39;, &#39;users&#39;, 2)
AS (id integer, username text);
 id |             username
----+----------------------------------
  6 | 5b795b0e73b00220843f82c4d0f81f37
  8 | c2673ee986c23f62aaeb669c32261402
(2 rows)</code></pre> <p>После работы с курсором его нужно обязательно закрыть через <code>dblink_close</code> функцию:</p> <pre><code># SELECT dblink_close(&#39;slave_db&#39;, &#39;users&#39;);
 dblink_close
--------------
 OK
(1 row)</code></pre> <h3 id="асинхронные-запросы">Асинхронные запросы</h3> <p>Последним вариантом для выполнения запросов в dblink является асинхронный запрос. При его использовании результаты не будут возвращены до полного выполнения результата запроса. Для создания асинхронного запроса используется <code>dblink_send_query</code> функция:</p> <pre><code># SELECT * FROM dblink_send_query(&#39;slave_db&#39;, &#39;SELECT id, username FROM users&#39;) AS users;
 users
-------
    1
(1 row)</code></pre> <p>Результат получается через <code>dblink_get_result</code> функцию:</p> <pre><code># SELECT id, username FROM dblink_get_result(&#39;slave_db&#39;)
AS (id integer, username text);
 id   |             username
------+----------------------------------
    1 | 64ec7083d7facb7c5d97684e7f415b65
    2 | 404c3b639a920b5ba814fc01353368f2
    3 | 153041f992e3eab6891f0e8da9d11f23
    4 | 318c33458b4840f90d87ee4ea8737515
    6 | 5b795b0e73b00220843f82c4d0f81f37
    8 | c2673ee986c23f62aaeb669c32261402
    9 | c53f14040fef954cd6e73b9aa2e31d0e
   10 | 2dbe27fd96cdb39f01ce115cf3c2a517</code></pre> <h2 id=postgres_fdw>Postgres_fdw</h2> <p><a href="https://www.postgresql.org/docs/current/static/postgres-fdw.html">Postgres_fdw</a> — расширение, которое позволяет подключить PostgreSQL к PostgreSQL, которые могут находится на разных хостах.</p> <h3 id="установка-и-использование-11">Установка и использование</h3> <p>Для начала инициализируем расширение в базе данных:</p> <pre><code># CREATE EXTENSION postgres_fdw;</code></pre> <p>Далее создадим сервер подключений, который будет содержать данные для подключения к другой PostgreSQL базе:</p> <pre><code># CREATE SERVER slave_db
FOREIGN DATA WRAPPER postgres_fdw
OPTIONS (host &#39;slave.example.com&#39;, dbname &#39;exampledb&#39;, port &#39;5432&#39;);</code></pre> <p>После этого нужно создать <code>USER MAPPING</code>, которое создаёт сопоставление пользователя на внешнем сервере:</p> <pre><code># CREATE USER MAPPING FOR admin
SERVER slave_db
OPTIONS (user &#39;admin&#39;, password &#39;password&#39;);</code></pre> <p>Теперь можно импортировать таблицы:</p> <pre><code># CREATE FOREIGN TABLE fdw_users (
  id             serial,
  username       text not null,
  password       text,
  created_on     timestamptz not null,
  last_logged_on timestamptz not null
)
SERVER slave_db
OPTIONS (schema_name &#39;public&#39;, table_name &#39;users&#39;);</code></pre> <p>Для того, чтобы не импортировать каждую таблицу отдельно, можно воспользоваться <code>IMPORT FOREIGN SCHEMA</code> командой:</p> <pre><code># IMPORT FOREIGN SCHEMA public
LIMIT TO (users, pages)
FROM SERVER slave_db INTO fdw;</code></pre> <p>Теперь можно проверить таблицы:</p> <pre><code># SELECT * FROM fdw_users LIMIT 1;
-[ RECORD 1 ]--+---------------------------------
id             | 1
username       | 64ec7083d7facb7c5d97684e7f415b65
password       | b82af3966b49c9ef0f7829107db642bc
created_on     | 2017-02-21 05:07:25.619561+00
last_logged_on | 2017-02-19 21:03:35.651561+00</code></pre> <p>По умолчанию, из таблиц можно не только читать, но и изменять в них данные (<code>INSERT/UPDATE/DELETE</code>). <code>updatable</code> опция может использовать для подключения к серверу в режиме «только на чтение»:</p> <pre><code># ALTER SERVER slave_db
OPTIONS (ADD updatable &#39;false&#39;);
ALTER SERVER
# DELETE FROM fdw_users WHERE id &lt; 10;
ERROR:  foreign table &quot;fdw_users&quot; does not allow deletes</code></pre> <p>Данную опцию можно установить не только на уровне сервера, но и на уровне отдельных таблиц:</p> <pre><code># ALTER FOREIGN TABLE fdw_users
OPTIONS (ADD updatable &#39;false&#39;);</code></pre> <h3 id="postgres_fdw-и-dblink">Postgres_fdw и DBLink</h3> <p>Как можно было заметить, postgres_fdw и dblink выполняют одну и ту же работу — подключение одной PostgreSQL базы к другой. Что лучше использовать в таком случае?</p> <p>PostgreSQL FDW (Foreign Data Wrapper) более новый и рекомендуемый метод подключения к другим источникам данных. Хотя функциональность dblink похожа на FDW, последний является более SQL совместимым и может обеспечивать улучшеную производительность по сравнению с dblink подключениями. Кроме того, в отличии от postgres_fdw, dblink не способен сделать данные «только на чтение». Это может быть достаточно важно, если требуется обеспечить, чтобы данные с другой базы нельзя было изменять.</p> <p>В dblink подключения работают только в течение работы сесии и их требуется пересоздавать каждый раз. Postgres_fdw создает постоянное подключение к другой базе данных. Это может быть как хорошо, так плохо, в зависимости от потребностей.</p> <p>Из положительных сторон dblink можно отнести множество полезных комманд, которые позволяю использовать его для программирования полезного функционала. Также dblink работает в версиях PostgreSQL 8.3 и выше, в то время как postgres_fdw работает только в PostgreSQL 9.3 и выше (такое может возникнуть, если нет возможности обновить PostgreSQL базу).</p> <h2 id=pg_cron>Pg_cron</h2> <p><a href="https://github.com/citusdata/pg_cron">Pg_cron</a> — cron-подобный планировщик задач для PostgreSQL 9.5 или выше, который работает как расширение к базе. Он может выполнять несколько задач параллельно, но одновременно может работать не более одного экземпляра задания (если при запуске задачи преведущий запуск будет еще выполняеться, то запуск будет отложен до выполнения текущей задачи).</p> <h3 id="установка-и-использование-12">Установка и использование</h3> <p>После установки расширения требуется добавить его в <code>postgresql.conf</code> и перезапустить PostgreSQL:</p> <pre><code>shared_preload_libraries = &#39;pg_cron&#39;</code></pre> <p>Далее требуется активировать расширение для <code>postgres</code> базы:</p> <pre><code># CREATE EXTENSION pg_cron;</code></pre> <p>По умолчанию <code>pg_cron</code> ожидает, что все таблицы с метаданными будут находится в <code>postgres</code> базе данных. Данное поведение можно изменить и указать через параметр <code>cron.database_name</code> в <code>postgresql.conf</code> другую базу данных, где <code>pg_cron</code> будет хранить свои данные.</p> <p>Внутри <code>pg_cron</code> использует libpq библиотеку, поэтому потребуется разрешить подключения с <code>localhost</code> без пароля (<code>trust</code> в <code>pg_hba.conf</code>) или же создать <a href="https://www.postgresql.org/docs/current/static/libpq-pgpass.html">.pgpass</a> файл для настройки подключения к базе.</p> <p>Для создания cron задач используется функция <code>cron.schedule</code>:</p> <pre><code>-- Delete old data on Saturday at 3:30am (GMT)
SELECT cron.schedule(&#39;30 3 * * 6&#39;, $$DELETE FROM events WHERE event_time &lt; now() - interval &#39;1 week&#39;$$);
 schedule
----------
       42</code></pre> <p>Для удаления созданых задач используется <code>cron.unschedule</code>:</p> <pre><code> -- Vacuum every day at 10:00am (GMT)
 SELECT cron.schedule(&#39;0 10 * * *&#39;, &#39;VACUUM&#39;);
  schedule
 ----------
        43

-- Stop scheduling a job
SELECT cron.unschedule(43);
 unschedule
------------
          t</code></pre> <p>В целях безопасности cron задачи выполняются в базе данных, в которой <code>cron.schedule</code> функция была вызвана с правами доступа текущего пользователя.</p> <p>Поскольку <code>pg_cron</code> использует libpq библиотеку, это позволят запускать cron задачи на других базах данных (даже на других серверах). С помощью суперпользователя возможно модифицировать <code>cron.job</code> таблицу и добавить в нее параметры подключения к другой базе через <code>nodename</code> и <code>nodeport</code> поля:</p> <pre><code>INSERT INTO cron.job (schedule, command, nodename, nodeport, database, username)
 VALUES (&#39;0 4 * * *&#39;, &#39;VACUUM&#39;, &#39;worker-node-1&#39;, 5432, &#39;postgres&#39;, &#39;marco&#39;);</code></pre> <p>В таком случае нужно будет создать <a href="https://www.postgresql.org/docs/current/static/libpq-pgpass.html">.pgpass</a> файл для настройки подключения к базе на другом сервере.</p> <h2 id=pgstrom>PGStrom</h2> <p><a href="http://strom.kaigai.gr.jp/">PGStrom</a> — PostgreSQL расширение, которое позволяет использовать GPU для выполнения некоторых SQL операций. В частности, за счёт привлечения GPU могут быть ускорены такие операции как сравнительный перебор элементов таблиц, агрегирование записей и слияние хэшей. Код для выполнения на стороне GPU генерируется в момент разбора SQL-запроса при помощи специального JIT-компилятора и в дальнейшем выполняется параллельно с другими связанными с текущим запросом операциями, выполняемыми на CPU. Для выполнения заданий на GPU задействован OpenCL. Увеличение производительности операций слияния таблиц (<code>JOIN</code>) при использовании GPU увеличивается в десятки раз.</p> <p>Областью применения PG-Strom являются огромные отчеты с использованием агрегации и объединения таблиц. Эти рабочие нагрузки чаще используются в пакетной обработке данных для <a href="https://ru.wikipedia.org/wiki/OLAP">OLAP</a> систем.</p> <h2 id=zombodb>ZomboDB</h2> <p><a href="https://www.zombodb.com/">ZomboDB</a> — PostgreSQL расширение, которое позволяет использовать <a href="https://en.wikipedia.org/wiki/Elasticsearch">Elasticsearch</a> индексы внутри базы (используется <a href="https://www.postgresql.org/docs/current/static/indexam.html">интерфейс для методов доступа индекса</a>). ZomboDB индекс для PostgreSQL ничем не отличается от стандартного btree индекса. Таким образом, стандартные команды SQL полностью поддерживаются, включая <code>SELECT</code>, <code>BEGIN</code>, <code>COMMIT</code>, <code>ABORT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>, <code>COPY</code> и <code>VACUUM</code> и данные индексы являются MVCC-безопасными.</p> <p>На низком уровне ZomboDB индексы взаимодействуют с Elasticsearch сервером через HTTP запросы и автоматически синхронизируются в процессе изменения данных в PostgreSQL базе.</p> <h3 id="установка-и-использование-13">Установка и использование</h3> <p>ZomboDB состоит из двух частей: PostgreSQL расширения (написан на C и SQL/PLPGSQL) и Elasticsearch плагина (написан на Java).</p> <p>После установки требуется добавить в <code>postgresql.conf</code> zombodb библиотеку:</p> <pre><code>local_preload_libraries = &#39;zombodb.so&#39;</code></pre> <p>И после перегрузки PostgreSQL активировать его для базы данных:</p> <pre><code>CREATE EXTENSION zombodb;</code></pre> <p>После этого требуется установить Elasticsearch плагин на все ноды сервера и изменить конфигурацию в <code>elasticsearch.yml</code>:</p> <pre><code>threadpool.bulk.queue_size: 1024
threadpool.bulk.size: 12

http.compression: true

http.max_content_length: 1024mb
index.query.bool.max_clause_count: 1000000</code></pre> <p>Для примера создадим таблицу с продуктами и заполним её данными:</p> <pre><code># CREATE TABLE products (
    id SERIAL8 NOT NULL PRIMARY KEY,
    name text NOT NULL,
    keywords varchar(64)[],
    short_summary phrase,
    long_description fulltext,
    price bigint,
    inventory_count integer,
    discontinued boolean default false,
    availability_date date
);

# COPY products FROM PROGRAM &#39;curl https://raw.githubusercontent.com/zombodb/zombodb/master/TUTORIAL-data.dmp&#39;;</code></pre> <p><code>zdb(record)</code> zombodb функция конвертирует запись в JSON формат (обертка поверх <code>row_to_json(record)</code>):</p> <pre><code># SELECT zdb(products) FROM products WHERE id = 1;
                    zdb
-------------------------------------------------
{&quot;id&quot;:1,&quot;name&quot;:&quot;Magical Widget&quot;,&quot;keywords&quot;:[&quot;magical&quot;,&quot;widget&quot;,&quot;round&quot;],&quot;short_summary&quot;:&quot;A widget that is quite magical&quot;,&quot;long_description&quot;:&quot;Magical Widgets come from the land of Magicville and are capable of things you can&#39;t imagine&quot;,&quot;price&quot;:9900,&quot;inventory_count&quot;:42,&quot;discontinued&quot;:false,&quot;availability_date&quot;:&quot;2015-08-31&quot;}</code></pre> <p><code>zdb(regclass, tid)</code> zombodb функция, которая используется для статического определения ссылок на таблицу/индекс в контексте последовательного сканирования. Благодаря этим двум функциям можно создать zombodb индекс для <code>products</code> таблицы:</p> <pre><code># CREATE INDEX idx_zdb_products
           ON products
        USING zombodb(zdb(&#39;products&#39;, products.ctid), zdb(products))
         WITH (url=&#39;http://localhost:9200/&#39;);</code></pre> <p>Теперь можно проверить работу индекса:</p> <pre><code># SELECT id, name, short_summary FROM products WHERE zdb(&#39;products&#39;, products.ctid) ==&gt; &#39;sports or box&#39;;
 id |   name   |         short_summary
----+----------+--------------------------------
  2 | Baseball | It&#39;s a baseball
  4 | Box      | Just an empty box made of wood
(2 rows)
# EXPLAIN SELECT * FROM products WHERE zdb(&#39;products&#39;, ctid) ==&gt; &#39;sports or box&#39;;
                                    QUERY PLAN
-----------------------------------------------------------------------------------
 Index Scan using idx_zdb_products on products  (cost=0.00..4.02 rows=2 width=153)
   Index Cond: (zdb(&#39;products&#39;::regclass, ctid) ==&gt; &#39;sports or box&#39;::text)
(2 rows)</code></pre> <p>ZomboDB содержит набор функций для агрегационных запросов. Например, если нужно выбрать уникальный набор ключевых слов для всех продуктов в <code>keywords</code> поле вместе с их количеством, то можно воспользоваться <code>zdb_tally</code> функцией:</p> <pre><code># SELECT * FROM zdb_tally(&#39;products&#39;, &#39;keywords&#39;, &#39;^.*&#39;, &#39;&#39;, 5000, &#39;term&#39;);
         term          | count
-----------------------+-------
 alexander graham bell |     1
 baseball              |     1
 box                   |     1
 communication         |     1
 magical               |     1
 negative space        |     1
 primitive             |     1
 round                 |     2
 sports                |     1
 square                |     1
 widget                |     1
 wooden                |     1
(12 rows)</code></pre> <p>Более подробно про использование ZomboDB возможно ознакомиться в <a href="https://github.com/zombodb/zombodb#quick-links">официальной документации</a>.</p> <h2 id="заключение-25">Заключение</h2> <p>Расширения помогают улучшить работу PostgreSQL в решении специфических проблем. Расширяемость PostgreSQL позволяет создавать собственные расширения, или же наоборот, не нагружать СУБД лишним, не требуемым функционалом.</p> <h1 id="бэкап-и-восстановление-postgresql">Бэкап и восстановление PostgreSQL</h1> <h2 id="введение-9">Введение</h2> <p>Любой хороший сисадмин знает — бэкапы нужны всегда. Насколько бы надежной ни казалась Ваша система, всегда может произойти случай, который был не учтен, и из-за которого могут быть потеряны данные.</p> <p>Тоже самое касается и PostgreSQL баз данных. Посыпавшийся винчестер на сервере, ошибка в файловой системе, ошибка в другой программе, которая перетерла весь каталог PostgreSQL и многое другое приведет только к плачевному результату. И даже если у Вас репликация с множеством слейвов, это не означает, что система в безопасности — неверный запрос на мастер (<code>DELETE/DROP/TRUNCATE</code>), и у слейвов такая же порция данных (точнее их отсутствие).</p> <p>Существуют три принципиально различных подхода к резервному копированию данных PostgreSQL:</p> <ul> <li><p>SQL бэкап;</p></li> <li><p>Бэкап уровня файловой системы;</p></li> <li><p>Непрерывное резервное копирование;</p></li> </ul> <p>Каждый из этих подходов имеет свои сильные и слабые стороны.</p> <h2 id="sql-бэкап">SQL бэкап</h2> <p>Идея этого подхода в создании текстового файла с командами SQL. Такой файл можно передать обратно на сервер и воссоздать базу данных в том же состоянии, в котором она была во время бэкапа. У PostgreSQL для этого есть специальная утилита — <code>pg_dump</code>. Пример использования <code>pg_dump</code>:</p> <pre><code>$ pg_dump dbname &gt; outfile</code></pre> <p>Для восстановления такого бэкапа достаточно выполнить:</p> <pre><code>$ psql dbname &lt; infile</code></pre> <p>При этом базу данных <code>dbname</code> потребуется создать перед восстановлением. Также потребуется создать пользователей, которые имеют доступ к данным, которые восстанавливаются (это можно и не делать, но тогда просто в выводе восстановления будут ошибки). Если нам требуется, чтобы восстановление прекратилось при возникновении ошибки, тогда потребуется восстанавливать бэкап таким способом:</p> <pre><code>$ psql --set ON_ERROR_STOP=on dbname &lt; infile</code></pre> <p>Также, можно делать бэкап и сразу восстанавливать его в другую базу:</p> <pre><code>$ pg_dump -h host1 dbname | psql -h host2 dbname</code></pre> <p>После восстановления бэкапа желательно запустить <code>ANALYZE</code>, чтобы оптимизатор запросов обновил статистику.</p> <p>А что, если нужно сделать бэкап не одной базы данных, а всех, да и еще получить в бэкапе информацию про роли и таблицы? В таком случае у PostgreSQL есть утилита <code>pg_dumpall</code>. <code>pg_dumpall</code> используется для создания бэкапа данных всего кластера PostgreSQL:</p> <pre><code>$ pg_dumpall &gt; outfile</code></pre> <p>Для восстановления такого бэкапа достаточно выполнить от суперпользователя:</p> <pre><code>$ psql -f infile postgres</code></pre> <h3 id="sql-бэкап-больших-баз-данных">SQL бэкап больших баз данных</h3> <p>Некоторые операционные системы имеют ограничения на максимальный размер файла, что может вызывать проблемы при создании больших бэкапов через <code>pg_dump</code>. К счастью, <code>pg_dump</code> можете бэкапить в стандартный вывод. Так что можно использовать стандартные инструменты Unix, чтобы обойти эту проблему. Есть несколько возможных способов:</p> <ul> <li><p><strong>Использовать сжатие для бэкапа</strong></p> <p>Можно использовать программу сжатия данных, например GZIP:</p> <pre><code>$ pg_dump dbname | gzip &gt; filename.gz</code></pre> <p>Восстановление:</p> <pre><code>$ gunzip -c filename.gz | psql dbname</code></pre> <p>или</p> <pre><code>cat filename.gz | gunzip | psql dbname</code></pre></li> <li><p><strong>Использовать команду split</strong></p> <p>Команда split позволяет разделить вывод в файлы меньшего размера, которые являются подходящими по размеру для файловой системы. Например, бэкап делится на куски по 1 мегабайту:</p> <pre><code>$ pg_dump dbname | split -b 1m - filename</code></pre> <p>Восстановление:</p> <pre><code>$ cat filename* | psql dbname</code></pre></li> <li><p><strong>Использовать пользовательский формат дампа <code>pg_dump</code></strong></p> <p>PostgreSQL построен на системе с библиотекой сжатия Zlib, поэтому пользовательский формат бэкапа будет в сжатом виде. Это похоже на метод с использованием GZIP, но он имеет дополнительное преимущество — таблицы могут быть восстановлены выборочно. Минус такого бэкапа — восстановить возможно только в такую же версию PostgreSQL (отличаться может только патч релиз, третья цифра после точки в версии):</p> <pre><code>$ pg_dump -Fc dbname &gt; filename</code></pre> <p>Через psql такой бэкап не восстановить, но для этого есть утилита <code>pg_restore</code>:</p> <pre><code>$ pg_restore -d dbname filename</code></pre></li> </ul> <p>При слишком большой базе данных, вариант с командой split нужно комбинировать со сжатием данных.</p> <h2 id="бэкап-уровня-файловой-системы">Бэкап уровня файловой системы</h2> <p>Альтернативный метод резервного копирования заключается в непосредственном копировании файлов, которые PostgreSQL использует для хранения данных в базе данных. Например:</p> <pre><code>$ tar -cf backup.tar /usr/local/pgsql/data</code></pre> <p>Но есть два ограничения, которые делает этот метод нецелесообразным, или, по крайней мере, уступающим SQL бэкапу:</p> <ul> <li><p>PostgreSQL база данных должна быть остановлена, для того, чтобы получить актуальный бэкап (PostgreSQL держит множество объектов в памяти, буферизация файловой системы). Излишне говорить, что во время восстановления такого бэкапа потребуется также остановить PostgreSQL;</p></li> <li><p>Не получится востановить только определенные данные с такого бэкапа;</p></li> </ul> <p>Как альтернатива, можно делать снимки (snapshot) файлов системы (папки с файлами PostgreSQL). В таком случае останавливать PostgreSQL не требуется. Однако, резервная копия, созданная таким образом, сохраняет файлы базы данных в состоянии, как если бы сервер базы данных был неправильно остановлен. Поэтому при запуске PostgreSQL из резервной копии, он будет думать, что предыдущий экземпляр сервера вышел из строя и восстановит данные в соотвествии с данными журнала WAL. Это не проблема, просто надо знать про это (и не забыть включить WAL файлы в резервную копию). Также, если файловая система PostgreSQL распределена по разным файловым системам, то такой метод бэкапа будет очень ненадежным — снимки файлов системы должны быть сделаны одновременно. Почитайте документацию файловой системы очень внимательно, прежде чем доверять снимкам файлов системы в таких ситуациях.</p> <p>Также возможен вариант с использованием <code>rsync</code> утилиты. Первым запуском rsync мы копируем основные файлы с директории PostgreSQL (PostgreSQL при этом продолжает работу). После этого мы останавливаем PostgreSQL и запускаем повторно <code>rsync</code>. Второй запуск <code>rsync</code> пройдет гораздо быстрее, чем первый, потому что будет передавать относительно небольшой размер данных, и конечный результат будет соответствовать остановленной СУБД. Этот метод позволяет делать бэкап уровня файловой системы с минимальным временем простоя.</p> <h2 id="непрерывное-резервное-копирование">Непрерывное резервное копирование</h2> <p>PostgreSQL поддерживает упреждающую запись логов (Write Ahead Log, WAL) в <code>pg_xlog</code> директорию, которая находится в директории данных СУБД. В логи пишутся все изменения сделанные с данными в СУБД. Этот журнал существует прежде всего для безопасности во время краха PostgreSQL: если происходят сбои в системе, базы данных могут быть восстановлены с помощью «перезапуска» этого журнала. Тем не менее, существование журнала делает возможным использование третьей стратегии для резервного копирования баз данных: мы можем объединить бэкап уровня файловой системы с резервной копией WAL файлов. Если требуется восстановить такой бэкап, то мы восстанавливаем файлы резервной копии файловой системы, а затем «перезапускаем» с резервной копии файлов WAL для приведения системы к актуальному состоянию. Этот подход является более сложным для администрирования, чем любой из предыдущих подходов, но он имеет некоторые преимущества:</p> <ul> <li><p>Не нужно согласовывать файлы резервной копии системы. Любая внутренняя противоречивость в резервной копии будет исправлена путем преобразования журнала (не отличается от того, что происходит во время восстановления после сбоя);</p></li> <li><p>Восстановление состояния сервера для определенного момента времени;</p></li> <li><p>Если мы постоянно будем «скармливать» файлы WAL на другую машину, которая была загружена с тех же файлов резервной базы, то у нас будет находящийся всегда в актуальном состоянии резервный сервер PostgreSQL (создание сервера горячего резерва);</p></li> </ul> <p>Как и бэкап файловой системы, этот метод может поддерживать только восстановление всей базы данных кластера. Кроме того, он требует много места для хранения WAL файлов.</p> <h3 id="настройка-8">Настройка</h3> <p>Первый шаг — активировать архивирование. Эта процедура будет копировать WAL файлы в архивный каталог из стандартного каталога <code>pg_xlog</code>. Это делается в файле <code>postgresql.conf</code>:</p> <pre><code>archive_mode = on # enable archiving
archive_command = &#39;cp -v %p /data/pgsql/archives/%f&#39;
archive_timeout = 300 # timeout to close buffers</code></pre> <p>После этого необходимо перенести файлы (в порядке их появления) в архивный каталог. Для этого можно использовать функцию <code>rsync</code>. Можно поставить функцию в <code>cron</code> и, таким образом, файлы могут автоматически перемещаться между хостами каждые несколько минут:</p> <pre><code>$ rsync -avz --delete prod1:/data/pgsql/archives/ \
/data/pgsql/archives/ &gt; /dev/null</code></pre> <p>В конце, необходимо скопировать файлы в каталог <code>pg_xlog</code> на сервере PostgreSQL (он должен быть в режиме восстановления). Для этого необходимо в каталоге данных PostgreSQL создать файл <code>recovery.conf</code> с заданной командой копирования файлов из архива в нужную директорию:</p> <pre><code>restore_command = &#39;cp /data/pgsql/archives/%f &quot;%p&quot;&#39;</code></pre> <p>Документация PostgreSQL предлагает хорошее описание настройки непрерывного копирования, поэтому данная глава не будет углубляться в детали (например, как перенести директорию СУБД с одного сервера на другой, какие могут быть проблемы). Более подробно вы можете почитать по <a href="http://www.postgresql.org/docs/current/static/continuous-archiving.html">этой ссылке</a>.</p> <h2 id="утилиты-для-непрерывного-резервного-копирования">Утилиты для непрерывного резервного копирования</h2> <p>Непрерывное резервное копирования один из лучших спрособ для создания бэкапов и восстановления их. Нередко бэкапы сохраняются на той же файловой системе, на которой расположена база данных. Это не очень безопасно, т.к. при выходе дисковой системы сервера из строя вы можете потерять все данные (и базу, и бэкапы), или попросту столкнуться с тем, что на жестком диске закончится свободное место. Поэтому лучше, когда бэкапы складываются на отдельный сервер или в «облачное хранилище» (например <a href="http://aws.amazon.com/s3/">AWS S3</a>). Чтобы не писать свой «велосипед» для автоматизации этого процесса на сегодняшний день существует набор программ, которые облегчает процесс настройки и поддержки процесса создания бэкапов на основе непрерывного резервного копирования.</p> <h3 id=wal-e>WAL-E</h3> <p><a href="https://github.com/wal-e/wal-e">WAL-E</a> предназначена для непрерывной архивации PostgreSQL WAL-logs в Amazon S3 или Windows Azure (начиная с версии 0.7) и управления использованием <code>pg_start_backup</code> и <code>pg_stop_backup</code>. Утилита написана на Python и разработана в компании <a href="http://www.heroku.com/">Heroku</a>, где её активно используют.</p> <h4 id="установка-8">Установка</h4> <p>У WAL-E есть пара зависимостей: <code>lzop</code>, <code>psql</code>, <code>pv</code> (в старых версиях используется <code>mbuffer</code>), python 3.4+ и несколько python библиотек (<code>gevent</code>, <code>boto</code>, <code>azure</code>). Также для удобства настроек переменных среды устанавливается <code>daemontools</code>. На Ubuntu это можно все поставить одной командой:</p> <pre><code>$ aptitude install git-core python-dev python-setuptools python-pip build-essential libevent-dev lzop pv daemontools daemontools-run</code></pre> <p>Теперь установим WAL-E:</p> <pre><code>$ pip install https://github.com/wal-e/wal-e/archive/v1.0.3.tar.gz</code></pre> <p>После успешной установки можно начать работать с WAL-E.</p> <h4 id="настройка-и-работа">Настройка и работа</h4> <p>Как уже писалось, WAL-E сливает все данные в AWS S3, поэтому нам потребуются «Access Key ID», «Secret Access Key» и «AWS Region» (эти данные можно найти в акаунте Amazon AWS). Команда для загрузки бэкапа всей базы данных в S3:</p> <pre><code>AWS_REGION=... AWS_SECRET_ACCESS_KEY=... wal-e                     \
  -k AWS_ACCESS_KEY_ID                                \
  --s3-prefix=s3://some-bucket/directory/or/whatever  \
  backup-push /var/lib/postgresql/9.2/main</code></pre> <p>Где <code>s3-prefix</code> — URL, который содержит имя S3 бакета (bucket) и путь к папке, куда следует складывать резервные копии. Команда для загрузки WAL-логов на S3:</p> <pre><code>AWS_REGION=... AWS_SECRET_ACCESS_KEY=... wal-e                     \
  -k AWS_ACCESS_KEY_ID                                \
  --s3-prefix=s3://some-bucket/directory/or/whatever  \
  wal-push /var/lib/postgresql/9.2/main/pg_xlog/WAL_SEGMENT_LONG_HEX</code></pre> <p>Для управления этими переменными окружения можно использовать команду <code>envdir</code> (идет в поставке с <code>daemontools</code>). Для этого создадим <code>envdir</code> каталог:</p> <pre><code>$ mkdir -p /etc/wal-e.d/env
$ echo &quot;aws_region&quot; &gt; /etc/wal-e.d/env/AWS_REGION
$ echo &quot;secret-key&quot; &gt; /etc/wal-e.d/env/AWS_SECRET_ACCESS_KEY
$ echo &quot;access-key&quot; &gt; /etc/wal-e.d/env/AWS_ACCESS_KEY_ID
$ echo &#39;s3://some-bucket/directory/or/whatever&#39; &gt; /etc/wal-e.d/env/WALE_S3_PREFIX
$ chown -R root:postgres /etc/wal-e.d</code></pre> <p>После создания данного каталога появляется возможность запускать WAL-E команды гораздо проще и с меньшим риском случайного использования некорректных значений:</p> <pre><code>$ envdir /etc/wal-e.d/env wal-e backup-push ...
$ envdir /etc/wal-e.d/env wal-e wal-push ...</code></pre> <p>Теперь настроим PostgreSQL для сбрасывания WAL-логов в S3 c помощью WAL-E. Отредактируем <code>postgresql.conf</code>:</p> <pre><code>wal_level = hot_standby # или archive, если PostgreSQL &lt; 9.0
archive_mode = on
archive_command = &#39;envdir /etc/wal-e.d/env /usr/local/bin/wal-e wal-push %p&#39;
archive_timeout = 60</code></pre> <p>Лучше указать полный путь к WAL-E (можно узнать командой <code>which wal-e</code>), поскольку PostgreSQL может его не найти. После этого нужно перегрузить PostgreSQL. В логах базы вы должны увидеть что-то подобное:</p> <pre><code>2016-11-07 14:52:19 UTC LOG:  database system was shut down at 2016-11-07 14:51:40 UTC
2016-11-07 14:52:19 UTC LOG:  database system is ready to accept connections
2016-11-07 14:52:19 UTC LOG:  autovacuum launcher started
2016-11-07T14:52:19.784+00 pid=7653 wal_e.worker.s3_worker INFO     MSG: begin archiving a file
        DETAIL: Uploading &quot;pg_xlog/000000010000000000000001&quot; to &quot;s3://cleverdb-pg-backups/pg/wal_005/000000010000000000000001.lzo&quot;.
2016-11-07 14:52:19 UTC LOG:  incomplete startup packet
2016-11-07T14:52:28.234+00 pid=7653 wal_e.worker.s3_worker INFO     MSG: completed archiving to a file
        DETAIL: Archiving to &quot;s3://cleverdb-pg-backups/pg/wal_005/000000010000000000000001.lzo&quot; complete at 21583.3KiB/s.
2016-11-07T14:52:28.341+00 pid=7697 wal_e.worker.s3_worker INFO     MSG: begin archiving a file
        DETAIL: Uploading &quot;pg_xlog/000000010000000000000002.00000020.backup&quot; to &quot;s3://cleverdb-pg-backups/pg/wal_005/000000010000000000000002.00000020.backup.lzo&quot;.
2016-11-07T14:52:34.027+00 pid=7697 wal_e.worker.s3_worker INFO     MSG: completed archiving to a file
        DETAIL: Archiving to &quot;s3://cleverdb-pg-backups/pg/wal_005/000000010000000000000002.00000020.backup.lzo&quot; complete at 00KiB/s.
2016-11-07T14:52:34.187+00 pid=7711 wal_e.worker.s3_worker INFO     MSG: begin archiving a file
        DETAIL: Uploading &quot;pg_xlog/000000010000000000000002&quot; to &quot;s3://cleverdb-pg-backups/pg/wal_005/000000010000000000000002.lzo&quot;.
2016-11-07T14:52:40.232+00 pid=7711 wal_e.worker.s3_worker INFO     MSG: completed archiving to a file
        DETAIL: Archiving to &quot;s3://cleverdb-pg-backups/pg/wal_005/000000010000000000000002.lzo&quot; complete at 2466.67KiB/s.</code></pre> <p>Если ничего похожего в логах не видно, тогда нужно смотреть что за ошибка появляется и исправлять её. Для того, чтобы бэкапить всю базу, достаточно выполнить данную команду:</p> <pre><code>$ envdir /etc/wal-e.d/env wal-e backup-push /var/lib/postgresql/9.2/main
2016-11-07T14:49:26.174+00 pid=7493 wal_e.operator.s3_operator INFO     MSG: start upload postgres version metadata
        DETAIL: Uploading to s3://cleverdb-pg-backups/pg/basebackups_005/base_000000010000000000000006_00000032/extended_version.txt.
2016-11-07T14:49:32.783+00 pid=7493 wal_e.operator.s3_operator INFO     MSG: postgres version metadata upload complete
2016-11-07T14:49:32.859+00 pid=7493 wal_e.worker.s3_worker INFO     MSG: beginning volume compression
        DETAIL: Building volume 0.
...
HINT:  Check that your archive_command is executing properly.  pg_stop_backup can be canceled safely, but the database backup will not be usable without all the WAL segments.
NOTICE:  pg_stop_backup complete, all required WAL segments have been archived</code></pre> <p>Данный бэкап лучше делать раз в сутки (например, добавить в <code>crontab</code>). На рис [fig:wal-e1]-[fig:wal-e3] видно как хранятся бэкапы на S3. Все бэкапы сжаты через <a href="http://en.wikipedia.org/wiki/Lzop">lzop</a>. Данный алгоритм сжимает хуже чем gzip, но скорость сжатия намного быстрее (приблизительно 25 Мб/сек используя 5% ЦПУ). Чтобы уменьшить нагрузку на чтение с жесткого диска бэкапы отправляются через <code>pv</code> утилиту (опцией <code>cluster-read-rate-limit</code> можно ограничить скорость чтения, если это требуется).</p> <p>Теперь перейдем к восстановлению данных. Для восстановления базы из резервной копии используется <code>backup-fetch</code> команда:</p> <pre><code>$ sudo -u postgres bash -c &quot;envdir /etc/wal-e.d/env wal-e  --s3-prefix=s3://some-bucket/directory/or/whatever backup-fetch /var/lib/postgresql/9.2/main LATEST&quot;</code></pre> <p>Где <code>LATEST</code> означает, что база восстановится из последнего актуального бэкапа (PostgreSQL в это время должен быть остановлен). Для восстановления из более поздней резервной копии:</p> <pre><code>$ sudo -u postgres bash -c &quot;envdir /etc/wal-e.d/env wal-e  --s3-prefix=s3://some-bucket/directory/or/whatever backup-fetch /var/lib/postgresql/9.2/main base_LONGWALNUMBER_POSITION_NUMBER&quot;</code></pre> <p>Для получения списка доступных резервных копий есть команда <code>backup-list</code>:</p> <pre><code>$ envdir /etc/wal-e.d/env wal-e backup-list
name	last_modified	expanded_size_bytes	wal_segment_backup_start	wal_segment_offset_backup_start	wal_segment_backup_stop	wal_segment_offset_backup_stop
base_000000010000000000000008_00000032	2016-11-07T14:00:07.000Z		000000010000000000000008	00000032
base_00000001000000000000000C_00000032	2016-11-08T15:00:08.000Z		00000001000000000000000C	00000032</code></pre> <p>После завершения работы с основной резервной копией для полного восстановления нужно считать WAL-логи (чтобы данные обновились до последнего состояния). Для этого используется <code>recovery.conf</code>:</p> <pre><code>restore_command = &#39;envdir /etc/wal-e.d/env /usr/local/bin/wal-e wal-fetch &quot;%f&quot; &quot;%p&quot;&#39;</code></pre> <p>После создания этого файла нужно запустить PostgreSQL. Через небольшой интервал времени база станет полностью восстановленной.</p> <p>Для удаления старых резервных копий (или вообще всех) используется команда <code>delete</code>:</p> <pre><code># удаление старых бэкапов старше base_00000004000002DF000000A6_03626144
$ envdir /etc/wal-e.d/env wal-e delete --confirm before base_00000004000002DF000000A6_03626144
# удаление всех бэкапов
$ envdir /etc/wal-e.d/env wal-e delete --confirm everything
# удалить все старше последних 20 бэкапов
$ envdir /etc/wal-e.d/env wal-e delete --confirm retain 20</code></pre> <p>Без опции <code>--confirm</code> команды будут запускаться и показывать, что будет удаляться, но фактического удаления не будет производиться (dry run).</p> <h4 id="заключение-26">Заключение</h4> <p>WAL-E помогает автоматизировать сбор резервных копий с PostgreSQL и хранить их в достаточно дешевом и надежном хранилище — Amazon S3 или Windows Azure.</p> <h3 id=barman>Barman</h3> <p><a href="http://www.pgbarman.org/">Barman</a>, как и WAL-E, позволяет создать систему для бэкапа и восстановления PostgreSQL на основе непрерывного резервного копирования. Barman использует для хранения бэкапов отдельный сервер, который может собирать бэкапы как с одного, так и с нескольких PostgreSQL баз данных.</p> <h4 id="установка-и-настройка-2">Установка и настройка</h4> <p>Рассмотрим простой случай с одним экземпляром PostgreSQL (один сервер) и пусть его хост будет <code>pghost</code>. Наша задача — автоматизировать сбор и хранение бэкапов этой базы на другом сервере (его хост будет <code>brhost</code>). Для взаимодействия эти два сервера должны быть полностью открыты по SSH (доступ без пароля, по ключам). Для этого можно использовать <code>authorized_keys</code> файл.</p> <pre><code># Проверка подключения с сервера PostgreSQL (pghost)
$ ssh barman@brhost
# Проверка подключения с сервера бэкапов (brhost)
$ ssh postgres@pghost</code></pre> <p>Далее нужно установить на сервере для бэкапов barman. Сам barman написан на python и имеет пару зависимостей: python 2.6+, <code>rsync</code> и python библиотеки (<code>argh</code>, <code>psycopg2</code>, <code>python-dateutil</code>, <code>distribute</code>). На Ubuntu все зависимости можно поставить одной командой:</p> <pre><code>$ aptitude install python-dev python-argh python-psycopg2 python-dateutil rsync python-setuptools</code></pre> <p>Далее нужно установить barman:</p> <pre><code>$ tar -xzf barman-2.1.tar.gz
$ cd barman-2.1/
$ ./setup.py build
$ sudo ./setup.py install</code></pre> <p>Или используя <a href="https://wiki.postgresql.org/wiki/Apt">PostgreSQL Community APT репозиторий</a>:</p> <pre><code>$ apt-get install barman</code></pre> <p>Теперь перейдем к серверу с PostgreSQL. Для того, чтобы barman мог подключаться к базе данных без проблем, нам нужно выставить настройки доступа в конфигах PostgreSQL:</p> <pre><code>listen_adress = &#39;*&#39;</code></pre> <pre><code>host  all  all  brhost/32  trust</code></pre> <p>После этих изменений нужно перегрузить PostgreSQL. Теперь можем проверить с сервера бэкапов подключение к PostgreSQL:</p> <pre><code>$ psql -c &#39;SELECT version()&#39; -U postgres -h pghost
                                                  version
------------------------------------------------------------------------------------------------------------
 PostgreSQL 9.3.1 on x86_64-unknown-linux-gnu, compiled by gcc (Ubuntu/Linaro 4.7.2-2ubuntu1) 4.7.2, 64-bit
(1 row)</code></pre> <p>Далее создадим папку на сервере с бэкапами для хранения этих самых бэкапов:</p> <pre><code>$ sudo mkdir -p /srv/barman
$ sudo chown barman:barman /srv/barman</code></pre> <p>Для настройки barman создадим <code>/etc/barman.conf</code>:</p> <pre><code>; Main directory
barman_home = /srv/barman

; Log location
log_file = /var/log/barman/barman.log

; Default compression level: possible values are None (default), bzip2, gzip or custom
compression = gzip

; &#39;main&#39; PostgreSQL Server configuration
[main]
; Human readable description
description =  &quot;Main PostgreSQL Database&quot;

; SSH options
ssh_command = ssh postgres@pghost

; PostgreSQL connection string
conninfo = host=pghost user=postgres</code></pre> <p>Секция «main» (так мы назвали для barman наш PostgreSQL сервер) содержит настройки для подключения к PostgreSQL серверу и базе. Проверим настройки:</p> <pre><code>$ barman show-server main
Server main:
	active: true
	description: Main PostgreSQL Database
	ssh_command: ssh postgres@pghost
	conninfo: host=pghost user=postgres
	backup_directory: /srv/barman/main
	basebackups_directory: /srv/barman/main/base
	wals_directory: /srv/barman/main/wals
	incoming_wals_directory: /srv/barman/main/incoming
	lock_file: /srv/barman/main/main.lock
	compression: gzip
	custom_compression_filter: None
	custom_decompression_filter: None
	retention_policy: None
	wal_retention_policy: None
	pre_backup_script: None
	post_backup_script: None
	current_xlog: None
	last_shipped_wal: None
	archive_command: None
	server_txt_version: 9.3.1
	data_directory: /var/lib/postgresql/9.3/main
	archive_mode: off
	config_file: /etc/postgresql/9.3/main/postgresql.conf
	hba_file: /etc/postgresql/9.3/main/pg_hba.conf
	ident_file: /etc/postgresql/9.3/main/pg_ident.conf

# barman check main
Server main:
	ssh: OK
	PostgreSQL: OK
	archive_mode: FAILED (please set it to &#39;on&#39;)
	archive_command: FAILED (please set it accordingly to documentation)
	directories: OK
	compression settings: OK</code></pre> <p>Все хорошо, вот только PostgreSQL не настроен. Для этого на сервере с PostgreSQL отредактируем конфиг базы:</p> <pre><code>wal_level = hot_standby # archive для PostgreSQL &lt; 9.0
archive_mode = on
archive_command = &#39;rsync -a %p barman@brhost:INCOMING_WALS_DIRECTORY/%f&#39;</code></pre> <p>где <code>INCOMING_WALS_DIRECTORY</code> — директория для складывания WAL-логов. Её можно узнать из вывода команды <code>barman show-server main</code>(листинг [lst:barman9], указано <code>/srv/barman/main/incoming</code>). После изменения настроек нужно перегрузить PostgreSQL. Теперь проверим статус на сервере бэкапов:</p> <pre><code>$ barman check main
Server main:
	ssh: OK
	PostgreSQL: OK
	archive_mode: OK
	archive_command: OK
	directories: OK
	compression settings: OK</code></pre> <p>Все готово. Для добавления нового сервера процедуру потребуется повторить, а в <code>barman.conf</code> добавить новый сервер.</p> <h4 id="создание-бэкапов">Создание бэкапов</h4> <p>Получение списка серверов:</p> <pre><code>$ barman list-server
main - Main PostgreSQL Database</code></pre> <p>Запуск создания резервной копии PostgreSQL (сервер указывается последним параметром):</p> <pre><code>$ barman backup main
Starting backup for server main in /srv/barman/main/base/20121109T090806
Backup start at xlog location: 0/3000020 (000000010000000000000003, 00000020)
Copying files.
Copy done.
Asking PostgreSQL server to finalize the backup.
Backup end at xlog location: 0/30000D8 (000000010000000000000003, 000000D8)
Backup completed</code></pre> <p>Такую задачу лучше выполнять раз в сутки (добавить в cron). Посмотреть список бэкапов для указаной базы:</p> <pre><code>$ barman list-backup main
main 20121110T091608 - Fri Nov 10 09:20:58 2012 - Size: 1.0 GiB - WAL Size: 446.0 KiB
main 20121109T090806 - Fri Nov  9 09:08:10 2012 - Size: 23.0 MiB - WAL Size: 477.0 MiB</code></pre> <p>Более подробная информация о выбраной резервной копии:</p> <pre><code>$ barman show-backup main 20121110T091608
Backup 20121109T091608:
  Server Name       : main
  Status:           : DONE
  PostgreSQL Version: 90201
  PGDATA directory  : /var/lib/postgresql/9.3/main

  Base backup information:
    Disk usage      : 1.0 GiB
    Timeline        : 1
    Begin WAL       : 00000001000000000000008C
    End WAL         : 000000010000000000000092
    WAL number      : 7
    Begin time      : 2012-11-10 09:16:08.856884
    End time        : 2012-11-10 09:20:58.478531
    Begin Offset    : 32
    End Offset      : 3576096
    Begin XLOG      : 0/8C000020
    End XLOG        : 0/92369120

  WAL information:
    No of files     : 1
    Disk usage      : 446.0 KiB
    Last available  : 000000010000000000000093

  Catalog information:
    Previous Backup : 20121109T090806
    Next Backup     : - (this is the latest base backup)</code></pre> <p>Также можно сжимать WAL-логи, которые накапливаются в каталогах командой «cron»:</p> <pre><code>$ barman cron
Processing xlog segments for main
	000000010000000000000001
	000000010000000000000002
	000000010000000000000003
	000000010000000000000003.00000020.backup
	000000010000000000000004
	000000010000000000000005
	000000010000000000000006</code></pre> <p>Эту команду требуется добавлять в <code>crontab</code>. Частота выполнения данной команды зависит от того, как много WAL-логов накапливается (чем больше файлов - тем дольше она выполняется). Barman может сжимать WAL-логи через gzip, bzip2 или другой компрессор данных (команды для сжатия и распаковки задаются через <code>custom_compression_filter</code> и <code>custom_decompression_filter</code> соответственно). Также можно активировать компрессию данных при передачи по сети через опцию <code>network_compression</code> (по умолчанию отключена). Через опции <code>bandwidth_limit</code> (по умолчанию 0, ограничений нет) и <code>tablespace_bandwidth_limit</code> возможно ограничить использования сетевого канала.</p> <p>Для восстановления базы из бэкапа используется команда <code>recover</code>:</p> <pre><code>$ barman recover --remote-ssh-command &quot;ssh postgres@pghost&quot; main 20121109T090806 /var/lib/postgresql/9.3/main
Starting remote restore for server main using backup 20121109T090806
Destination directory: /var/lib/postgresql/9.3/main
Copying the base backup.
Copying required wal segments.
The archive_command was set to &#39;false&#39; to prevent data losses.

Your PostgreSQL server has been successfully prepared for recovery!

Please review network and archive related settings in the PostgreSQL
configuration file before starting the just recovered instance.

WARNING: Before starting up the recovered PostgreSQL server,
please review also the settings of the following configuration
options as they might interfere with your current recovery attempt:

    data_directory = &#39;/var/lib/postgresql/9.3/main&#39;		# use data in another directory
    external_pid_file = &#39;/var/run/postgresql/9.3-main.pid&#39;		# write an extra PID file
    hba_file = &#39;/etc/postgresql/9.3/main/pg_hba.conf&#39;	# host-based authentication file
    ident_file = &#39;/etc/postgresql/9.3/main/pg_ident.conf&#39;	# ident configuration file</code></pre> <p>Barman может восстановить базу из резервной копии на удаленном сервере через SSH (для этого есть опция <code>remote-ssh-command</code>). Также barman может восстановить базу, используя <a href="http://en.wikipedia.org/wiki/Point-in-time_recovery">PITR</a>: для этого используются опции <code>target-time</code> (указывается время) или <code>target-xid</code> (id транзакции).</p> <h4 id="заключение-27">Заключение</h4> <p>Barman помогает автоматизировать сбор и хранение резервных копий PostgreSQL данных на отдельном сервере. Утилита проста, позволяет хранить и удобно управлять бэкапами нескольких PostgreSQL серверов.</p> <h3 id=pg_arman>Pg_arman</h3> <p><a href="https://github.com/michaelpq/pg_arman">Pg_arman</a> — менеджер резервного копирования и восстановления для PostgreSQL 9.5 или выше. Это ответвление проекта <code>pg_arman</code>, изначально разрабатываемого в NTT. Теперь его разрабатывает и поддерживает Мишель Пакье. Утилита предоставляет следующие возможности:</p> <ul> <li><p>Резервное копирование во время работы базы данных, включая табличные пространства, с помощью всего одной команды;</p></li> <li><p>Восстановление из резервной копии всего одной командой, с нестандартными вариантами, включая использование <a href="https://www.postgresql.org/docs/current/static/continuous-archiving.html">PITR</a>;</p></li> <li><p>Поддержка полного и дифференциального копирования;</p></li> <li><p>Управление резервными копиями со встроенными каталогами;</p></li> </ul> <h4 id="использование">Использование</h4> <p>Сначала требуется создать «каталог резервного копирования», в котором будут храниться файлы копий и их метаданные. До инициализации этого каталога рекомендуется настроить параметры <code>archive_mode</code> и <code>archive_command</code> в <code>postgresql.conf</code>. Если переменные инициализированы, <code>pg_arman</code> может скорректировать файл конфигурации. В этом случае потребуется задать путь к кластеру баз данных: переменной окружения <code>PGDATA</code> или через параметр <code>-D/--pgdata</code>.</p> <pre><code>$ pg_arman init -B /path/to/backup/</code></pre> <p>После этого возможен один из следующих вариантов резервного копирования:</p> <ul> <li><p>Полное резервное копирование (копируется весь кластер баз данных);</p> <pre><code>  $ pg_arman backup --backup-mode=full
  $ pg_arman validate
  </code></pre></li> <li><p>Дифференциальное резервное копирование: копируются только файлы или страницы, изменённые после последней проверенной копии. Для этого выполняется сканирование записей WAL от позиции последнего копирования до LSN выполнения <code>pg_start_backup</code> и все изменённые блоки записываются и отслеживаются как часть резервной копии. Так как просканированные сегменты WAL должны находиться в архиве WAL, последний сегмент, задействованный после запуска <code>pg_start_backup</code>, должен быть переключен принудительно;</p> <pre><code>  $ pg_arman backup --backup-mode=page
  $ pg_arman validate
  </code></pre></li> </ul> <p>После резервного копирования рекомендуется проверять файлы копий как только это будет возможно. Непроверенные копии нельзя использовать в операциях восстановления и резервного копирования.</p> <p>До начала восстановления через <code>pg_arman</code> PostgreSQL кластер должен быть остановлен. Если кластер баз данных всё ещё существует, команда восстановления сохранит незаархивированный журнал транзакций и удалит все файлы баз данных. После восстановления файлов <code>pg_arman</code> создаёт <code>recovery.conf</code> в <code>$PGDATA</code> каталоге. Этот конфигурационный файл содержит параметры для восстановления. После успешного восстановления рекомендуется при первой же возможности сделать полную резервную копию. Если ключ <code>--recovery-target-timeline</code> не задан, целевой точкой восстановления будет <code>TimeLineID</code> последней контрольной точки в файле (<code>$PGDATA/global/pg_control</code>). Если файл <code>pg_control</code> отсутствует, целевой точкой будет <code>TimeLineID</code> в полной резервной копии, используемой при восстановлении.</p> <pre><code>$ pg_ctl stop -m immediate
$ pg_arman restore
$ pg_ctl start</code></pre> <p><code>Pg_arman</code> имеет ряд ограничений:</p> <ul> <li><p>Требуются права чтения каталога баз данных и записи в каталог резервного копирования. Обычно для этого на сервере БД требуется смонтировать диск, где размещён каталог резервных копий, используя NFS или другую технологию;</p></li> <li><p>Основные версии <code>pg_arman</code> и сервера должны совпадать;</p></li> <li><p>Размеры блоков <code>pg_arman</code> и сервера должны совпадать;</p></li> <li><p>Если в каталоге с журналами сервера или каталоге с архивом WAL оказываются нечитаемые файлы/каталоги, резервное копирование или восстановление завершится сбоем вне зависимости от выбранного режима копирования;</p></li> </ul> <h2 id="заключение-28">Заключение</h2> <p>В любом случае, усилия и время, затраченные на создание оптимальной системы создания бэкапов, будут оправданы. Невозможно предугадать когда произойдут проблемы с базой данных, поэтому бэкапы должны быть настроены для PostgreSQL (особенно, если это продакшн система).</p> <h1 id="стратегии-масштабирования-для-postgresql">Стратегии масштабирования для PostgreSQL</h1> <h2 id="введение-10">Введение</h2> <p>Многие разработчики крупных проектов сталкиваются с проблемой, когда один-единственный сервер базы данных никак не может справиться с нагрузками. Очень часто такие проблемы происходят из-за неверного проектирования приложения(плохая структура БД для приложения, отсутствие кеширования). Но в данном случае пусть у нас есть «идеальное» приложение, для которого оптимизированы все SQL запросы, используется кеширование, PostgreSQL настроен, но все равно не справляется с нагрузкой. Такая проблема может возникнуть как на этапе проектирования, так и на этапе роста приложения. И тут возникает вопрос: какую стратегию выбрать при возникновении подобной ситуации?</p> <p>Если Ваш заказчик готов купить супер сервер за несколько тысяч долларов (а по мере роста — десятков тысяч и т.д.), чтобы сэкономить время разработчиков, но сделать все быстро, можете дальше эту главу не читать. Но такой заказчик — мифическое существо и, в основном, такая проблема ложится на плечи разработчиков.</p> <h3 id="суть-проблемы">Суть проблемы</h3> <p>Для того, что-бы сделать какой-то выбор, необходимо знать суть проблемы. Существуют два предела, в которые могут уткнуться сервера баз данных:</p> <ul> <li><p>Ограничение пропускной способности чтения данных;</p></li> <li><p>Ограничение пропускной способности записи данных;</p></li> </ul> <p>Практически никогда не возникает одновременно две проблемы, по крайне мере, это маловероятно (если вы конечно не Twitter или Facebook пишете). Если вдруг такое происходит — возможно система неверно спроектирована, и её реализацию следует пересмотреть.</p> <h2 id="проблема-чтения-данных">Проблема чтения данных</h2> <p>Проблема с чтением данных обычно начинается, когда СУБД не в состоянии обеспечить то количество выборок, которое требуется. В основном такое происходит в блогах, новостных лентах и т.д. Хочу сразу отметить, что подобную проблему лучше решать внедрением кеширования, а потом уже думать как масштабировать СУБД.</p> <h3 id="методы-решения">Методы решения</h3> <ul> <li><p><strong>PgPool-II v.3 + PostgreSQL v.9 с Streaming Replication</strong> — отличное решение для масштабирования на чтение, более подробно можно ознакомится по <a href="http://pgpool.projects.pgfoundry.org/contrib_docs/simple_sr_setting/index.html">ссылке</a>. Основные преимущества:</p> <ul> <li><p>Низкая задержка репликации между мастером и слейвом;</p></li> <li><p>Производительность записи падает незначительно;</p></li> <li><p>Отказоустойчивость (failover);</p></li> <li><p>Пулы соединений;</p></li> <li><p>Интеллектуальная балансировка нагрузки – проверка задержки репликации между мастером и слейвом (сам проверяет <code>pg_current_xlog_location</code> и <code>pg_last_xlog_receive_location</code>);</p></li> <li><p>Добавление слейвов СУБД без остановки pgpool-II;</p></li> <li><p>Простота в настройке и обслуживании;</p></li> </ul></li> <li><p><strong>PgPool-II v.3 + PostgreSQL с Slony/Londiste/Bucardo</strong> — аналогично предыдущему решению, но с использованием Slony/Londiste/Bucardo. Основные преимущества:</p> <ul> <li><p>Отказоустойчивость (failover);</p></li> <li><p>Пулы соединений;</p></li> <li><p>Интеллектуальная балансировка нагрузки – проверка задержки репликации между мастером и слейвом;</p></li> <li><p>Добавление слейв СУБД без остановки pgpool-II;</p></li> <li><p>Можно использовать Postgresql ниже 9 версии;</p></li> </ul></li> <li><p><strong>Citus</strong> — подробнее можно прочитать в «[sec:citus] » главе;</p></li> <li><p><strong>Postgres-X2</strong> — подробнее можно прочитать в «[sec:postgres-x2] » главе;</p></li> <li><p><strong>Postgres-XL</strong> — подробнее можно прочитать в «[sec:postgres-xl] » главе;</p></li> </ul> <h2 id="проблема-записи-данных">Проблема записи данных</h2> <p>Обычно такая проблема возникает в системах, которые производят анализ больших объемов данных (например аналог Google Analytics). Данные активно пишутся и мало читаются (или читается только суммарный вариант собранных данных).</p> <h3 id="методы-решения-1">Методы решения</h3> <p>Один из самых популярных методов решение проблем — размазать нагрузку по времени с помощью систем очередей.</p> <ul> <li><p><strong>PgQ</strong> — это система очередей, разработанная на базе PostgreSQL. Разработчики — компания Skype. Используется в Londiste (подробнее «[sec:londiste] »). Особенности:</p> <ul> <li><p>Высокая производительность благодаря особенностям PostgreSQL;</p></li> <li><p>Общая очередь, с поддержкой нескольких обработчиков и нескольких генераторов событий;</p></li> <li><p>PgQ гарантирует, что каждый обработчик увидит каждое событие как минимум один раз;</p></li> <li><p>События достаются из очереди «пачками» (batches);</p></li> <li><p>Чистое API на SQL функциях;</p></li> <li><p>Удобный мониторинг;</p></li> </ul></li> <li><p><strong>Citus</strong> — подробнее можно прочитать в «[sec:citus] » главе;</p></li> <li><p><strong>Postgres-X2</strong> — подробнее можно прочитать в «[sec:postgres-x2] » главе;</p></li> <li><p><strong>Postgres-XL</strong> — подробнее можно прочитать в «[sec:postgres-xl] » главе;</p></li> </ul> <h2 id="заключение-29">Заключение</h2> <p>В данной главе показаны только несколько возможных вариантов решения задач масштабирования PostgreSQL. Таких стратегий существует огромное количество и каждая из них имеет как сильные, так и слабые стороны. Самое важное то, что выбор оптимальной стратегии масштабирования для решения поставленных задач остается на плечах разработчиков и/или администраторов СУБД.</p> <h1 id="утилиты-для-postgresql">Утилиты для PostgreSQL</h1> <h2 id="введение-11">Введение</h2> <p>В данной главе собраны полезные утилиты для PostgreSQL, которые не упоминались в других разделах книги.</p> <h2 id=pgcli>Pgcli</h2> <p><a href="http://pgcli.com/">Pgcli</a> — интерфейс командной строки для PostgreSQL с автозаполнением и подсветкой синтаксиса. Написан на Python.</p> <h2 id=pgloader>Pgloader</h2> <p><a href="http://pgloader.io/">Pgloader</a> — консольная утилита для переноса данных с CSV файлов, HTTP ресурсов, SQLite, dBase или MySQL баз данных в PostgreSQL. Для быстрой загрузки данных используется <code>COPY</code> протокол. Pgloader содержит модули для преобразование данных, которые позволяют преобразовывать данные во время переноса (например, преобразование набора цифр в IP адрес или разбить строку на два текстовых поля).</p> <h2 id="postgres.app">Postgres.app</h2> <p><a href="http://postgresapp.com/">Postgres.app</a> — полнофункциональный PostgreSQL, который упакован в качестве стандартного Mac приложения, поэтому работает только на Mac OS системах. Приложение имеет красивый пользовательский интерфейс и работает в системной строке меню.</p> <h2 id=pgadmin>pgAdmin</h2> <p><a href="https://www.pgadmin.org/">pgAdmin</a> — инструмент c графическим интерфейсом для управления PostgreSQL и производных от него баз данных. Он может быть запущен в качестве десктоп или веб-приложения. Написан на Python (с использованием Flask фреймворка) и JavaScript (с использованием jQuery и Bootstrap).</p> <p>Cуществуют альтернативные програмные продукты для PostgreSQL (как платные, так и бесплатные). Вот примеры бесплатных альтернатив:</p> <ul> <li><p><a href="http://dbeaver.jkiss.org/">DBeaver</a>;</p></li> <li><p><a href="http://dbglass.web-pal.com/">DBGlass</a>;</p></li> <li><p><a href="http://www.metabase.com/">Metabase</a>;</p></li> <li><p><a href="http://pgmodeler.com.br/">pgModeler</a>;</p></li> <li><p><a href="http://sosedoff.github.io/pgweb/">Pgweb</a>;</p></li> <li><p><a href="https://github.com/Paxa/postbird">Postbird</a>;</p></li> <li><p><a href="http://www.sqltabs.com/">SQL Tabs</a>;</p></li> </ul> <h2 id=postgrest>PostgREST</h2> <p><a href="http://postgrest.com/">PostgREST</a> — инструмент создания HTTP <a href="https://ru.wikipedia.org/wiki/REST">REST</a> API для PostgreSQL базы. Написан на Haskell.</p> <h2 id=ngx_postgres>Ngx_postgres</h2> <p><a href="https://github.com/FRiCKLE/ngx_postgres">Ngx_postgres</a> — модуль для <a href="https://www.nginx.com/">Nginx</a>, который позволяет напрямую работать с PostgreSQL базой. Ответы генерируется в формате RDS (Resty DBD Stream), поэтому он совместим с <code>ngx_rds_json</code>, <code>ngx_rds_csv</code> и <code>ngx_drizzle</code> модулями.</p> <h2 id="заключение-30">Заключение</h2> <p>В данной главе рассмотрено лишь несколько полезных утилит для PostgreSQL. Каждый день для это базы данных появляется все больше интересных инструментов, которые улучшают, упрощают или автоматизируют работу с данной базой данных.</p> <h1 id="полезные-мелочи">Полезные мелочи</h1> <h2 id="введение-12">Введение</h2> <p>Иногда возникают очень интересные проблемы по работе с PostgreSQL, которые при нахождении ответа поражают своей лаконичностью, красотой и простым исполнением. В данной главе я решил собрать интересные методы решения разных проблем, с которыми сталкиваются люди при работе с PostgreSQL.</p> <h2 id="мелочи">Мелочи</h2> <h3 id="размер-объектов-в-базе-данных">Размер объектов в базе данных</h3> <p>Данный запрос показывает размер объектов в базе данных (например, таблиц и индексов).</p> <p>Пример вывода:</p> <pre><code>        relation        |    size
------------------------+------------
 public.accounts        | 326 MB
 public.accounts_pkey   | 44 MB
 public.history         | 592 kB
 public.tellers_pkey    | 16 kB
 public.branches_pkey   | 16 kB
 public.tellers         | 16 kB
 public.branches        | 8192 bytes</code></pre> <h3 id="размер-самых-больших-таблиц">Размер самых больших таблиц</h3> <p>Данный запрос показывает размер самых больших таблиц в базе данных.</p> <p>Пример вывода:</p> <pre><code>            relation            | total_size
--------------------------------+------------
 public.actions                 | 4249 MB
 public.product_history_records | 197 MB
 public.product_updates         | 52 MB
 public.import_products         | 34 MB
 public.products                | 29 MB
 public.visits                  | 25 MB</code></pre> <h3 id="средний-count">«Средний» count</h3> <p>Данный метод позволяет узнать приблизительное количество записей в таблице. Для огромных таблиц этот метод работает быстрее, чем обыкновенный count.</p> <p>Пример:</p> <pre><code>CREATE TABLE foo (r double precision);
INSERT INTO foo SELECT random() FROM generate_series(1, 1000);
ANALYZE foo;

# SELECT count(*) FROM foo WHERE r &lt; 0.1;
 count
-------
    92
(1 row)

# SELECT count_estimate(&#39;SELECT * FROM foo WHERE r &lt; 0.1&#39;);
 count_estimate
----------------
             94
(1 row)</code></pre> <h3 id="случайное-число-из-диапазона">Случайное число из диапазона</h3> <p>Данный метод позволяет взять случайное число из указаного диапазона (целое или с плавающей запятой).</p> <p>Пример:</p> <pre><code>SELECT random(1,10)::int, random(1,10);
 random |      random
--------+------------------
      6 | 5.11675184825435
(1 row)

SELECT random(1,10)::int, random(1,10);
 random |      random
--------+------------------
      7 | 1.37060070643201
(1 row)</code></pre> <h3 id="алгоритм-луна">Алгоритм Луна</h3> <p><a href="http://en.wikipedia.org/wiki/Luhn_algorithm">Алгоритм Луна или формула Луна</a> — алгоритм вычисления контрольной цифры, получивший широкую популярность. Он используется, в частности, при первичной проверке номеров банковских пластиковых карт, номеров социального страхования в США и Канаде. Алгоритм был разработан сотрудником компании «IBM» Хансом Петером Луном и запатентован в 1960 году.</p> <p>Контрольные цифры вообще и алгоритм Луна в частности предназначены для защиты от случайных ошибок, а не преднамеренных искажений данных.</p> <p>Алгоритм Луна реализован на чистом SQL. Обратите внимание, что эта реализация является чисто арифметической.</p> <p>Пример:</p> <pre><code>Select luhn_verify(49927398716);
 luhn_verify
-------------
 t
(1 row)

Select luhn_verify(49927398714);
 luhn_verify
-------------
 f
(1 row)</code></pre> <h3 id="выборка-и-сортировка-по-данному-набору-данных">Выборка и сортировка по данному набору данных</h3> <p>Выбор данных по определенному набору данных можно сделать с помощью обыкновенного <code>IN</code>. Но как сделать подобную выборку и отсортировать данные в том же порядке, в котором передан набор данных? Например:</p> <p>Дан набор: (2,6,4,10,25,7,9). Нужно получить найденные данные в таком же порядке т.е. 2 2 2 6 6 4 4</p> <p>где</p> <p><code>VALUES(3),(2),(6),(1),(4)</code> — наш набор данных</p> <p><code>foo</code> – таблица, из которой идет выборка</p> <p><code>foo.catalog_id</code> — поле, по которому ищем набор данных (замена <code>foo.catalog_id IN(3,2,6,1,4)</code>)</p> <h3 id="quine-запрос-который-выводит-сам-себя">Quine — запрос который выводит сам себя</h3> <p>Куайн, квайн (англ. quine) — компьютерная программа (частный случай метапрограммирования), которая выдаёт на выходе точную копию своего исходного текста.</p> <h3 id="поиск-дубликатов-индексов">Поиск дубликатов индексов</h3> <p>Запрос находит индексы, созданные на одинаковый набор столбцов (такие индексы эквивалентны, а значит бесполезны).</p> <h3 id="размер-и-статистика-использования-индексов">Размер и статистика использования индексов</h3> <h3 id="sec:snippets-bloating">Размер распухания (bloat) таблиц и индексов в базе данных</h3> <p>Запрос, который показывает «приблизительный» bloat (раздутие) таблиц и индексов в базе:</p> <p><span>9</span></p> <p>Алексей Борзов (Sad Spirit) borz_off@cs.msu.su <em>PostgreSQL: настройка производительности</em> http://www.phpclub.ru/detail/store/pdf/postgresql-performance.pdf</p> <p>Eugene Kuzin eugene@kuzin.net <em>Настройка репликации в PostgreSQL с помощью системы Slony-I</em> http://www.kuzin.net/work/sloniki-privet.html</p> <p>Sergey Konoplev gray.ru@gmail.com <em>Установка Londiste в подробностях</em> http://gray-hemp.blogspot.com/2010/04/londiste.html</p> <p>Dmitry Stasyuk <em>Учебное руководство по pgpool-II</em> http://undenied.ru/2009/03/04/uchebnoe-rukovodstvo-po-pgpool-ii/</p> <p>Чиркин Дима dmitry.chirkin@gmail.com <em>Горизонтальное масштабирование PostgreSQL с помощью PL/Proxy</em> http://habrahabr.ru/blogs/postgresql/45475/</p> <p>Иван Блинков wordpress@insight-it.ru <em>Hadoop</em> http://www.insight-it.ru/masshtabiruemost/hadoop/</p> <p>Padraig O’Sullivan <em>Up and Running with HadoopDB</em> http://posulliv.github.com/2010/05/10/hadoopdb-mysql.html</p> <p>Иван Золотухин <em>Масштабирование PostgreSQL: готовые решения от Skype</em> http://postgresmen.ru/articles/view/25</p> <p><em>Streaming Replication</em>. http://wiki.postgresql.org/wiki/Streaming_Replication</p> <p>Den Golotyuk <em>Шардинг, партиционирование, репликация - зачем и когда?</em> http://highload.com.ua/index.php/2009/05/06/шардинг-партиционирование-репликац/</p> <p><em>Postgres-XC — A PostgreSQL Clustering Solution</em> http://www.linuxforu.com/2012/01/postgres-xc-database-clustering-solution/</p> <p><em>Введение в PostgreSQL BDR</em> http://habrahabr.ru/post/227959/</p> <p><em>Популярный обзор внутренностей базы данных. Часть пятая</em> http://zamotivator.livejournal.com/332814.html</p> <p><em>BRIN-индексы в PostgreSQL</em> http://langtoday.com/?p=485</p> <p><em>Huge Pages в PostgreSQL</em> https://habrahabr.ru/post/228793/</p> <p><em>Greenplum DB</em> https://habrahabr.ru/company/tinkoff/blog/267733/</p> <p><em>Введение в PostGIS</em> https://live.osgeo.org/ru/quickstart/postgis_quickstart.html</p> <p><em>Введение в полнотекстовый поиск в PostgreSQL</em> http://www.sai.msu.su/ megera/postgres/talks/fts_pgsql_intro.html</p> <p><em>pg_arman</em> https://postgrespro.ru/docs/postgrespro/9.5/pg-arman.html</p> <p><em>It Probably Works</em> http://queue.acm.org/detail.cfm?id=2855183</p> <p><em>Кластер PostgreSQL высокой надежности на базе Patroni, Haproxy, Keepalived</em> https://habrahabr.ru/post/322036/</p> <section class=footnotes> <hr/> <ol> <li id=fn1><p>RULE — реализованное в PostgreSQL расширение стандарта SQL, позволяющее, в частности, создавать обновляемые представления<a href="#fnref1">↩</a></p></li> <li id=fn2><p>«на нашем форуме более 10000 зарегистрированных пользователей, оставивших более 50000 сообщений!»<a href="#fnref2">↩</a></p></li> </ol> </section> </body> </html>